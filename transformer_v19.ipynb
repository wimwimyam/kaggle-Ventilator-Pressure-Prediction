{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:22.047222Z",
     "iopub.status.busy": "2021-10-13T04:17:22.046447Z",
     "iopub.status.idle": "2021-10-13T04:17:23.269684Z",
     "shell.execute_reply": "2021-10-13T04:17:23.269210Z",
     "shell.execute_reply.started": "2021-10-12T05:39:55.354395Z"
    },
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1633176713080,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "vsRfljafuYSE",
    "papermill": {
     "duration": 1.246673,
     "end_time": "2021-10-13T04:17:23.269823",
     "exception": false,
     "start_time": "2021-10-13T04:17:22.023150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:23.310367Z",
     "iopub.status.busy": "2021-10-13T04:17:23.309536Z",
     "iopub.status.idle": "2021-10-13T04:17:23.311571Z",
     "shell.execute_reply": "2021-10-13T04:17:23.312113Z",
     "shell.execute_reply.started": "2021-10-12T05:39:56.704014Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633176713080,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "cagWoXxKwV31",
    "papermill": {
     "duration": 0.023844,
     "end_time": "2021-10-13T04:17:23.312240",
     "exception": false,
     "start_time": "2021-10-13T04:17:23.288396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:23.350976Z",
     "iopub.status.busy": "2021-10-13T04:17:23.350429Z",
     "iopub.status.idle": "2021-10-13T04:17:23.352666Z",
     "shell.execute_reply": "2021-10-13T04:17:23.353093Z",
     "shell.execute_reply.started": "2021-10-12T05:39:56.712676Z"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1633176713313,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "9VBtJTT0uo22",
    "papermill": {
     "duration": 0.023661,
     "end_time": "2021-10-13T04:17:23.353210",
     "exception": false,
     "start_time": "2021-10-13T04:17:23.329549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'transformer_v19'\n",
    "base_dir = \"./\"\n",
    "if not os.path.exists(f'models/{MODEL_NAME}'):\n",
    "    os.makedirs(f'models/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:23.392557Z",
     "iopub.status.busy": "2021-10-13T04:17:23.392066Z",
     "iopub.status.idle": "2021-10-13T04:17:35.818002Z",
     "shell.execute_reply": "2021-10-13T04:17:35.818695Z",
     "shell.execute_reply.started": "2021-10-12T05:39:56.724572Z"
    },
    "executionInfo": {
     "elapsed": 9971,
     "status": "ok",
     "timestamp": 1633176723595,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "kLXYOgoeuvy6",
    "papermill": {
     "duration": 12.448382,
     "end_time": "2021-10-13T04:17:35.818990",
     "exception": false,
     "start_time": "2021-10-13T04:17:23.370608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(base_dir + 'train.csv')\n",
    "test_df = pd.read_csv(base_dir + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:35.874558Z",
     "iopub.status.busy": "2021-10-13T04:17:35.873440Z",
     "iopub.status.idle": "2021-10-13T04:17:36.550713Z",
     "shell.execute_reply": "2021-10-13T04:17:36.550223Z",
     "shell.execute_reply.started": "2021-10-12T05:40:08.547318Z"
    },
    "papermill": {
     "duration": 0.705919,
     "end_time": "2021-10-13T04:17:36.550858",
     "exception": false,
     "start_time": "2021-10-13T04:17:35.844939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_df['pressure'] = le.fit_transform(train_df['pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:17:42.924598Z",
     "iopub.status.busy": "2021-10-13T04:17:42.924002Z",
     "iopub.status.idle": "2021-10-13T04:18:03.293805Z",
     "shell.execute_reply": "2021-10-13T04:18:03.294610Z",
     "shell.execute_reply.started": "2021-10-12T05:40:09.33342Z"
    },
    "executionInfo": {
     "elapsed": 15912,
     "status": "ok",
     "timestamp": 1633176739504,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "OhZfYGCOu7jn",
    "papermill": {
     "duration": 26.726301,
     "end_time": "2021-10-13T04:18:03.294787",
     "exception": false,
     "start_time": "2021-10-13T04:17:36.568486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['RC'] = (train_df['R'].astype(str) + '_' + train_df['C'].astype(str))\n",
    "train_df['RC'] = train_df['RC'].map({'20_50':0, '20_20':1, '50_20':2, '50_50':3, '5_50':4, '5_20':5, '50_10':6, '20_10':7, '5_10':8})\n",
    "test_df['RC'] = (test_df['R'].astype(str) + '_' + test_df['C'].astype(str))\n",
    "test_df['RC'] = test_df['RC'].map({'20_50':0, '20_20':1, '50_20':2, '50_50':3, '5_50':4, '5_20':5, '50_10':6, '20_10':7, '5_10':8})\n",
    "# train_df['u_in_cat'] = train_df['u_in'].round().astype(int)\n",
    "# test_df['u_in_cat'] = test_df['u_in'].round().astype(int)\n",
    "train_df['u_in_0'] = (train_df['u_in'].round() == 0).astype(int)\n",
    "test_df['u_in_0'] = (test_df['u_in'].round() == 0).astype(int)\n",
    "train_df['u_in_5'] = (train_df['u_in'].round() == 5).astype(int)\n",
    "test_df['u_in_5'] = (test_df['u_in'].round() == 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:03.374649Z",
     "iopub.status.busy": "2021-10-13T04:18:03.373814Z",
     "iopub.status.idle": "2021-10-13T04:18:03.377606Z",
     "shell.execute_reply": "2021-10-13T04:18:03.377190Z",
     "shell.execute_reply.started": "2021-10-12T05:40:37.325349Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633176739505,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "e-n7EBcytRFf",
    "papermill": {
     "duration": 0.022961,
     "end_time": "2021-10-13T04:18:03.377711",
     "exception": false,
     "start_time": "2021-10-13T04:18:03.354750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = train_df.columns.drop(['id','breath_id','pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:03.419638Z",
     "iopub.status.busy": "2021-10-13T04:18:03.418779Z",
     "iopub.status.idle": "2021-10-13T04:18:09.870641Z",
     "shell.execute_reply": "2021-10-13T04:18:09.870135Z",
     "shell.execute_reply.started": "2021-10-12T05:40:37.338221Z"
    },
    "executionInfo": {
     "elapsed": 3412,
     "status": "ok",
     "timestamp": 1633176742913,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "SW8IEFydtP3S",
    "papermill": {
     "duration": 6.473443,
     "end_time": "2021-10-13T04:18:09.870773",
     "exception": false,
     "start_time": "2021-10-13T04:18:03.397330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "RS = StandardScaler()\n",
    "all_df = pd.concat([train_df,test_df])\n",
    "train_df['u_in'] = np.log1p(train_df['u_in'] - all_df['u_in'].min())\n",
    "test_df['u_in'] = np.log1p(test_df['u_in'] - all_df['u_in'].min())\n",
    "all_df['u_in'] = np.log1p(all_df['u_in'] - all_df['u_in'].min())\n",
    "\n",
    "RS.fit(all_df[['u_in','time_step']])\n",
    "train_df[['u_in','time_step']] = RS.transform(train_df[['u_in','time_step']])\n",
    "test_df[['u_in','time_step']] = RS.transform(test_df[['u_in','time_step']])\n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "# RS = StandardScaler()\n",
    "# all_df = pd.concat([train_df,test_df])\n",
    "# RS.fit(all_df[['u_in','time_step']])\n",
    "# train_df[['u_in','time_step']] = RS.transform(train_df[['u_in','time_step']])\n",
    "# test_df[['u_in','time_step']] = RS.transform(test_df[['u_in','time_step']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:09.991081Z",
     "iopub.status.busy": "2021-10-13T04:18:09.989870Z",
     "iopub.status.idle": "2021-10-13T04:18:10.177419Z",
     "shell.execute_reply": "2021-10-13T04:18:10.176927Z",
     "shell.execute_reply.started": "2021-10-12T05:40:43.444726Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633176742914,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "ahQ_qNl6wYf7",
    "papermill": {
     "duration": 0.288427,
     "end_time": "2021-10-13T04:18:10.177548",
     "exception": false,
     "start_time": "2021-10-13T04:18:09.889121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_tr = train_df[['RC','u_in','u_out','u_in_0','u_in_5','time_step']].values.reshape(-1,80,6)\n",
    "X_test = test_df[['RC','u_in','u_out','u_in_0','u_in_5','time_step']].values.reshape(-1,80,6)\n",
    "# X_tr = train_df[features].values.reshape(-1,80,18)\n",
    "# X_test = test_df[features].values.reshape(-1,80,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.215778Z",
     "iopub.status.busy": "2021-10-13T04:18:10.215249Z",
     "iopub.status.idle": "2021-10-13T04:18:10.218888Z",
     "shell.execute_reply": "2021-10-13T04:18:10.218459Z",
     "shell.execute_reply.started": "2021-10-12T05:40:43.782967Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633176743344,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "am4C6rtTwxcU",
    "papermill": {
     "duration": 0.024291,
     "end_time": "2021-10-13T04:18:10.219004",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.194713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_tr = train_df['pressure'].values.reshape(-1,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo_label = np.load(\"subs/sub_logits_transformer_v11.npy\")\n",
    "# from scipy.special import softmax\n",
    "# pseudo_label = softmax(pseudo_label,axis=2)\n",
    "# mask = test_df['u_out'].values.reshape(-1,80) == 0\n",
    "# tmp1 = pseudo_label.max(2)\n",
    "# res = []\n",
    "# for i in tqdm(range(len(tmp1))):\n",
    "#     res.append(tmp1[i][mask[i]].mean())\n",
    "# res = np.array(res)\n",
    "# mask = res > 0.6\n",
    "# np.mean(mask)\n",
    "# pseudo_label = pseudo_label.argmax(-1)\n",
    "# X_tr = np.concatenate([X_tr,X_test[mask]],axis=0)\n",
    "# y_tr = np.concatenate([y_tr,pseudo_label[mask]],axis=0)\n",
    "# import gc\n",
    "# del pseudo_label, mask, res, tmp1; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_pressures = train_df[\"pressure\"].unique()\n",
    "# sorted_pressures = np.sort(unique_pressures)\n",
    "# total_pressures_len = len(sorted_pressures)\n",
    "# def find_nearest(prediction):\n",
    "#     insert_idx = np.searchsorted(sorted_pressures, prediction)\n",
    "#     if insert_idx == total_pressures_len:\n",
    "#         # If the predicted value is bigger than the highest pressure in the train dataset,\n",
    "#         # return the max value.\n",
    "#         return sorted_pressures[-1]\n",
    "#     elif insert_idx == 0:\n",
    "#         # Same control but for the lower bound.\n",
    "#         return sorted_pressures[0]\n",
    "#     lower_val = sorted_pressures[insert_idx - 1]\n",
    "#     upper_val = sorted_pressures[insert_idx]\n",
    "#     return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n",
    "# pseudo_label = pd.read_csv(\"pressure_submission.csv\")['pressure'].values\n",
    "# for i in range(len(pseudo_label)):\n",
    "#     pseudo_label[i] = find_nearest(pseudo_label[i])\n",
    "# pseudo_label = le.transform(pseudo_label).reshape(-1,80)\n",
    "# X_tr = np.concatenate([X_tr,X_test],axis=0)\n",
    "# y_tr = np.concatenate([y_tr,pseudo_label],axis=0)\n",
    "# import gc\n",
    "# del pseudo_label; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.259036Z",
     "iopub.status.busy": "2021-10-13T04:18:10.258276Z",
     "iopub.status.idle": "2021-10-13T04:18:10.261260Z",
     "shell.execute_reply": "2021-10-13T04:18:10.261648Z",
     "shell.execute_reply.started": "2021-10-12T05:40:43.799012Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1633176743345,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "mBJplxgVzR3Z",
    "outputId": "f65abfaa-6083-4682-9027-09b75a68e97d",
    "papermill": {
     "duration": 0.025961,
     "end_time": "2021-10-13T04:18:10.261757",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.235796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75450, 80, 6), (50300, 80, 6), (75450, 80))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape,X_test.shape,y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.299977Z",
     "iopub.status.busy": "2021-10-13T04:18:10.299375Z",
     "iopub.status.idle": "2021-10-13T04:18:10.302105Z",
     "shell.execute_reply": "2021-10-13T04:18:10.301680Z",
     "shell.execute_reply.started": "2021-10-12T05:40:43.819363Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633176743345,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "n0ncwyMZwYUs",
    "papermill": {
     "duration": 0.023285,
     "end_time": "2021-10-13T04:18:10.302209",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.278924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    X_tr = X_tr[:1000]\n",
    "    y_tr = y_tr[:1000]\n",
    "    X_test = X_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.352282Z",
     "iopub.status.busy": "2021-10-13T04:18:10.351510Z",
     "iopub.status.idle": "2021-10-13T04:18:10.353809Z",
     "shell.execute_reply": "2021-10-13T04:18:10.353418Z",
     "shell.execute_reply.started": "2021-10-12T05:49:50.818657Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633176743345,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "KPHlkF2WcG-S",
    "papermill": {
     "duration": 0.034617,
     "end_time": "2021-10-13T04:18:10.353908",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.319291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, train_dataloader, epoch, device = torch.device('cpu')):\n",
    "    model.train()\n",
    "    MA_loss = 0\n",
    "    count = 0\n",
    "    for X,y in train_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y = F.one_hot(y,950).float()\n",
    "        y[:,:,:-1] += 0.1 * y[:,:,1:]\n",
    "        y[:,:,1:] += 0.1 * y[:,:,:-1]\n",
    "        y[y==1] = 0.8\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mask1 = X[:,:,2] == 0\n",
    "        mask2 = X[:,:,2] == 1\n",
    "        pred = model(X)\n",
    "\n",
    "        pred = torch.sigmoid(pred[mask1].reshape(-1,950))\n",
    "        y = y[mask1].reshape(-1,950)\n",
    "\n",
    "        loss = -torch.sum(y * torch.log(1e-8 + pred) + (1-y) * torch.log(1 - pred + 1e-8),dim=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch < 100:\n",
    "            scheduler.step()\n",
    "        \n",
    "        MA_loss += loss.item() * len(y)\n",
    "        count += len(y)\n",
    "    MA_loss /= count\n",
    "    return MA_loss\n",
    "\n",
    "def evaluation(model, val_dataloader, device = torch.device('cpu')):\n",
    "    model.eval()\n",
    "    criterion = nn.L1Loss()\n",
    "    MA_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in val_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = X[:,:,2] == 0\n",
    "            pred = model(X)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred[mask].reshape(-1).cpu().numpy()\n",
    "            pred = torch.Tensor(le.inverse_transform(pred)).to(device)\n",
    "            y = y[mask].reshape(-1).cpu().long().numpy()\n",
    "            y = torch.Tensor(le.inverse_transform(y)).to(device)\n",
    "            loss = criterion(pred, y)\n",
    "            # loss = criterion(pred.reshape(-1), y.reshape(-1))\n",
    "            MA_loss += loss.item() * len(y)\n",
    "            count += len(y)\n",
    "        MA_loss /= count\n",
    "    return MA_loss\n",
    "\n",
    "def inference(model, test_dataloader, device = torch.device('cpu'), istest = False):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        if istest:\n",
    "            for X in test_dataloader:\n",
    "                X = X[0]\n",
    "                X = X.to(device)\n",
    "                pred = model(X).cpu()\n",
    "                pred = torch.argmax(pred,dim=-1)\n",
    "                prediction.append(pred)\n",
    "        else:\n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                pred = model(X).cpu()\n",
    "                pred = torch.argmax(pred,dim=-1)\n",
    "                prediction.append(pred)\n",
    "    prediction = torch.cat(prediction,dim=0).numpy()\n",
    "    prediction = le.inverse_transform(prediction.reshape(-1)).reshape(-1,80)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.396705Z",
     "iopub.status.busy": "2021-10-13T04:18:10.395885Z",
     "iopub.status.idle": "2021-10-13T04:18:10.408552Z",
     "shell.execute_reply": "2021-10-13T04:18:10.408977Z",
     "shell.execute_reply.started": "2021-10-12T05:49:51.173858Z"
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1633176743750,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "Up4IIaxk6onI",
    "papermill": {
     "duration": 0.037697,
     "end_time": "2021-10-13T04:18:10.409090",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.371393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def calc_same_padding(kernel_size):\n",
    "    pad = kernel_size // 2\n",
    "    return (pad, pad - (kernel_size + 1) % 2)\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x.sigmoid()\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = x.chunk(2, dim=self.dim)\n",
    "        return out * gate.sigmoid()\n",
    "\n",
    "class DepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups = chan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, self.padding)\n",
    "        return self.conv(x)\n",
    "\n",
    "# attention, feedforward, and conv module\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, scale, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        mult = 4,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        causal = False,\n",
    "        expansion_factor = 2,\n",
    "        kernel_size = 31,\n",
    "        dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = dim * expansion_factor\n",
    "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Rearrange('b n c -> b c n'),\n",
    "            nn.Conv1d(dim, inner_dim * 2, 1),\n",
    "            GLU(dim=1),\n",
    "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding),\n",
    "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
    "            Swish(),\n",
    "            nn.Conv1d(inner_dim, dim, 1),\n",
    "            Rearrange('b c n -> b n c'),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.446377Z",
     "iopub.status.busy": "2021-10-13T04:18:10.445524Z",
     "iopub.status.idle": "2021-10-13T04:18:10.465735Z",
     "shell.execute_reply": "2021-10-13T04:18:10.466163Z",
     "shell.execute_reply.started": "2021-10-12T05:49:51.626554Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633176743750,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "oDGYzzf44rRS",
    "papermill": {
     "duration": 0.04015,
     "end_time": "2021-10-13T04:18:10.466275",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.426125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        max_pos_emb = 512,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads= heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.max_pos_emb = max_pos_emb\n",
    "        # self.rel_pos_emb1 = nn.Linear(1, heads, bias=False)\n",
    "        # self.rel_pos_emb2 = nn.Linear(1, heads, bias=False)\n",
    "        self.rel_pos_emb = nn.Sequential(nn.Linear(1, dim_head),nn.GELU(),nn.Linear(dim_head, dim_head))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "    def forward(self, x, position, context = None, mask = None, context_mask = None):\n",
    "        n, device, h, max_pos_emb, has_context = x.shape[-2], x.device, self.heads, self.max_pos_emb, exists(context)\n",
    "        context = default(context, x)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        # shaw's relative positional embedding\n",
    "        dist = rearrange(position, 'b i -> b i () ()') - rearrange(position, 'b j -> b () j ()')\n",
    "        # rel_pos_emb = self.rel_pos_emb1(F.relu(dist)) + self.rel_pos_emb2(F.relu(-dist))   #(bijh)\n",
    "        # pos_attn = rel_pos_emb.permute(0,3,1,2)\n",
    "        rel_pos_emb = self.rel_pos_emb(dist)\n",
    "        pos_attn = einsum('b h n d, b n r d -> b h n r', q, rel_pos_emb) * self.scale\n",
    "        dots = dots + pos_attn\n",
    "\n",
    "        if exists(mask) or exists(context_mask):\n",
    "            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))\n",
    "            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(context_mask, 'b j -> b () () j')\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.tril(torch.ones(dots.shape[-2:],device=dots.device)).T\n",
    "            mask = rearrange(mask, 'n r -> () () n r')\n",
    "            dots = dots - mask * 999\n",
    "\n",
    "        attn = dots.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class CustomConformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        ff_mult = 4,\n",
    "        conv_expansion_factor = 2,\n",
    "        conv_kernel_size = 31,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        conv_dropout = 0.,\n",
    "        causal = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ff1 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
    "        self.attn = CustomAttention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, causal = causal)\n",
    "        self.conv = ConformerConvModule(dim = dim, causal = causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout)\n",
    "        self.ff2 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
    "\n",
    "        self.attn = PreNorm(dim, self.attn)\n",
    "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
    "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
    "\n",
    "    def forward(self, x, pos, mask = None):\n",
    "        x = self.ff1(x) + x\n",
    "        x = self.attn(x, position = pos, mask = mask) + x\n",
    "        x = self.conv(x) + x\n",
    "        x = self.ff2(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.514157Z",
     "iopub.status.busy": "2021-10-13T04:18:10.513366Z",
     "iopub.status.idle": "2021-10-13T04:18:10.515694Z",
     "shell.execute_reply": "2021-10-13T04:18:10.515287Z",
     "shell.execute_reply.started": "2021-10-12T05:49:51.952469Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633176743750,
     "user": {
      "displayName": "Pak Wing YAM",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01265674556408476665"
     },
     "user_tz": -480
    },
    "id": "yoWvAtiNcHBm",
    "papermill": {
     "duration": 0.032424,
     "end_time": "2021-10-13T04:18:10.515800",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.483376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.pe\n",
    "\n",
    "class BrainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        DIM = 256\n",
    "        n_layers = 4\n",
    "        self.input_layer = nn.Sequential(nn.Linear(5,DIM),nn.Mish(),nn.Linear(DIM,DIM),nn.Mish())\n",
    "        self.emb_RC = nn.Embedding(9,DIM)\n",
    "        self.emb_u_in = nn.Embedding(101,DIM)\n",
    "        self.scale_layer = nn.Linear(2*DIM,DIM)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder.append(CustomConformerBlock(dim = DIM,\n",
    "                          dim_head = DIM//8,\n",
    "                          heads = 8,\n",
    "                          ff_mult = 4,\n",
    "                          conv_expansion_factor = 2,\n",
    "                          conv_kernel_size = 5,\n",
    "                          attn_dropout = 0.1,\n",
    "                          ff_dropout = 0.2,\n",
    "                          conv_dropout = 0.05,\n",
    "                          causal = False))\n",
    "        self.fc = nn.Sequential(nn.Linear(DIM,DIM),nn.Mish(),nn.Linear(DIM,950))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #(B,L,C)\n",
    "        pos = X[:,:,-1]\n",
    "        X_dense = self.input_layer(X[:,:,1:])\n",
    "        X = torch.cat([X_dense, self.emb_RC(X[:,:,0].long())],dim=-1)\n",
    "        X = self.scale_layer(X)\n",
    "        for layer in self.encoder:\n",
    "            X = layer(X,pos)\n",
    "        y = self.fc(X)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-13T04:18:10.562898Z",
     "iopub.status.busy": "2021-10-13T04:18:10.557434Z",
     "iopub.status.idle": "2021-10-13T13:02:23.401088Z",
     "shell.execute_reply": "2021-10-13T13:02:23.399054Z"
    },
    "id": "zHGGP6fbfKSv",
    "outputId": "1271a4dd-a37e-4b4d-b314-832921640556",
    "papermill": {
     "duration": 31452.86825,
     "end_time": "2021-10-13T13:02:23.401227",
     "exception": false,
     "start_time": "2021-10-13T04:18:10.532977",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yampa\\.conda\\envs\\pytorch15\\lib\\site-packages\\pytorch_ranger\\ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1025.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train_loss: 15.986838961387319\n",
      "val_loss: 2.5670292145290547\n",
      "best loss: 2.5670292145290547\n",
      "*********************************\n",
      "epoch 1\n",
      "Train_loss: 6.480173484608512\n",
      "val_loss: 1.7791233783686908\n",
      "best loss: 1.7791233783686908\n",
      "*********************************\n",
      "epoch 2\n",
      "Train_loss: 5.960901202312708\n",
      "val_loss: 1.1844997011580676\n",
      "best loss: 1.1844997011580676\n",
      "*********************************\n",
      "epoch 3\n",
      "Train_loss: 5.60958658609226\n",
      "val_loss: 0.7876959423536998\n",
      "best loss: 0.7876959423536998\n",
      "*********************************\n",
      "epoch 4\n",
      "Train_loss: 5.3796870260288285\n",
      "val_loss: 0.6117173619078342\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 5\n",
      "Train_loss: 5.367123644797926\n",
      "val_loss: 0.72032560790852\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 6\n",
      "Train_loss: 5.467260171024537\n",
      "val_loss: 0.770443998267314\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 7\n",
      "Train_loss: 5.483155807481745\n",
      "val_loss: 0.8698997534911287\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 8\n",
      "Train_loss: 5.407748548741241\n",
      "val_loss: 0.724959466622018\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 9\n",
      "Train_loss: 5.273813401759809\n",
      "val_loss: 0.647591563468986\n",
      "best loss: 0.6117173619078342\n",
      "*********************************\n",
      "epoch 10\n",
      "Train_loss: 5.097413959675474\n",
      "val_loss: 0.5518334655915731\n",
      "best loss: 0.5518334655915731\n",
      "*********************************\n",
      "epoch 11\n",
      "Train_loss: 4.898534744170469\n",
      "val_loss: 0.5499385953039572\n",
      "best loss: 0.5499385953039572\n",
      "*********************************\n",
      "epoch 12\n",
      "Train_loss: 4.670276183619658\n",
      "val_loss: 0.4175528718370045\n",
      "best loss: 0.4175528718370045\n",
      "*********************************\n",
      "epoch 13\n",
      "Train_loss: 4.451446171220297\n",
      "val_loss: 0.30722641831545433\n",
      "best loss: 0.30722641831545433\n",
      "*********************************\n",
      "epoch 14\n",
      "Train_loss: 4.266858724121255\n",
      "val_loss: 0.2812911566522985\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 15\n",
      "Train_loss: 4.244297474427998\n",
      "val_loss: 0.2942825003915184\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 16\n",
      "Train_loss: 4.37716396240932\n",
      "val_loss: 0.36415966160511276\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 17\n",
      "Train_loss: 4.563855955371923\n",
      "val_loss: 0.4505278112984744\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 18\n",
      "Train_loss: 4.6880230836795524\n",
      "val_loss: 0.4330542433616803\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 19\n",
      "Train_loss: 4.69430358257792\n",
      "val_loss: 0.41466007452828946\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 20\n",
      "Train_loss: 4.63642978830638\n",
      "val_loss: 0.37687887944744203\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 21\n",
      "Train_loss: 4.475612361553199\n",
      "val_loss: 0.38302177273775057\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 22\n",
      "Train_loss: 4.279800718977356\n",
      "val_loss: 0.2975051199374254\n",
      "best loss: 0.2812911566522985\n",
      "*********************************\n",
      "epoch 23\n",
      "Train_loss: 4.078834157387268\n",
      "val_loss: 0.24160128920492283\n",
      "best loss: 0.24160128920492283\n",
      "*********************************\n",
      "epoch 24\n",
      "Train_loss: 3.911375138930986\n",
      "val_loss: 0.22076663138325084\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 25\n",
      "Train_loss: 3.8903916396829086\n",
      "val_loss: 0.23372989092114854\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 26\n",
      "Train_loss: 4.015776565727453\n",
      "val_loss: 0.2680926091797946\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 27\n",
      "Train_loss: 4.1925963355957085\n",
      "val_loss: 0.34201413332837316\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 28\n",
      "Train_loss: 4.346344034467311\n",
      "val_loss: 0.42028684245371356\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 29\n",
      "Train_loss: 4.407510379296745\n",
      "val_loss: 0.32209129479893145\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 30\n",
      "Train_loss: 4.360231282567411\n",
      "val_loss: 0.3192919685787031\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 31\n",
      "Train_loss: 4.2472862765095885\n",
      "val_loss: 0.34557257613911396\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 32\n",
      "Train_loss: 4.078045288051681\n",
      "val_loss: 0.2568457267824081\n",
      "best loss: 0.22076663138325084\n",
      "*********************************\n",
      "epoch 33\n",
      "Train_loss: 3.8704460972089074\n",
      "val_loss: 0.21333771839001736\n",
      "best loss: 0.21333771839001736\n",
      "*********************************\n",
      "epoch 34\n",
      "Train_loss: 3.721863740341428\n",
      "val_loss: 0.19720186284621413\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 35\n",
      "Train_loss: 3.703357869711946\n",
      "val_loss: 0.20560507358656782\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 36\n",
      "Train_loss: 3.816299169186515\n",
      "val_loss: 0.24398437267966577\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 37\n",
      "Train_loss: 3.995878100075091\n",
      "val_loss: 0.3027868515274889\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 38\n",
      "Train_loss: 4.159398290682584\n",
      "val_loss: 0.3244802269622826\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 39\n",
      "Train_loss: 4.221555475378929\n",
      "val_loss: 0.27197751337834164\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 40\n",
      "Train_loss: 4.1955786382670865\n",
      "val_loss: 0.2902872520336731\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 41\n",
      "Train_loss: 4.105677633204092\n",
      "val_loss: 0.2542135393499652\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 42\n",
      "Train_loss: 3.925162359183695\n",
      "val_loss: 0.22532668786005577\n",
      "best loss: 0.19720186284621413\n",
      "*********************************\n",
      "epoch 43\n",
      "Train_loss: 3.73823229925761\n",
      "val_loss: 0.1960437053171313\n",
      "best loss: 0.1960437053171313\n",
      "*********************************\n",
      "epoch 44\n",
      "Train_loss: 3.595617137777285\n",
      "val_loss: 0.18245900224853695\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 45\n",
      "Train_loss: 3.575000906289667\n",
      "val_loss: 0.18922978932978432\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 46\n",
      "Train_loss: 3.696456492751222\n",
      "val_loss: 0.23389304227637908\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 47\n",
      "Train_loss: 3.8634236409186387\n",
      "val_loss: 0.24890034698654664\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 48\n",
      "Train_loss: 4.0215758118441\n",
      "val_loss: 0.28097483736882223\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 49\n",
      "Train_loss: 4.090788445414478\n",
      "val_loss: 0.28488766175803093\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 50\n",
      "Train_loss: 4.091455030542136\n",
      "val_loss: 0.25949095556988894\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 51\n",
      "Train_loss: 3.9821244634486317\n",
      "val_loss: 0.24126776889437715\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 52\n",
      "Train_loss: 3.820198994643642\n",
      "val_loss: 0.22337595308242852\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 53\n",
      "Train_loss: 3.635430379308393\n",
      "val_loss: 0.18362330238712934\n",
      "best loss: 0.18245900224853695\n",
      "*********************************\n",
      "epoch 54\n",
      "Train_loss: 3.5024890850326567\n",
      "val_loss: 0.17392319032401912\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 55\n",
      "Train_loss: 3.4850370155314834\n",
      "val_loss: 0.1794908462429067\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 56\n",
      "Train_loss: 3.596023893948411\n",
      "val_loss: 0.21078011807657066\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 57\n",
      "Train_loss: 3.758756815481181\n",
      "val_loss: 0.23468423449144296\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 58\n",
      "Train_loss: 3.930998914136482\n",
      "val_loss: 0.2932535926020444\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 59\n",
      "Train_loss: 3.998756550690408\n",
      "val_loss: 0.246384197541133\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 60\n",
      "Train_loss: 3.9884605572046827\n",
      "val_loss: 0.25560513671068086\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 61\n",
      "Train_loss: 3.902427341887885\n",
      "val_loss: 0.23105042593713404\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62\n",
      "Train_loss: 3.7358270619719485\n",
      "val_loss: 0.19745694423032709\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 63\n",
      "Train_loss: 3.5613778158317326\n",
      "val_loss: 0.17692741507255008\n",
      "best loss: 0.17392319032401912\n",
      "*********************************\n",
      "epoch 64\n",
      "Train_loss: 3.429143888980296\n",
      "val_loss: 0.16701396469958987\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 65\n",
      "Train_loss: 3.4114748143108367\n",
      "val_loss: 0.17050840772727802\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 66\n",
      "Train_loss: 3.5190103494449323\n",
      "val_loss: 0.2002860592453604\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 67\n",
      "Train_loss: 3.6758987183144844\n",
      "val_loss: 0.22580784707918933\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 68\n",
      "Train_loss: 3.8498115788614475\n",
      "val_loss: 0.2685995540299427\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 69\n",
      "Train_loss: 3.92675395336769\n",
      "val_loss: 0.2348911175481128\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 70\n",
      "Train_loss: 3.920599637836002\n",
      "val_loss: 0.2449385574342259\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 71\n",
      "Train_loss: 3.824306095929333\n",
      "val_loss: 0.21384513331945296\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 72\n",
      "Train_loss: 3.6737983387000623\n",
      "val_loss: 0.1947315970622498\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 73\n",
      "Train_loss: 3.4939590547739066\n",
      "val_loss: 0.17077975989422434\n",
      "best loss: 0.16701396469958987\n",
      "*********************************\n",
      "epoch 74\n",
      "Train_loss: 3.3685494542403642\n",
      "val_loss: 0.16279739972166404\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 75\n",
      "Train_loss: 3.355571344985154\n",
      "val_loss: 0.16755237493293973\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 76\n",
      "Train_loss: 3.449347173344604\n",
      "val_loss: 0.19611399588192716\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 77\n",
      "Train_loss: 3.616577018509513\n",
      "val_loss: 0.23165207181912256\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 78\n",
      "Train_loss: 3.8016590511020563\n",
      "val_loss: 0.26521867645799657\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 79\n",
      "Train_loss: 3.8620232622645023\n",
      "val_loss: 0.22902494616074076\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 80\n",
      "Train_loss: 3.867339537777853\n",
      "val_loss: 0.23245569643866054\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 81\n",
      "Train_loss: 3.7678553509871584\n",
      "val_loss: 0.20653132262414683\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 82\n",
      "Train_loss: 3.6147241709070443\n",
      "val_loss: 0.18714460616425588\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 83\n",
      "Train_loss: 3.439043512443268\n",
      "val_loss: 0.16743204796254355\n",
      "best loss: 0.16279739972166404\n",
      "*********************************\n",
      "epoch 84\n",
      "Train_loss: 3.3154517164838424\n",
      "val_loss: 0.16026819424207883\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 85\n",
      "Train_loss: 3.3016512823294373\n",
      "val_loss: 0.16448584169618047\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 86\n",
      "Train_loss: 3.404077976001429\n",
      "val_loss: 0.18268492765031266\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 87\n",
      "Train_loss: 3.558236322427719\n",
      "val_loss: 0.2118554155293567\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 88\n",
      "Train_loss: 3.7148381337787977\n",
      "val_loss: 0.2574667038330397\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 89\n",
      "Train_loss: 3.831197763148113\n",
      "val_loss: 0.22737917383516762\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 90\n",
      "Train_loss: 3.8182037302644205\n",
      "val_loss: 0.21722400089524194\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 91\n",
      "Train_loss: 3.7088410376807266\n",
      "val_loss: 0.20522459612148883\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 92\n",
      "Train_loss: 3.556838824233839\n",
      "val_loss: 0.18302012078379662\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 93\n",
      "Train_loss: 3.3951080809958407\n",
      "val_loss: 0.16418025678950002\n",
      "best loss: 0.16026819424207883\n",
      "*********************************\n",
      "epoch 94\n",
      "Train_loss: 3.2748479531342793\n",
      "val_loss: 0.15716927243790518\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 95\n",
      "Train_loss: 3.257527871614163\n",
      "val_loss: 0.16344155581411066\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 96\n",
      "Train_loss: 3.354701436894752\n",
      "val_loss: 0.18491346609786508\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 97\n",
      "Train_loss: 3.5175606350795934\n",
      "val_loss: 0.20548980705334985\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 98\n",
      "Train_loss: 3.673547424145823\n",
      "val_loss: 0.23681207994386863\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 99\n",
      "Train_loss: 3.7847894962966966\n",
      "val_loss: 0.2072393306564363\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 100\n",
      "Train_loss: 3.790379035577076\n",
      "val_loss: 0.22928632895067627\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 101\n",
      "Train_loss: 3.7759238195492895\n",
      "val_loss: 0.22168505442995123\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 102\n",
      "Train_loss: 3.762003699050433\n",
      "val_loss: 0.23344487946654954\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 103\n",
      "Train_loss: 3.751553436614566\n",
      "val_loss: 0.23306839219022615\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 104\n",
      "Train_loss: 3.768747706586782\n",
      "val_loss: 0.20506896486248838\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 105\n",
      "Train_loss: 3.7513006887818583\n",
      "val_loss: 0.21665980610493832\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 106\n",
      "Train_loss: 3.748239025756613\n",
      "val_loss: 0.21682755748694826\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 107\n",
      "Train_loss: 3.7398812885657775\n",
      "val_loss: 0.21965052025780635\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 108\n",
      "Train_loss: 3.736208810556015\n",
      "val_loss: 0.23344380208920182\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 109\n",
      "Train_loss: 3.731145450035308\n",
      "val_loss: 0.20324362650565148\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 110\n",
      "Train_loss: 3.731703523228916\n",
      "val_loss: 0.21589991422251315\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 111\n",
      "Train_loss: 3.716979714540605\n",
      "val_loss: 0.22291090244047937\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 112\n",
      "Train_loss: 3.701453788156463\n",
      "val_loss: 0.23295405388321627\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 113\n",
      "Train_loss: 3.693578371663401\n",
      "val_loss: 0.22270554509715637\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 114\n",
      "Train_loss: 3.6938248220925125\n",
      "val_loss: 0.19475754137897006\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 115\n",
      "Train_loss: 3.684777156060161\n",
      "val_loss: 0.20494433420483524\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 116\n",
      "Train_loss: 3.686245934399415\n",
      "val_loss: 0.20744945048840596\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 117\n",
      "Train_loss: 3.67641077264433\n",
      "val_loss: 0.21074896233379437\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 118\n",
      "Train_loss: 3.6705353120037976\n",
      "val_loss: 0.23727145114723433\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 119\n",
      "Train_loss: 3.6569683534608255\n",
      "val_loss: 0.19631322001292612\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 120\n",
      "Train_loss: 3.6568379419062005\n",
      "val_loss: 0.20745911975887427\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 121\n",
      "Train_loss: 3.6521249335516743\n",
      "val_loss: 0.22826123263527726\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 122\n",
      "Train_loss: 3.638958508786574\n",
      "val_loss: 0.22090491629127823\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123\n",
      "Train_loss: 3.6490234424852632\n",
      "val_loss: 0.2106663910778467\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 124\n",
      "Train_loss: 3.632966570992863\n",
      "val_loss: 0.21537777474619918\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "Epoch    26: reducing learning rate of group 0 to 1.5000e-03.\n",
      "epoch 125\n",
      "Train_loss: 3.630953912954998\n",
      "val_loss: 0.19719511773909335\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 126\n",
      "Train_loss: 3.352154959473374\n",
      "val_loss: 0.16362588809336379\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 127\n",
      "Train_loss: 3.299366246750593\n",
      "val_loss: 0.16964062694569418\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 128\n",
      "Train_loss: 3.2801505983230834\n",
      "val_loss: 0.16661016012235097\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 129\n",
      "Train_loss: 3.263396641588307\n",
      "val_loss: 0.15940625093025007\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 130\n",
      "Train_loss: 3.249318381443529\n",
      "val_loss: 0.1621082730855567\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 131\n",
      "Train_loss: 3.2448057230867717\n",
      "val_loss: 0.16331125298557866\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 132\n",
      "Train_loss: 3.240335129784339\n",
      "val_loss: 0.16615815859550906\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 133\n",
      "Train_loss: 3.2274735546476188\n",
      "val_loss: 0.165624656684999\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 134\n",
      "Train_loss: 3.2226816346728375\n",
      "val_loss: 0.15875058744831838\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 135\n",
      "Train_loss: 3.216735398558203\n",
      "val_loss: 0.16307842436213232\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 136\n",
      "Train_loss: 3.210546048620669\n",
      "val_loss: 0.1624318089809401\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 137\n",
      "Train_loss: 3.1996819845056916\n",
      "val_loss: 0.1619029171534186\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 138\n",
      "Train_loss: 3.1967580735565613\n",
      "val_loss: 0.16163309676512616\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 139\n",
      "Train_loss: 3.1951546994943283\n",
      "val_loss: 0.15833388038933477\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 140\n",
      "Train_loss: 3.19181070790366\n",
      "val_loss: 0.16039834786566404\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 141\n",
      "Train_loss: 3.185663730583048\n",
      "val_loss: 0.1637258016382035\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 142\n",
      "Train_loss: 3.1814321166002433\n",
      "val_loss: 0.15734623358367783\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 143\n",
      "Train_loss: 3.1755124409617204\n",
      "val_loss: 0.16630365960817953\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 144\n",
      "Train_loss: 3.1677281873175285\n",
      "val_loss: 0.1577813520072147\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 145\n",
      "Train_loss: 3.1688017369516084\n",
      "val_loss: 0.1576584209561572\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 146\n",
      "Train_loss: 3.16886342109597\n",
      "val_loss: 0.1621177839126465\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 147\n",
      "Train_loss: 3.162331527699087\n",
      "val_loss: 0.16101809881153137\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 148\n",
      "Train_loss: 3.159304141467988\n",
      "val_loss: 0.16199945574531924\n",
      "best loss: 0.15716927243790518\n",
      "*********************************\n",
      "epoch 149\n",
      "Train_loss: 3.154444697535198\n",
      "val_loss: 0.15457868065887226\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 150\n",
      "Train_loss: 3.1486265944947145\n",
      "val_loss: 0.15751260809205633\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 151\n",
      "Train_loss: 3.1475696601397916\n",
      "val_loss: 0.15789216830676994\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 152\n",
      "Train_loss: 3.142942538349385\n",
      "val_loss: 0.1648310120559459\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 153\n",
      "Train_loss: 3.1427981628739037\n",
      "val_loss: 0.15910420640703996\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 154\n",
      "Train_loss: 3.145109918461411\n",
      "val_loss: 0.15705923117453235\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 155\n",
      "Train_loss: 3.133330640470317\n",
      "val_loss: 0.155661330621363\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 156\n",
      "Train_loss: 3.131492902941458\n",
      "val_loss: 0.16046725984192928\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 157\n",
      "Train_loss: 3.130849607158802\n",
      "val_loss: 0.164702859924778\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 158\n",
      "Train_loss: 3.12885939725224\n",
      "val_loss: 0.16120811070100427\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 159\n",
      "Train_loss: 3.1224886103937384\n",
      "val_loss: 0.1558473489614765\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "Epoch    61: reducing learning rate of group 0 to 4.5000e-04.\n",
      "epoch 160\n",
      "Train_loss: 3.123250982608412\n",
      "val_loss: 0.15766348245773426\n",
      "best loss: 0.15457868065887226\n",
      "*********************************\n",
      "epoch 161\n",
      "Train_loss: 3.0182387588570485\n",
      "val_loss: 0.14976323417348827\n",
      "best loss: 0.14976323417348827\n",
      "*********************************\n",
      "epoch 162\n",
      "Train_loss: 2.999253228674982\n",
      "val_loss: 0.14939656618996697\n",
      "best loss: 0.14939656618996697\n",
      "*********************************\n",
      "epoch 163\n",
      "Train_loss: 2.9944851597707127\n",
      "val_loss: 0.14811469833891844\n",
      "best loss: 0.14811469833891844\n",
      "*********************************\n",
      "epoch 164\n",
      "Train_loss: 2.987786896163258\n",
      "val_loss: 0.14703957003892043\n",
      "best loss: 0.14703957003892043\n",
      "*********************************\n",
      "epoch 165\n",
      "Train_loss: 2.9870870462255583\n",
      "val_loss: 0.14652740692569655\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 166\n",
      "Train_loss: 2.983285535026314\n",
      "val_loss: 0.1477477290121363\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 167\n",
      "Train_loss: 2.981012799049345\n",
      "val_loss: 0.15063085759095937\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 168\n",
      "Train_loss: 2.981251797305392\n",
      "val_loss: 0.1490817771903862\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 169\n",
      "Train_loss: 2.9771861022083637\n",
      "val_loss: 0.14782431489880704\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 170\n",
      "Train_loss: 2.972872525219328\n",
      "val_loss: 0.14683528866888468\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 171\n",
      "Train_loss: 2.9700709908506036\n",
      "val_loss: 0.14753255027675208\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 172\n",
      "Train_loss: 2.9746105945322756\n",
      "val_loss: 0.1479541590374674\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 173\n",
      "Train_loss: 2.9693533829549255\n",
      "val_loss: 0.14714240102339513\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 174\n",
      "Train_loss: 2.9669898217220227\n",
      "val_loss: 0.14693489548604122\n",
      "best loss: 0.14652740692569655\n",
      "*********************************\n",
      "epoch 175\n",
      "Train_loss: 2.9644234198686807\n",
      "val_loss: 0.14616519181414442\n",
      "best loss: 0.14616519181414442\n",
      "*********************************\n",
      "epoch 176\n",
      "Train_loss: 2.9636346716159068\n",
      "val_loss: 0.1472481478739876\n",
      "best loss: 0.14616519181414442\n",
      "*********************************\n",
      "epoch 177\n",
      "Train_loss: 2.9627661728763655\n",
      "val_loss: 0.14905645535077153\n",
      "best loss: 0.14616519181414442\n",
      "*********************************\n",
      "epoch 178\n",
      "Train_loss: 2.9623967321720004\n",
      "val_loss: 0.14839050302707907\n",
      "best loss: 0.14616519181414442\n",
      "*********************************\n",
      "epoch 179\n",
      "Train_loss: 2.957523091363223\n",
      "val_loss: 0.1459256093680247\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 180\n",
      "Train_loss: 2.958663813873384\n",
      "val_loss: 0.14645864607110284\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 181\n",
      "Train_loss: 2.9588551112330905\n",
      "val_loss: 0.147348218200333\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 182\n",
      "Train_loss: 2.9548162042084303\n",
      "val_loss: 0.1492668756666494\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 183\n",
      "Train_loss: 2.954185710841532\n",
      "val_loss: 0.1481051826271881\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 184\n",
      "Train_loss: 2.95208471078138\n",
      "val_loss: 0.14702007806006798\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 185\n",
      "Train_loss: 2.9510052606065127\n",
      "val_loss: 0.14733394510516024\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 186\n",
      "Train_loss: 2.950563579846338\n",
      "val_loss: 0.14661642461256844\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 187\n",
      "Train_loss: 2.947646602760517\n",
      "val_loss: 0.1476826525944956\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 188\n",
      "Train_loss: 2.948307527872717\n",
      "val_loss: 0.1471434779506367\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 189\n",
      "Train_loss: 2.946651430003691\n",
      "val_loss: 0.14652050036144457\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "Epoch    91: reducing learning rate of group 0 to 1.3500e-04.\n",
      "epoch 190\n",
      "Train_loss: 2.9455183405148198\n",
      "val_loss: 0.1462100076648557\n",
      "best loss: 0.1459256093680247\n",
      "*********************************\n",
      "epoch 191\n",
      "Train_loss: 2.906427414742365\n",
      "val_loss: 0.14430117209984894\n",
      "best loss: 0.14430117209984894\n",
      "*********************************\n",
      "epoch 192\n",
      "Train_loss: 2.8992287262770104\n",
      "val_loss: 0.1439986610832033\n",
      "best loss: 0.1439986610832033\n",
      "*********************************\n",
      "epoch 193\n",
      "Train_loss: 2.901022582721479\n",
      "val_loss: 0.14415782191118773\n",
      "best loss: 0.1439986610832033\n",
      "*********************************\n",
      "epoch 194\n",
      "Train_loss: 2.897652988787032\n",
      "val_loss: 0.1439457084167477\n",
      "best loss: 0.1439457084167477\n",
      "*********************************\n",
      "epoch 195\n",
      "Train_loss: 2.8976529994786917\n",
      "val_loss: 0.14386252424204113\n",
      "best loss: 0.14386252424204113\n",
      "*********************************\n",
      "epoch 196\n",
      "Train_loss: 2.8951809389199212\n",
      "val_loss: 0.14400679669121486\n",
      "best loss: 0.14386252424204113\n",
      "*********************************\n",
      "epoch 197\n",
      "Train_loss: 2.8939901473378096\n",
      "val_loss: 0.14442426163969888\n",
      "best loss: 0.14386252424204113\n",
      "*********************************\n",
      "epoch 198\n",
      "Train_loss: 2.8936365978869247\n",
      "val_loss: 0.14406865073363387\n",
      "best loss: 0.14386252424204113\n",
      "*********************************\n",
      "epoch 199\n",
      "Train_loss: 2.8924675830441244\n",
      "val_loss: 0.14376552607137752\n",
      "best loss: 0.14376552607137752\n",
      "*********************************\n",
      "epoch 200\n",
      "Train_loss: 2.891364068963174\n",
      "val_loss: 0.14328620702331663\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 201\n",
      "Train_loss: 2.890691484666117\n",
      "val_loss: 0.14387157939685075\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 202\n",
      "Train_loss: 2.8901822661484187\n",
      "val_loss: 0.14417363085568372\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 203\n",
      "Train_loss: 2.8907731920589472\n",
      "val_loss: 0.14407862551842465\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 204\n",
      "Train_loss: 2.890004916663753\n",
      "val_loss: 0.14371303604841448\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 205\n",
      "Train_loss: 2.890316901807743\n",
      "val_loss: 0.1438198555821862\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 206\n",
      "Train_loss: 2.88704027229602\n",
      "val_loss: 0.1435595553536591\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 207\n",
      "Train_loss: 2.88957100354665\n",
      "val_loss: 0.14373666993679457\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 208\n",
      "Train_loss: 2.8871784142390005\n",
      "val_loss: 0.14402260490515048\n",
      "best loss: 0.14328620702331663\n",
      "*********************************\n",
      "epoch 209\n",
      "Train_loss: 2.8872915645210995\n",
      "val_loss: 0.14324169731068723\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 210\n",
      "Train_loss: 2.8861702621159226\n",
      "val_loss: 0.14396105848734125\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 211\n",
      "Train_loss: 2.88546817041501\n",
      "val_loss: 0.14336570947849328\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 212\n",
      "Train_loss: 2.886504027363872\n",
      "val_loss: 0.14338136445477706\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 213\n",
      "Train_loss: 2.8860942343472877\n",
      "val_loss: 0.14385469793342714\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 214\n",
      "Train_loss: 2.8828158193493527\n",
      "val_loss: 0.1441983395725668\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 215\n",
      "Train_loss: 2.880635193724775\n",
      "val_loss: 0.14413909901033692\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 216\n",
      "Train_loss: 2.882419648735161\n",
      "val_loss: 0.14366960165047168\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 217\n",
      "Train_loss: 2.88551900439467\n",
      "val_loss: 0.14335680870446063\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 218\n",
      "Train_loss: 2.8829012535632352\n",
      "val_loss: 0.14359439558996076\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 219\n",
      "Train_loss: 2.883293958214504\n",
      "val_loss: 0.14390365637159044\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "Epoch   121: reducing learning rate of group 0 to 4.0500e-05.\n",
      "epoch 220\n",
      "Train_loss: 2.8801731408808555\n",
      "val_loss: 0.1441219078723297\n",
      "best loss: 0.14324169731068723\n",
      "*********************************\n",
      "epoch 221\n",
      "Train_loss: 2.8690011430761753\n",
      "val_loss: 0.1427477966923397\n",
      "best loss: 0.1427477966923397\n",
      "*********************************\n",
      "epoch 222\n",
      "Train_loss: 2.868672126615988\n",
      "val_loss: 0.1429209231678911\n",
      "best loss: 0.1427477966923397\n",
      "*********************************\n",
      "epoch 223\n",
      "Train_loss: 2.8658001306921097\n",
      "val_loss: 0.1423883462699953\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 224\n",
      "Train_loss: 2.8668062078678833\n",
      "val_loss: 0.14256745629725498\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 225\n",
      "Train_loss: 2.866253638706788\n",
      "val_loss: 0.14310064720335783\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 226\n",
      "Train_loss: 2.8658445551595886\n",
      "val_loss: 0.14263943992317418\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 227\n",
      "Train_loss: 2.866053756165219\n",
      "val_loss: 0.1427329099126485\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 228\n",
      "Train_loss: 2.8642856454862344\n",
      "val_loss: 0.14285907131760872\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 229\n",
      "Train_loss: 2.865501185455412\n",
      "val_loss: 0.14275516581322506\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 230\n",
      "Train_loss: 2.8659117839966575\n",
      "val_loss: 0.142585875141228\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 231\n",
      "Train_loss: 2.864657558229296\n",
      "val_loss: 0.14297817409144206\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 232\n",
      "Train_loss: 2.8656262052815262\n",
      "val_loss: 0.14276514170695034\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 233\n",
      "Train_loss: 2.8654639726189832\n",
      "val_loss: 0.14299827839398685\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "Epoch   135: reducing learning rate of group 0 to 1.2150e-05.\n",
      "epoch 234\n",
      "Train_loss: 2.8643248293341506\n",
      "val_loss: 0.14263038468973588\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 235\n",
      "Train_loss: 2.8608454012244513\n",
      "val_loss: 0.14246846242049083\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 236\n",
      "Train_loss: 2.8611270198927645\n",
      "val_loss: 0.14258357353013393\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 237\n",
      "Train_loss: 2.8602608392102193\n",
      "val_loss: 0.14271326461468528\n",
      "best loss: 0.1423883462699953\n",
      "*********************************\n",
      "epoch 238\n",
      "Train_loss: 2.861396622494595\n",
      "val_loss: 0.14238297262430175\n",
      "best loss: 0.14238297262430175\n",
      "*********************************\n",
      "epoch 239\n",
      "Train_loss: 2.86027981075823\n",
      "val_loss: 0.1424586404401296\n",
      "best loss: 0.14238297262430175\n",
      "*********************************\n",
      "epoch 240\n",
      "Train_loss: 2.8612537528303656\n",
      "val_loss: 0.14256530846100574\n",
      "best loss: 0.14238297262430175\n",
      "*********************************\n",
      "epoch 241\n",
      "Train_loss: 2.8598662548817195\n",
      "val_loss: 0.1423599522456807\n",
      "best loss: 0.1423599522456807\n",
      "*********************************\n",
      "epoch 242\n",
      "Train_loss: 2.8600759144417465\n",
      "val_loss: 0.14245449549820868\n",
      "best loss: 0.1423599522456807\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 243\n",
      "Train_loss: 2.8584203519029887\n",
      "val_loss: 0.14222181763325192\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 244\n",
      "Train_loss: 2.857307589232596\n",
      "val_loss: 0.14245326942044553\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 245\n",
      "Train_loss: 2.8581434350718578\n",
      "val_loss: 0.14246692749857623\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 246\n",
      "Train_loss: 2.858590644781092\n",
      "val_loss: 0.14230715474934694\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 247\n",
      "Train_loss: 2.8602566469384465\n",
      "val_loss: 0.14260291153726076\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 248\n",
      "Train_loss: 2.8592946458469988\n",
      "val_loss: 0.14251788342416863\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 249\n",
      "Train_loss: 2.859008695245008\n",
      "val_loss: 0.14238680949245017\n",
      "best loss: 0.14222181763325192\n",
      "*********************************\n",
      "epoch 250\n",
      "Train_loss: 2.8597965598350696\n",
      "val_loss: 0.14210855277329393\n",
      "best loss: 0.14210855277329393\n",
      "*********************************\n",
      "epoch 251\n",
      "Train_loss: 2.8590946892035314\n",
      "val_loss: 0.1420770882239726\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 252\n",
      "Train_loss: 2.858023133980862\n",
      "val_loss: 0.14286873980221304\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 253\n",
      "Train_loss: 2.859393000422549\n",
      "val_loss: 0.1424006230333237\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 254\n",
      "Train_loss: 2.8575273268698753\n",
      "val_loss: 0.14236455606277282\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 255\n",
      "Train_loss: 2.8590454932254272\n",
      "val_loss: 0.14252724397504865\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 256\n",
      "Train_loss: 2.8591273346408244\n",
      "val_loss: 0.14244590051521164\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 257\n",
      "Train_loss: 2.8580006705257537\n",
      "val_loss: 0.14228797074833152\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 258\n",
      "Train_loss: 2.858693067269784\n",
      "val_loss: 0.14233416873591664\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 259\n",
      "Train_loss: 2.8579039046259807\n",
      "val_loss: 0.14253062301580116\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 260\n",
      "Train_loss: 2.8586101693682133\n",
      "val_loss: 0.14220631804641745\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 261\n",
      "Train_loss: 2.8580831722460283\n",
      "val_loss: 0.1423901895448354\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "Epoch   163: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch 262\n",
      "Train_loss: 2.8565420829674952\n",
      "val_loss: 0.14240001065829663\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 263\n",
      "Train_loss: 2.8567747878366943\n",
      "val_loss: 0.14234521810284922\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 264\n",
      "Train_loss: 2.8562838616194006\n",
      "val_loss: 0.14246876931564628\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 265\n",
      "Train_loss: 2.857872800904508\n",
      "val_loss: 0.14240108487660588\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 266\n",
      "Train_loss: 2.8580508837090464\n",
      "val_loss: 0.14237422566119137\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 267\n",
      "Train_loss: 2.855647666315353\n",
      "val_loss: 0.14254888669007526\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 268\n",
      "Train_loss: 2.8568309381791117\n",
      "val_loss: 0.1424359218941086\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 269\n",
      "Train_loss: 2.8587494666978612\n",
      "val_loss: 0.14232542001076895\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 270\n",
      "Train_loss: 2.856973327455937\n",
      "val_loss: 0.14236317589789477\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 271\n",
      "Train_loss: 2.8581149408597812\n",
      "val_loss: 0.14240292662383075\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 272\n",
      "Train_loss: 2.855810687120043\n",
      "val_loss: 0.1424302468759734\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 273\n",
      "Train_loss: 2.857062985146948\n",
      "val_loss: 0.14236378771647015\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 274\n",
      "Train_loss: 2.857349743093745\n",
      "val_loss: 0.1424601738868698\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 275\n",
      "Train_loss: 2.855477845216752\n",
      "val_loss: 0.14240016582900403\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 276\n",
      "Train_loss: 2.857311113639131\n",
      "val_loss: 0.14235043867592337\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 277\n",
      "Train_loss: 2.8563769633245766\n",
      "val_loss: 0.1421877462071199\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 278\n",
      "Train_loss: 2.8565091185027955\n",
      "val_loss: 0.14241750745129264\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 279\n",
      "Train_loss: 2.8576616898496465\n",
      "val_loss: 0.14239340960150298\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 280\n",
      "Train_loss: 2.8568044632542478\n",
      "val_loss: 0.14233662272033254\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 281\n",
      "Train_loss: 2.8570980057377207\n",
      "val_loss: 0.14230193885713588\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 282\n",
      "Train_loss: 2.857157070384242\n",
      "val_loss: 0.14252847454811507\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 283\n",
      "Train_loss: 2.8577011997090067\n",
      "val_loss: 0.14237606722119736\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 284\n",
      "Train_loss: 2.8553812954867652\n",
      "val_loss: 0.1423616401633102\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 285\n",
      "Train_loss: 2.855596029418062\n",
      "val_loss: 0.14215060471682273\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 286\n",
      "Train_loss: 2.8569172735303665\n",
      "val_loss: 0.14236424812472182\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 287\n",
      "Train_loss: 2.8581008171503006\n",
      "val_loss: 0.1422990217749898\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 288\n",
      "Train_loss: 2.8557575983329797\n",
      "val_loss: 0.14222964882868605\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 289\n",
      "Train_loss: 2.854967338455406\n",
      "val_loss: 0.1423852760677338\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 290\n",
      "Train_loss: 2.8558325682452157\n",
      "val_loss: 0.1423229631989409\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 291\n",
      "Train_loss: 2.8564229510066093\n",
      "val_loss: 0.14254412898938545\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 292\n",
      "Train_loss: 2.854984343335328\n",
      "val_loss: 0.14241474641241442\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 293\n",
      "Train_loss: 2.8548197887083306\n",
      "val_loss: 0.14244267722839163\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 294\n",
      "Train_loss: 2.8556629384488534\n",
      "val_loss: 0.14236685991551648\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 295\n",
      "Train_loss: 2.8535466480653793\n",
      "val_loss: 0.14222350778693557\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 296\n",
      "Train_loss: 2.8564107838788324\n",
      "val_loss: 0.14242395331193927\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 297\n",
      "Train_loss: 2.8565866387187357\n",
      "val_loss: 0.1424053840145247\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 298\n",
      "Train_loss: 2.8558559006261834\n",
      "val_loss: 0.14215505611823695\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "epoch 299\n",
      "Train_loss: 2.8561605440339912\n",
      "val_loss: 0.14228367280640156\n",
      "best loss: 0.1420770882239726\n",
      "*********************************\n",
      "fold 0 score: 0.14207708856503878\n",
      "fold: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train_loss: 15.187339897216582\n",
      "val_loss: 2.0077972624271223\n",
      "best loss: 2.0077972624271223\n",
      "*********************************\n",
      "epoch 1\n",
      "Train_loss: 6.43866029958777\n",
      "val_loss: 1.1907401582423127\n",
      "best loss: 1.1907401582423127\n",
      "*********************************\n",
      "epoch 2\n",
      "Train_loss: 5.9371331643295155\n",
      "val_loss: 1.1324756958095874\n",
      "best loss: 1.1324756958095874\n",
      "*********************************\n",
      "epoch 3\n",
      "Train_loss: 5.5751837001046605\n",
      "val_loss: 1.0392523747241842\n",
      "best loss: 1.0392523747241842\n",
      "*********************************\n",
      "epoch 4\n",
      "Train_loss: 5.363919622727346\n",
      "val_loss: 0.6175010417556518\n",
      "best loss: 0.6175010417556518\n",
      "*********************************\n",
      "epoch 5\n",
      "Train_loss: 5.338855033603209\n",
      "val_loss: 0.6468271130388062\n",
      "best loss: 0.6175010417556518\n",
      "*********************************\n",
      "epoch 6\n",
      "Train_loss: 5.452475249046611\n",
      "val_loss: 0.8456083096494381\n",
      "best loss: 0.6175010417556518\n",
      "*********************************\n",
      "epoch 7\n",
      "Train_loss: 5.494450397838508\n",
      "val_loss: 0.9173887478927604\n",
      "best loss: 0.6175010417556518\n",
      "*********************************\n",
      "epoch 8\n",
      "Train_loss: 5.398439205007697\n",
      "val_loss: 0.8445869532718103\n",
      "best loss: 0.6175010417556518\n",
      "*********************************\n",
      "epoch 9\n",
      "Train_loss: 5.265424360415959\n",
      "val_loss: 0.5612991979138752\n",
      "best loss: 0.5612991979138752\n",
      "*********************************\n",
      "epoch 10\n",
      "Train_loss: 5.0984546203717125\n",
      "val_loss: 0.5726097113324929\n",
      "best loss: 0.5612991979138752\n",
      "*********************************\n",
      "epoch 11\n",
      "Train_loss: 4.896420038550609\n",
      "val_loss: 0.4398762724015015\n",
      "best loss: 0.4398762724015015\n",
      "*********************************\n",
      "epoch 12\n",
      "Train_loss: 4.667699076852587\n",
      "val_loss: 0.4050288375730455\n",
      "best loss: 0.4050288375730455\n",
      "*********************************\n",
      "epoch 13\n",
      "Train_loss: 4.4319980359155\n",
      "val_loss: 0.3070961674337905\n",
      "best loss: 0.3070961674337905\n",
      "*********************************\n",
      "epoch 14\n",
      "Train_loss: 4.25503423145677\n",
      "val_loss: 0.28024614720461116\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 15\n",
      "Train_loss: 4.237479154183914\n",
      "val_loss: 0.29969673659859225\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 16\n",
      "Train_loss: 4.366134399468572\n",
      "val_loss: 0.37568575431484\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 17\n",
      "Train_loss: 4.552588248922683\n",
      "val_loss: 0.4583731625599958\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 18\n",
      "Train_loss: 4.671996356880884\n",
      "val_loss: 0.6789610274258403\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 19\n",
      "Train_loss: 4.679401085821536\n",
      "val_loss: 0.3816091657477062\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 20\n",
      "Train_loss: 4.600162912156461\n",
      "val_loss: 0.4138529092543407\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 21\n",
      "Train_loss: 4.4553578132907745\n",
      "val_loss: 0.3530292491439835\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 22\n",
      "Train_loss: 4.264478259391807\n",
      "val_loss: 0.29025753122351233\n",
      "best loss: 0.28024614720461116\n",
      "*********************************\n",
      "epoch 23\n",
      "Train_loss: 4.064638995422831\n",
      "val_loss: 0.23522182256593652\n",
      "best loss: 0.23522182256593652\n",
      "*********************************\n",
      "epoch 24\n",
      "Train_loss: 3.902421839203609\n",
      "val_loss: 0.21919912692905064\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 25\n",
      "Train_loss: 3.8846799994858476\n",
      "val_loss: 0.23142362827408444\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 26\n",
      "Train_loss: 4.006020298402774\n",
      "val_loss: 0.2686051855205821\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 27\n",
      "Train_loss: 4.183606063776094\n",
      "val_loss: 0.3434946300572751\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 28\n",
      "Train_loss: 4.335290828228288\n",
      "val_loss: 0.40229616482502595\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 29\n",
      "Train_loss: 4.393373133044533\n",
      "val_loss: 0.31125134012091654\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 30\n",
      "Train_loss: 4.3509168668466796\n",
      "val_loss: 0.30526364682141166\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 31\n",
      "Train_loss: 4.240426362776069\n",
      "val_loss: 0.2884072178643135\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 32\n",
      "Train_loss: 4.065265787240712\n",
      "val_loss: 0.2495296598034498\n",
      "best loss: 0.21919912692905064\n",
      "*********************************\n",
      "epoch 33\n",
      "Train_loss: 3.870895555728503\n",
      "val_loss: 0.20772000197654492\n",
      "best loss: 0.20772000197654492\n",
      "*********************************\n",
      "epoch 34\n",
      "Train_loss: 3.7172940494891002\n",
      "val_loss: 0.1937016355610448\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 35\n",
      "Train_loss: 3.6981043397324482\n",
      "val_loss: 0.20236278928555046\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 36\n",
      "Train_loss: 3.8186838684036757\n",
      "val_loss: 0.24473067453067218\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 37\n",
      "Train_loss: 3.998410407133262\n",
      "val_loss: 0.33305370195273715\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 38\n",
      "Train_loss: 4.147857421947629\n",
      "val_loss: 0.33405680987878117\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 39\n",
      "Train_loss: 4.22458830543134\n",
      "val_loss: 0.2833155344351858\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 40\n",
      "Train_loss: 4.202657605916705\n",
      "val_loss: 0.27179468432113546\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 41\n",
      "Train_loss: 4.096579583985695\n",
      "val_loss: 0.28688164425159063\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 42\n",
      "Train_loss: 3.92028088085907\n",
      "val_loss: 0.22097888475695904\n",
      "best loss: 0.1937016355610448\n",
      "*********************************\n",
      "epoch 43\n",
      "Train_loss: 3.7302183378780813\n",
      "val_loss: 0.18873605594529252\n",
      "best loss: 0.18873605594529252\n",
      "*********************************\n",
      "epoch 44\n",
      "Train_loss: 3.594570296507399\n",
      "val_loss: 0.18044520405369024\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 45\n",
      "Train_loss: 3.5766620994893805\n",
      "val_loss: 0.18569120379099124\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 46\n",
      "Train_loss: 3.6875401349109516\n",
      "val_loss: 0.22369376155735357\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 47\n",
      "Train_loss: 3.8667825583776145\n",
      "val_loss: 0.3206738246907036\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 48\n",
      "Train_loss: 4.0251412011139305\n",
      "val_loss: 0.3070806980122392\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 49\n",
      "Train_loss: 4.094321188136839\n",
      "val_loss: 0.25403381208236\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 50\n",
      "Train_loss: 4.08386425929787\n",
      "val_loss: 0.2790028253667487\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 51\n",
      "Train_loss: 3.981525216977494\n",
      "val_loss: 0.23607473234669268\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 52\n",
      "Train_loss: 3.815805473478729\n",
      "val_loss: 0.21981410791964265\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 53\n",
      "Train_loss: 3.633526051117963\n",
      "val_loss: 0.18293090919368182\n",
      "best loss: 0.18044520405369024\n",
      "*********************************\n",
      "epoch 54\n",
      "Train_loss: 3.499386006759493\n",
      "val_loss: 0.17221035269172136\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 55\n",
      "Train_loss: 3.4848934037043287\n",
      "val_loss: 0.17628866206712746\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 56\n",
      "Train_loss: 3.5932153351912817\n",
      "val_loss: 0.20076404461134206\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 57\n",
      "Train_loss: 3.7610982768829566\n",
      "val_loss: 0.24535578185877085\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 58\n",
      "Train_loss: 3.918763990906171\n",
      "val_loss: 0.2842950106798233\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 59\n",
      "Train_loss: 4.010438185430426\n",
      "val_loss: 0.2592979074693701\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 60\n",
      "Train_loss: 4.002000353425612\n",
      "val_loss: 0.2468405416676157\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 61\n",
      "Train_loss: 3.907288002786137\n",
      "val_loss: 0.23320967294963527\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62\n",
      "Train_loss: 3.735045729237546\n",
      "val_loss: 0.19894762074535124\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 63\n",
      "Train_loss: 3.561178112964405\n",
      "val_loss: 0.1758793890390063\n",
      "best loss: 0.17221035269172136\n",
      "*********************************\n",
      "epoch 64\n",
      "Train_loss: 3.430753696217084\n",
      "val_loss: 0.16612065361283654\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 65\n",
      "Train_loss: 3.4160441127795425\n",
      "val_loss: 0.17005123884344636\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 66\n",
      "Train_loss: 3.520593997009253\n",
      "val_loss: 0.19635929395936283\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 67\n",
      "Train_loss: 3.6893859889975826\n",
      "val_loss: 0.22030606937441066\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 68\n",
      "Train_loss: 3.8318012042122365\n",
      "val_loss: 0.2507165095136677\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 69\n",
      "Train_loss: 3.929007780887686\n",
      "val_loss: 0.23803196753292707\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 70\n",
      "Train_loss: 3.940554405723337\n",
      "val_loss: 0.24149590287252132\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 71\n",
      "Train_loss: 3.8344550817312273\n",
      "val_loss: 0.22285727040212647\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 72\n",
      "Train_loss: 3.6725550659814012\n",
      "val_loss: 0.1909312131526008\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 73\n",
      "Train_loss: 3.5013332092214804\n",
      "val_loss: 0.17009173577358258\n",
      "best loss: 0.16612065361283654\n",
      "*********************************\n",
      "epoch 74\n",
      "Train_loss: 3.373138579191465\n",
      "val_loss: 0.16224805295658173\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 75\n",
      "Train_loss: 3.3556418841406277\n",
      "val_loss: 0.16625334314125548\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 76\n",
      "Train_loss: 3.4558381600196406\n",
      "val_loss: 0.19149189741187708\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 77\n",
      "Train_loss: 3.6221518710989153\n",
      "val_loss: 0.21940498699995664\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 78\n",
      "Train_loss: 3.7790627273794275\n",
      "val_loss: 0.25522235881393673\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 79\n",
      "Train_loss: 3.8732339759735854\n",
      "val_loss: 0.22456691982790522\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 80\n",
      "Train_loss: 3.8778092950031637\n",
      "val_loss: 0.22131482927024249\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 81\n",
      "Train_loss: 3.7661436380933258\n",
      "val_loss: 0.20311475474629065\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 82\n",
      "Train_loss: 3.609070646288894\n",
      "val_loss: 0.18254741111673056\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 83\n",
      "Train_loss: 3.4394624199908654\n",
      "val_loss: 0.1678122017907326\n",
      "best loss: 0.16224805295658173\n",
      "*********************************\n",
      "epoch 84\n",
      "Train_loss: 3.3207137374448075\n",
      "val_loss: 0.1582334053096533\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 85\n",
      "Train_loss: 3.3030992578647407\n",
      "val_loss: 0.16177710922592026\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 86\n",
      "Train_loss: 3.402265169063984\n",
      "val_loss: 0.17709769808785084\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 87\n",
      "Train_loss: 3.5672020348325333\n",
      "val_loss: 0.22386020477923227\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 88\n",
      "Train_loss: 3.7334935490334424\n",
      "val_loss: 0.241098449601913\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 89\n",
      "Train_loss: 3.8123573943739713\n",
      "val_loss: 0.22096322684616074\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 90\n",
      "Train_loss: 3.813439545774117\n",
      "val_loss: 0.23095805232717193\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 91\n",
      "Train_loss: 3.717164724823052\n",
      "val_loss: 0.20389035355598253\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 92\n",
      "Train_loss: 3.568273541471458\n",
      "val_loss: 0.18498034499019336\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 93\n",
      "Train_loss: 3.398820480438417\n",
      "val_loss: 0.16377239510711275\n",
      "best loss: 0.1582334053096533\n",
      "*********************************\n",
      "epoch 94\n",
      "Train_loss: 3.274714494077615\n",
      "val_loss: 0.15669050110298\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 95\n",
      "Train_loss: 3.2602425931627783\n",
      "val_loss: 0.1599070069970686\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 96\n",
      "Train_loss: 3.3521361294260004\n",
      "val_loss: 0.1779961698109296\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 97\n",
      "Train_loss: 3.5224956620529153\n",
      "val_loss: 0.20286717488932082\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 98\n",
      "Train_loss: 3.682311293311131\n",
      "val_loss: 0.24509162423183398\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 99\n",
      "Train_loss: 3.7700572067849585\n",
      "val_loss: 0.22166872164540868\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 100\n",
      "Train_loss: 3.779518984582623\n",
      "val_loss: 0.21125619746274227\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 101\n",
      "Train_loss: 3.7811240739420198\n",
      "val_loss: 0.24048024572261162\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 102\n",
      "Train_loss: 3.7682203132567573\n",
      "val_loss: 0.2307830284201254\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 103\n",
      "Train_loss: 3.7542127462876356\n",
      "val_loss: 0.2468219917719116\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 104\n",
      "Train_loss: 3.7572897256133304\n",
      "val_loss: 0.20719045547247472\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 105\n",
      "Train_loss: 3.75609087104887\n",
      "val_loss: 0.21896242657507203\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 106\n",
      "Train_loss: 3.7452628948557773\n",
      "val_loss: 0.22250292586264933\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 107\n",
      "Train_loss: 3.747711398101319\n",
      "val_loss: 0.22101309235543284\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 108\n",
      "Train_loss: 3.7202569760715263\n",
      "val_loss: 0.2687116405417098\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 109\n",
      "Train_loss: 3.7217803707841672\n",
      "val_loss: 0.20747700782469464\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 110\n",
      "Train_loss: 3.719402447474513\n",
      "val_loss: 0.2176993208022042\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 111\n",
      "Train_loss: 3.716856282801383\n",
      "val_loss: 0.23056720429953484\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 112\n",
      "Train_loss: 3.7137157584276252\n",
      "val_loss: 0.23345786212220732\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 113\n",
      "Train_loss: 3.7072258586686506\n",
      "val_loss: 0.22904683634823828\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 114\n",
      "Train_loss: 3.695403769468917\n",
      "val_loss: 0.2139805841761731\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 115\n",
      "Train_loss: 3.6901913377975477\n",
      "val_loss: 0.20645583140411797\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 116\n",
      "Train_loss: 3.690013165923686\n",
      "val_loss: 0.20634905098475848\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 117\n",
      "Train_loss: 3.671011775040844\n",
      "val_loss: 0.25259351321263895\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 118\n",
      "Train_loss: 3.658612410794428\n",
      "val_loss: 0.22980770691189942\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 119\n",
      "Train_loss: 3.658958320361503\n",
      "val_loss: 0.1974120873705502\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 120\n",
      "Train_loss: 3.6692763021877295\n",
      "val_loss: 0.20761476310009808\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 121\n",
      "Train_loss: 3.642130050230817\n",
      "val_loss: 0.21485573180394635\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 122\n",
      "Train_loss: 3.645757769939162\n",
      "val_loss: 0.20593395166799747\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 123\n",
      "Train_loss: 3.6388005685775275\n",
      "val_loss: 0.2242582830309904\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124\n",
      "Train_loss: 3.633802494217706\n",
      "val_loss: 0.19203692373925307\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 125\n",
      "Train_loss: 3.630469079915954\n",
      "val_loss: 0.21843350467914383\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 126\n",
      "Train_loss: 3.6256932629190906\n",
      "val_loss: 0.20209417671022356\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 127\n",
      "Train_loss: 3.6142574114049792\n",
      "val_loss: 0.22684569223032244\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 128\n",
      "Train_loss: 3.604625409407334\n",
      "val_loss: 0.2198043000860962\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 129\n",
      "Train_loss: 3.6130841918225065\n",
      "val_loss: 0.19119045847278746\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 130\n",
      "Train_loss: 3.596791389202567\n",
      "val_loss: 0.18895833158310907\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 131\n",
      "Train_loss: 3.593748250417473\n",
      "val_loss: 0.20845233089944287\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 132\n",
      "Train_loss: 3.5946743132820487\n",
      "val_loss: 0.21384376266823293\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 133\n",
      "Train_loss: 3.5936029621093324\n",
      "val_loss: 0.23147747447816822\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 134\n",
      "Train_loss: 3.5893739302092755\n",
      "val_loss: 0.1929461317788042\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 135\n",
      "Train_loss: 3.571051766653386\n",
      "val_loss: 0.18930057039433915\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 136\n",
      "Train_loss: 3.5619614445201746\n",
      "val_loss: 0.19834430628065164\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 137\n",
      "Train_loss: 3.5624851684092036\n",
      "val_loss: 0.2021402033359642\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 138\n",
      "Train_loss: 3.5512962002214072\n",
      "val_loss: 0.2145058365989487\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 139\n",
      "Train_loss: 3.5660350455626784\n",
      "val_loss: 0.18623010077613292\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 140\n",
      "Train_loss: 3.5489795678309517\n",
      "val_loss: 0.20863503236523112\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 141\n",
      "Train_loss: 3.5513038072378875\n",
      "val_loss: 0.19810652951409197\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 142\n",
      "Train_loss: 3.533166227078294\n",
      "val_loss: 0.19982293835576082\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 143\n",
      "Train_loss: 3.5409212379485533\n",
      "val_loss: 0.2004652233578097\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 144\n",
      "Train_loss: 3.5395038718023546\n",
      "val_loss: 0.17960227355458366\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 145\n",
      "Train_loss: 3.5233483457322112\n",
      "val_loss: 0.18109854503538317\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 146\n",
      "Train_loss: 3.5173733856030873\n",
      "val_loss: 0.18835317380941666\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 147\n",
      "Train_loss: 3.523614193528773\n",
      "val_loss: 0.20306965966534585\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 148\n",
      "Train_loss: 3.512286217414505\n",
      "val_loss: 0.2030501734143718\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 149\n",
      "Train_loss: 3.5185705871638624\n",
      "val_loss: 0.1861388210904313\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 150\n",
      "Train_loss: 3.503429886323739\n",
      "val_loss: 0.18017583547792843\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 151\n",
      "Train_loss: 3.504473496913711\n",
      "val_loss: 0.1997245975885916\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 152\n",
      "Train_loss: 3.5077425000846914\n",
      "val_loss: 0.20743513806032648\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 153\n",
      "Train_loss: 3.511287238890783\n",
      "val_loss: 0.20781587019252953\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 154\n",
      "Train_loss: 3.497037224864669\n",
      "val_loss: 0.18387939217779314\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "Epoch    56: reducing learning rate of group 0 to 1.5000e-03.\n",
      "epoch 155\n",
      "Train_loss: 3.4973092184268655\n",
      "val_loss: 0.1933409927615186\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 156\n",
      "Train_loss: 3.231661493523111\n",
      "val_loss: 0.1601588894900291\n",
      "best loss: 0.15669050110298\n",
      "*********************************\n",
      "epoch 157\n",
      "Train_loss: 3.187986528163172\n",
      "val_loss: 0.15625837275506727\n",
      "best loss: 0.15625837275506727\n",
      "*********************************\n",
      "epoch 158\n",
      "Train_loss: 3.1664951862976958\n",
      "val_loss: 0.15678790757760722\n",
      "best loss: 0.15625837275506727\n",
      "*********************************\n",
      "epoch 159\n",
      "Train_loss: 3.152509385075186\n",
      "val_loss: 0.15439792588423681\n",
      "best loss: 0.15439792588423681\n",
      "*********************************\n",
      "epoch 160\n",
      "Train_loss: 3.1471059705752293\n",
      "val_loss: 0.1565745238306203\n",
      "best loss: 0.15439792588423681\n",
      "*********************************\n",
      "epoch 161\n",
      "Train_loss: 3.1368954285266497\n",
      "val_loss: 0.16858012632409947\n",
      "best loss: 0.15439792588423681\n",
      "*********************************\n",
      "epoch 162\n",
      "Train_loss: 3.129474125096646\n",
      "val_loss: 0.15482407729286785\n",
      "best loss: 0.15439792588423681\n",
      "*********************************\n",
      "epoch 163\n",
      "Train_loss: 3.1264511020101917\n",
      "val_loss: 0.1565869539456016\n",
      "best loss: 0.15439792588423681\n",
      "*********************************\n",
      "epoch 164\n",
      "Train_loss: 3.114101083185408\n",
      "val_loss: 0.15233039082596614\n",
      "best loss: 0.15233039082596614\n",
      "*********************************\n",
      "epoch 165\n",
      "Train_loss: 3.115887645204766\n",
      "val_loss: 0.15441664194399435\n",
      "best loss: 0.15233039082596614\n",
      "*********************************\n",
      "epoch 166\n",
      "Train_loss: 3.1056662708488325\n",
      "val_loss: 0.15549888069059659\n",
      "best loss: 0.15233039082596614\n",
      "*********************************\n",
      "epoch 167\n",
      "Train_loss: 3.1067243394796025\n",
      "val_loss: 0.15406566065624022\n",
      "best loss: 0.15233039082596614\n",
      "*********************************\n",
      "epoch 168\n",
      "Train_loss: 3.09568199270094\n",
      "val_loss: 0.1571106629141247\n",
      "best loss: 0.15233039082596614\n",
      "*********************************\n",
      "epoch 169\n",
      "Train_loss: 3.093789103684114\n",
      "val_loss: 0.1504762384928452\n",
      "best loss: 0.1504762384928452\n",
      "*********************************\n",
      "epoch 170\n",
      "Train_loss: 3.088164003649238\n",
      "val_loss: 0.15336539097299326\n",
      "best loss: 0.1504762384928452\n",
      "*********************************\n",
      "epoch 171\n",
      "Train_loss: 3.091569477159373\n",
      "val_loss: 0.15231535917414252\n",
      "best loss: 0.1504762384928452\n",
      "*********************************\n",
      "epoch 172\n",
      "Train_loss: 3.078845477814126\n",
      "val_loss: 0.1534500655834991\n",
      "best loss: 0.1504762384928452\n",
      "*********************************\n",
      "epoch 173\n",
      "Train_loss: 3.084677662708897\n",
      "val_loss: 0.15523748982644345\n",
      "best loss: 0.1504762384928452\n",
      "*********************************\n",
      "epoch 174\n",
      "Train_loss: 3.0760896484664237\n",
      "val_loss: 0.14949539444619964\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 175\n",
      "Train_loss: 3.0714096500330546\n",
      "val_loss: 0.15124845901052275\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 176\n",
      "Train_loss: 3.0685047390877784\n",
      "val_loss: 0.15147870983310624\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 177\n",
      "Train_loss: 3.0676567644275674\n",
      "val_loss: 0.15140262675503813\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 178\n",
      "Train_loss: 3.064577812362962\n",
      "val_loss: 0.15403789619824892\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 179\n",
      "Train_loss: 3.063393592516405\n",
      "val_loss: 0.1506574038858517\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 180\n",
      "Train_loss: 3.05815259464253\n",
      "val_loss: 0.1509080597643562\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 181\n",
      "Train_loss: 3.054377442842924\n",
      "val_loss: 0.15285563584701437\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 182\n",
      "Train_loss: 3.0532330307257385\n",
      "val_loss: 0.1583694691328358\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 183\n",
      "Train_loss: 3.0493711143219087\n",
      "val_loss: 0.15657867115268875\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 184\n",
      "Train_loss: 3.044787252846028\n",
      "val_loss: 0.1501102214511245\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    86: reducing learning rate of group 0 to 4.5000e-04.\n",
      "epoch 185\n",
      "Train_loss: 3.048828372263581\n",
      "val_loss: 0.14957915184981993\n",
      "best loss: 0.14949539444619964\n",
      "*********************************\n",
      "epoch 186\n",
      "Train_loss: 2.950798740927688\n",
      "val_loss: 0.1437985534969936\n",
      "best loss: 0.1437985534969936\n",
      "*********************************\n",
      "epoch 187\n",
      "Train_loss: 2.935612871187617\n",
      "val_loss: 0.14426397123452384\n",
      "best loss: 0.1437985534969936\n",
      "*********************************\n",
      "epoch 188\n",
      "Train_loss: 2.9298961098130634\n",
      "val_loss: 0.1444991353007665\n",
      "best loss: 0.1437985534969936\n",
      "*********************************\n",
      "epoch 189\n",
      "Train_loss: 2.9294280470373764\n",
      "val_loss: 0.14306345863874417\n",
      "best loss: 0.14306345863874417\n",
      "*********************************\n",
      "epoch 190\n",
      "Train_loss: 2.9232219130156407\n",
      "val_loss: 0.1429847620889495\n",
      "best loss: 0.1429847620889495\n",
      "*********************************\n",
      "epoch 191\n",
      "Train_loss: 2.922013823581974\n",
      "val_loss: 0.1436609558341665\n",
      "best loss: 0.1429847620889495\n",
      "*********************************\n",
      "epoch 192\n",
      "Train_loss: 2.9190638809066445\n",
      "val_loss: 0.14342533213236922\n",
      "best loss: 0.1429847620889495\n",
      "*********************************\n",
      "epoch 193\n",
      "Train_loss: 2.91678980222034\n",
      "val_loss: 0.1438083726116852\n",
      "best loss: 0.1429847620889495\n",
      "*********************************\n",
      "epoch 194\n",
      "Train_loss: 2.917904789934816\n",
      "val_loss: 0.14255110039099744\n",
      "best loss: 0.14255110039099744\n",
      "*********************************\n",
      "epoch 195\n",
      "Train_loss: 2.912651394372045\n",
      "val_loss: 0.14230918708291923\n",
      "best loss: 0.14230918708291923\n",
      "*********************************\n",
      "epoch 196\n",
      "Train_loss: 2.9124065402094517\n",
      "val_loss: 0.1433572193250023\n",
      "best loss: 0.14230918708291923\n",
      "*********************************\n",
      "epoch 197\n",
      "Train_loss: 2.908045319604046\n",
      "val_loss: 0.14492482445976443\n",
      "best loss: 0.14230918708291923\n",
      "*********************************\n",
      "epoch 198\n",
      "Train_loss: 2.911788364047885\n",
      "val_loss: 0.14366954512511082\n",
      "best loss: 0.14230918708291923\n",
      "*********************************\n",
      "epoch 199\n",
      "Train_loss: 2.907545351285926\n",
      "val_loss: 0.14296006575251138\n",
      "best loss: 0.14230918708291923\n",
      "*********************************\n",
      "epoch 200\n",
      "Train_loss: 2.9049927011250496\n",
      "val_loss: 0.14229614695857107\n",
      "best loss: 0.14229614695857107\n",
      "*********************************\n",
      "epoch 201\n",
      "Train_loss: 2.905373256318835\n",
      "val_loss: 0.14491714983172865\n",
      "best loss: 0.14229614695857107\n",
      "*********************************\n",
      "epoch 202\n",
      "Train_loss: 2.902685436707916\n",
      "val_loss: 0.1445305838329941\n",
      "best loss: 0.14229614695857107\n",
      "*********************************\n",
      "epoch 203\n",
      "Train_loss: 2.9028847833745512\n",
      "val_loss: 0.1453423770536891\n",
      "best loss: 0.14229614695857107\n",
      "*********************************\n",
      "epoch 204\n",
      "Train_loss: 2.900882521100943\n",
      "val_loss: 0.1430812550301288\n",
      "best loss: 0.14229614695857107\n",
      "*********************************\n",
      "epoch 205\n",
      "Train_loss: 2.898090543762943\n",
      "val_loss: 0.14198152317163173\n",
      "best loss: 0.14198152317163173\n",
      "*********************************\n",
      "epoch 206\n",
      "Train_loss: 2.8986501853491955\n",
      "val_loss: 0.14396744946598253\n",
      "best loss: 0.14198152317163173\n",
      "*********************************\n",
      "epoch 207\n",
      "Train_loss: 2.898304989791878\n",
      "val_loss: 0.1436966966003051\n",
      "best loss: 0.14198152317163173\n",
      "*********************************\n",
      "epoch 208\n",
      "Train_loss: 2.8951100419534264\n",
      "val_loss: 0.14351230843118734\n",
      "best loss: 0.14198152317163173\n",
      "*********************************\n",
      "epoch 209\n",
      "Train_loss: 2.8927739971962\n",
      "val_loss: 0.14189761229677444\n",
      "best loss: 0.14189761229677444\n",
      "*********************************\n",
      "epoch 210\n",
      "Train_loss: 2.8966229791882587\n",
      "val_loss: 0.142611080671933\n",
      "best loss: 0.14189761229677444\n",
      "*********************************\n",
      "epoch 211\n",
      "Train_loss: 2.891924256057555\n",
      "val_loss: 0.14229691685874915\n",
      "best loss: 0.14189761229677444\n",
      "*********************************\n",
      "epoch 212\n",
      "Train_loss: 2.891700225853248\n",
      "val_loss: 0.14199625136217625\n",
      "best loss: 0.14189761229677444\n",
      "*********************************\n",
      "epoch 213\n",
      "Train_loss: 2.886883670688715\n",
      "val_loss: 0.14303247281119189\n",
      "best loss: 0.14189761229677444\n",
      "*********************************\n",
      "epoch 214\n",
      "Train_loss: 2.886755327419691\n",
      "val_loss: 0.1414838915317475\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 215\n",
      "Train_loss: 2.889714906094633\n",
      "val_loss: 0.14188472851498124\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 216\n",
      "Train_loss: 2.8865382977001843\n",
      "val_loss: 0.14207433058411414\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 217\n",
      "Train_loss: 2.8855079342123684\n",
      "val_loss: 0.14318633070512501\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 218\n",
      "Train_loss: 2.883634199512472\n",
      "val_loss: 0.14366862409531794\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 219\n",
      "Train_loss: 2.8830321503185905\n",
      "val_loss: 0.1416675139594706\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 220\n",
      "Train_loss: 2.8845744688471244\n",
      "val_loss: 0.14176154553550516\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 221\n",
      "Train_loss: 2.8809988188692235\n",
      "val_loss: 0.14156979616294174\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 222\n",
      "Train_loss: 2.878840626077789\n",
      "val_loss: 0.14233127701972487\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 223\n",
      "Train_loss: 2.8828267834051156\n",
      "val_loss: 0.1436134012178376\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 224\n",
      "Train_loss: 2.881874389671825\n",
      "val_loss: 0.14184085747453695\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "Epoch   126: reducing learning rate of group 0 to 1.3500e-04.\n",
      "epoch 225\n",
      "Train_loss: 2.8779394870857584\n",
      "val_loss: 0.14155276940413983\n",
      "best loss: 0.1414838915317475\n",
      "*********************************\n",
      "epoch 226\n",
      "Train_loss: 2.844807826531626\n",
      "val_loss: 0.14031007106778398\n",
      "best loss: 0.14031007106778398\n",
      "*********************************\n",
      "epoch 227\n",
      "Train_loss: 2.8393795358432063\n",
      "val_loss: 0.1406684127281375\n",
      "best loss: 0.14031007106778398\n",
      "*********************************\n",
      "epoch 228\n",
      "Train_loss: 2.8377256106736746\n",
      "val_loss: 0.14119304458053508\n",
      "best loss: 0.14031007106778398\n",
      "*********************************\n",
      "epoch 229\n",
      "Train_loss: 2.836698870691387\n",
      "val_loss: 0.14010144421650386\n",
      "best loss: 0.14010144421650386\n",
      "*********************************\n",
      "epoch 230\n",
      "Train_loss: 2.832056409785232\n",
      "val_loss: 0.13964492422250127\n",
      "best loss: 0.13964492422250127\n",
      "*********************************\n",
      "epoch 231\n",
      "Train_loss: 2.8356030591066026\n",
      "val_loss: 0.1397157971606108\n",
      "best loss: 0.13964492422250127\n",
      "*********************************\n",
      "epoch 232\n",
      "Train_loss: 2.8352566090985416\n",
      "val_loss: 0.13969707984842233\n",
      "best loss: 0.13964492422250127\n",
      "*********************************\n",
      "epoch 233\n",
      "Train_loss: 2.833348665541332\n",
      "val_loss: 0.14024794421844355\n",
      "best loss: 0.13964492422250127\n",
      "*********************************\n",
      "epoch 234\n",
      "Train_loss: 2.833531197835493\n",
      "val_loss: 0.13973021487744652\n",
      "best loss: 0.13964492422250127\n",
      "*********************************\n",
      "epoch 235\n",
      "Train_loss: 2.8312110042123395\n",
      "val_loss: 0.13898775364179627\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 236\n",
      "Train_loss: 2.832528577980234\n",
      "val_loss: 0.14083638890588074\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 237\n",
      "Train_loss: 2.8313544928340395\n",
      "val_loss: 0.13983222844625826\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 238\n",
      "Train_loss: 2.8298551801346146\n",
      "val_loss: 0.14024778989673775\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 239\n",
      "Train_loss: 2.8293563891849134\n",
      "val_loss: 0.14018888467278337\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 240\n",
      "Train_loss: 2.8293780868515115\n",
      "val_loss: 0.13943292498948823\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 241\n",
      "Train_loss: 2.83156046045594\n",
      "val_loss: 0.13960534547491701\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 242\n",
      "Train_loss: 2.8297328438967715\n",
      "val_loss: 0.13960120330778264\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 243\n",
      "Train_loss: 2.829242043372456\n",
      "val_loss: 0.14016188219047263\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 244\n",
      "Train_loss: 2.8291873170454775\n",
      "val_loss: 0.14042235772519987\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 245\n",
      "Train_loss: 2.8277823098374455\n",
      "val_loss: 0.13965903863030765\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "Epoch   147: reducing learning rate of group 0 to 4.0500e-05.\n",
      "epoch 246\n",
      "Train_loss: 2.8276740334081625\n",
      "val_loss: 0.13975399160068908\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 247\n",
      "Train_loss: 2.816292212141058\n",
      "val_loss: 0.13908025573300034\n",
      "best loss: 0.13898775364179627\n",
      "*********************************\n",
      "epoch 248\n",
      "Train_loss: 2.81540187179735\n",
      "val_loss: 0.13887669476793232\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 249\n",
      "Train_loss: 2.813852254088871\n",
      "val_loss: 0.13937862096114226\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 250\n",
      "Train_loss: 2.815270596917017\n",
      "val_loss: 0.13895753648705994\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 251\n",
      "Train_loss: 2.811958029205576\n",
      "val_loss: 0.1389443422914336\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 252\n",
      "Train_loss: 2.8113575736234866\n",
      "val_loss: 0.13897394799939644\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 253\n",
      "Train_loss: 2.8134979665441224\n",
      "val_loss: 0.13909544321719192\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 254\n",
      "Train_loss: 2.813582167279463\n",
      "val_loss: 0.13911492328098998\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 255\n",
      "Train_loss: 2.813155179678516\n",
      "val_loss: 0.139203438594806\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 256\n",
      "Train_loss: 2.8145786656935976\n",
      "val_loss: 0.1390130675477724\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 257\n",
      "Train_loss: 2.812232502616571\n",
      "val_loss: 0.13902687229732058\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 258\n",
      "Train_loss: 2.8134300435803272\n",
      "val_loss: 0.1390632277344807\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "Epoch   160: reducing learning rate of group 0 to 1.2150e-05.\n",
      "epoch 259\n",
      "Train_loss: 2.810257172043088\n",
      "val_loss: 0.13912796450472273\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 260\n",
      "Train_loss: 2.8083274907757176\n",
      "val_loss: 0.1389332979529692\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 261\n",
      "Train_loss: 2.805978182230778\n",
      "val_loss: 0.13901383268104597\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 262\n",
      "Train_loss: 2.8081239729045047\n",
      "val_loss: 0.13916294005825208\n",
      "best loss: 0.13887669476793232\n",
      "*********************************\n",
      "epoch 263\n",
      "Train_loss: 2.8093557170769397\n",
      "val_loss: 0.13885905274270102\n",
      "best loss: 0.13885905274270102\n",
      "*********************************\n",
      "epoch 264\n",
      "Train_loss: 2.8075360688526962\n",
      "val_loss: 0.1387946227219261\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 265\n",
      "Train_loss: 2.808466709289523\n",
      "val_loss: 0.13887163247855336\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 266\n",
      "Train_loss: 2.807442033068803\n",
      "val_loss: 0.13889172702166672\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 267\n",
      "Train_loss: 2.8077967000606465\n",
      "val_loss: 0.13904374635757938\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 268\n",
      "Train_loss: 2.806029357692509\n",
      "val_loss: 0.13883818990228314\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 269\n",
      "Train_loss: 2.806247596306125\n",
      "val_loss: 0.1388302112253477\n",
      "best loss: 0.1387946227219261\n",
      "*********************************\n",
      "epoch 270\n",
      "Train_loss: 2.8064995589976585\n",
      "val_loss: 0.13878511279617659\n",
      "best loss: 0.13878511279617659\n",
      "*********************************\n",
      "epoch 271\n",
      "Train_loss: 2.807606074374536\n",
      "val_loss: 0.13873847892562532\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 272\n",
      "Train_loss: 2.8073009837814364\n",
      "val_loss: 0.138777291266273\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 273\n",
      "Train_loss: 2.808026961306917\n",
      "val_loss: 0.13893667426871192\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 274\n",
      "Train_loss: 2.806309077039659\n",
      "val_loss: 0.13897087880199926\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 275\n",
      "Train_loss: 2.8078820502683883\n",
      "val_loss: 0.13903438744559501\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 276\n",
      "Train_loss: 2.8052899828354345\n",
      "val_loss: 0.13882760459034182\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 277\n",
      "Train_loss: 2.8054016603900815\n",
      "val_loss: 0.1388131848680689\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 278\n",
      "Train_loss: 2.8056857644687714\n",
      "val_loss: 0.13896980904252879\n",
      "best loss: 0.13873847892562532\n",
      "*********************************\n",
      "epoch 279\n",
      "Train_loss: 2.806482965802482\n",
      "val_loss: 0.1386930727435857\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 280\n",
      "Train_loss: 2.803955073873335\n",
      "val_loss: 0.13869752026290358\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 281\n",
      "Train_loss: 2.8061021342038606\n",
      "val_loss: 0.1387344894002635\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 282\n",
      "Train_loss: 2.8063199291029948\n",
      "val_loss: 0.138697061872204\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 283\n",
      "Train_loss: 2.806439375767438\n",
      "val_loss: 0.13903899120228305\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 284\n",
      "Train_loss: 2.8064708904484346\n",
      "val_loss: 0.13873679058570454\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 285\n",
      "Train_loss: 2.8067362906719544\n",
      "val_loss: 0.13890676157353501\n",
      "best loss: 0.1386930727435857\n",
      "*********************************\n",
      "epoch 286\n",
      "Train_loss: 2.8045857203408286\n",
      "val_loss: 0.13868294930335995\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 287\n",
      "Train_loss: 2.805937981877294\n",
      "val_loss: 0.1388633496608969\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 288\n",
      "Train_loss: 2.8057169326155726\n",
      "val_loss: 0.1391554230440607\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 289\n",
      "Train_loss: 2.8074713083660376\n",
      "val_loss: 0.13899757571258775\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "Epoch   191: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch 290\n",
      "Train_loss: 2.8056329972192002\n",
      "val_loss: 0.13879431734057698\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 291\n",
      "Train_loss: 2.805338554878532\n",
      "val_loss: 0.13895293365222663\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 292\n",
      "Train_loss: 2.8048381045377178\n",
      "val_loss: 0.13899634536158328\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 293\n",
      "Train_loss: 2.8050253638288396\n",
      "val_loss: 0.13904650590247342\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 294\n",
      "Train_loss: 2.8071178576514746\n",
      "val_loss: 0.13888528255234392\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 295\n",
      "Train_loss: 2.8041637259770154\n",
      "val_loss: 0.138755506072912\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 296\n",
      "Train_loss: 2.8030800231253976\n",
      "val_loss: 0.1387464571460229\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 297\n",
      "Train_loss: 2.804562277865339\n",
      "val_loss: 0.13880045196510515\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 298\n",
      "Train_loss: 2.8060649393611765\n",
      "val_loss: 0.13890706591935828\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "epoch 299\n",
      "Train_loss: 2.8066259839564123\n",
      "val_loss: 0.13898576060118759\n",
      "best loss: 0.13868294930335995\n",
      "*********************************\n",
      "fold 1 score: 0.13868294753811475\n",
      "fold: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train_loss: 16.18567198714709\n",
      "val_loss: 3.585577989101509\n",
      "best loss: 3.585577989101509\n",
      "*********************************\n",
      "epoch 1\n",
      "Train_loss: 6.474237113819644\n",
      "val_loss: 1.3874224926089522\n",
      "best loss: 1.3874224926089522\n",
      "*********************************\n",
      "epoch 2\n",
      "Train_loss: 5.943839400702508\n",
      "val_loss: 1.0997642655569118\n",
      "best loss: 1.0997642655569118\n",
      "*********************************\n",
      "epoch 3\n",
      "Train_loss: 5.596039424723521\n",
      "val_loss: 0.7518286831267236\n",
      "best loss: 0.7518286831267236\n",
      "*********************************\n",
      "epoch 4\n",
      "Train_loss: 5.3755637759904795\n",
      "val_loss: 0.6189456712045524\n",
      "best loss: 0.6189456712045524\n",
      "*********************************\n",
      "epoch 5\n",
      "Train_loss: 5.345136824847776\n",
      "val_loss: 0.6667943407785376\n",
      "best loss: 0.6189456712045524\n",
      "*********************************\n",
      "epoch 6\n",
      "Train_loss: 5.4729328262055255\n",
      "val_loss: 1.0596925545695763\n",
      "best loss: 0.6189456712045524\n",
      "*********************************\n",
      "epoch 7\n",
      "Train_loss: 5.474201383544833\n",
      "val_loss: 0.897415724978012\n",
      "best loss: 0.6189456712045524\n",
      "*********************************\n",
      "epoch 8\n",
      "Train_loss: 5.415010174622129\n",
      "val_loss: 0.8303844135821642\n",
      "best loss: 0.6189456712045524\n",
      "*********************************\n",
      "epoch 9\n",
      "Train_loss: 5.285741450356303\n",
      "val_loss: 0.5760243098046893\n",
      "best loss: 0.5760243098046893\n",
      "*********************************\n",
      "epoch 10\n",
      "Train_loss: 5.099939285207103\n",
      "val_loss: 0.49868020833301624\n",
      "best loss: 0.49868020833301624\n",
      "*********************************\n",
      "epoch 11\n",
      "Train_loss: 4.911813292692492\n",
      "val_loss: 0.5529363408643262\n",
      "best loss: 0.49868020833301624\n",
      "*********************************\n",
      "epoch 12\n",
      "Train_loss: 4.681426228475226\n",
      "val_loss: 0.4504048058034379\n",
      "best loss: 0.4504048058034379\n",
      "*********************************\n",
      "epoch 13\n",
      "Train_loss: 4.450989941055489\n",
      "val_loss: 0.3222270544921454\n",
      "best loss: 0.3222270544921454\n",
      "*********************************\n",
      "epoch 14\n",
      "Train_loss: 4.268426538322777\n",
      "val_loss: 0.29135367924763445\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 15\n",
      "Train_loss: 4.2455414326458225\n",
      "val_loss: 0.3131304484553295\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 16\n",
      "Train_loss: 4.383009839373086\n",
      "val_loss: 0.4020912088425555\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 17\n",
      "Train_loss: 4.55221652262941\n",
      "val_loss: 0.42972901092673277\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 18\n",
      "Train_loss: 4.691521606433997\n",
      "val_loss: 0.45454956272511704\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 19\n",
      "Train_loss: 4.671719690784802\n",
      "val_loss: 0.37534843882271873\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 20\n",
      "Train_loss: 4.62477579464078\n",
      "val_loss: 0.3572488508891781\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 21\n",
      "Train_loss: 4.49284944578953\n",
      "val_loss: 0.35857432819386165\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 22\n",
      "Train_loss: 4.292482784564204\n",
      "val_loss: 0.3063861861282647\n",
      "best loss: 0.29135367924763445\n",
      "*********************************\n",
      "epoch 23\n",
      "Train_loss: 4.075618281644737\n",
      "val_loss: 0.25721481320358586\n",
      "best loss: 0.25721481320358586\n",
      "*********************************\n",
      "epoch 24\n",
      "Train_loss: 3.9122562619418764\n",
      "val_loss: 0.22676080735700863\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 25\n",
      "Train_loss: 3.8958330645209824\n",
      "val_loss: 0.23443004876540097\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 26\n",
      "Train_loss: 4.013266971508053\n",
      "val_loss: 0.272220622217137\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 27\n",
      "Train_loss: 4.198737002697207\n",
      "val_loss: 0.3328034825954214\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 28\n",
      "Train_loss: 4.3581550258151465\n",
      "val_loss: 0.39079480225363045\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 29\n",
      "Train_loss: 4.409822714192857\n",
      "val_loss: 0.3104055604221656\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 30\n",
      "Train_loss: 4.354981583659805\n",
      "val_loss: 0.3213276954929797\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 31\n",
      "Train_loss: 4.245421636414391\n",
      "val_loss: 0.28568184361380705\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 32\n",
      "Train_loss: 4.069147588780374\n",
      "val_loss: 0.2509499117317201\n",
      "best loss: 0.22676080735700863\n",
      "*********************************\n",
      "epoch 33\n",
      "Train_loss: 3.872301720679755\n",
      "val_loss: 0.21254232204775086\n",
      "best loss: 0.21254232204775086\n",
      "*********************************\n",
      "epoch 34\n",
      "Train_loss: 3.722924396707215\n",
      "val_loss: 0.1991533795814734\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 35\n",
      "Train_loss: 3.7005421817633675\n",
      "val_loss: 0.2068399575580046\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 36\n",
      "Train_loss: 3.817113495265741\n",
      "val_loss: 0.2365037096710311\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 37\n",
      "Train_loss: 3.997152212328852\n",
      "val_loss: 0.30874510608974753\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 38\n",
      "Train_loss: 4.144659684123453\n",
      "val_loss: 0.33348050033137705\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 39\n",
      "Train_loss: 4.217255599414842\n",
      "val_loss: 0.29003236409429545\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 40\n",
      "Train_loss: 4.1953237724465176\n",
      "val_loss: 0.31254443492572853\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 41\n",
      "Train_loss: 4.096551194586816\n",
      "val_loss: 0.26333532564784273\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 42\n",
      "Train_loss: 3.920293868600874\n",
      "val_loss: 0.23233612246346375\n",
      "best loss: 0.1991533795814734\n",
      "*********************************\n",
      "epoch 43\n",
      "Train_loss: 3.734415446590774\n",
      "val_loss: 0.19704165503900958\n",
      "best loss: 0.19704165503900958\n",
      "*********************************\n",
      "epoch 44\n",
      "Train_loss: 3.5921448044791138\n",
      "val_loss: 0.184136674604032\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 45\n",
      "Train_loss: 3.573050673793542\n",
      "val_loss: 0.19169865676505912\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 46\n",
      "Train_loss: 3.6888331471463687\n",
      "val_loss: 0.21879740660141483\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 47\n",
      "Train_loss: 3.8628007794860757\n",
      "val_loss: 0.25844806051929736\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 48\n",
      "Train_loss: 4.024884700074462\n",
      "val_loss: 0.3033066812569031\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 49\n",
      "Train_loss: 4.104811353102653\n",
      "val_loss: 0.26373212664623097\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 50\n",
      "Train_loss: 4.087313117164441\n",
      "val_loss: 0.24878095307651438\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 51\n",
      "Train_loss: 3.98125423685381\n",
      "val_loss: 0.23870061184068975\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 52\n",
      "Train_loss: 3.8156163889282473\n",
      "val_loss: 0.2132022981698981\n",
      "best loss: 0.184136674604032\n",
      "*********************************\n",
      "epoch 53\n",
      "Train_loss: 3.635882456776941\n",
      "val_loss: 0.18327154062380263\n",
      "best loss: 0.18327154062380263\n",
      "*********************************\n",
      "epoch 54\n",
      "Train_loss: 3.498979251282444\n",
      "val_loss: 0.17555550078289262\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 55\n",
      "Train_loss: 3.482553585720282\n",
      "val_loss: 0.18057549030365705\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 56\n",
      "Train_loss: 3.587974519584205\n",
      "val_loss: 0.20858065798024744\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 57\n",
      "Train_loss: 3.7570104867234266\n",
      "val_loss: 0.23331081720087934\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 58\n",
      "Train_loss: 3.92750127569627\n",
      "val_loss: 0.2826621900803018\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 59\n",
      "Train_loss: 4.006019175261382\n",
      "val_loss: 0.2542149412174827\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 60\n",
      "Train_loss: 4.004542354003382\n",
      "val_loss: 0.2548122969111717\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 61\n",
      "Train_loss: 3.8971608659781856\n",
      "val_loss: 0.23694349234412826\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62\n",
      "Train_loss: 3.746429173098124\n",
      "val_loss: 0.2005922400164363\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 63\n",
      "Train_loss: 3.560230733177541\n",
      "val_loss: 0.17977312647480387\n",
      "best loss: 0.17555550078289262\n",
      "*********************************\n",
      "epoch 64\n",
      "Train_loss: 3.42881452942794\n",
      "val_loss: 0.17095119279184148\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 65\n",
      "Train_loss: 3.4116820852203493\n",
      "val_loss: 0.17560153233684414\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 66\n",
      "Train_loss: 3.51791374774085\n",
      "val_loss: 0.19156208873491776\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 67\n",
      "Train_loss: 3.6787972743361266\n",
      "val_loss: 0.2261481002480043\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 68\n",
      "Train_loss: 3.851178960736453\n",
      "val_loss: 0.2641441364616877\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 69\n",
      "Train_loss: 3.9369168037772604\n",
      "val_loss: 0.23417945970450102\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 70\n",
      "Train_loss: 3.923455793783436\n",
      "val_loss: 0.2787419242566936\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 71\n",
      "Train_loss: 3.8310426553194343\n",
      "val_loss: 0.23819254361758735\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 72\n",
      "Train_loss: 3.6629749707291985\n",
      "val_loss: 0.19078058216183885\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 73\n",
      "Train_loss: 3.496583580290132\n",
      "val_loss: 0.17347630639562198\n",
      "best loss: 0.17095119279184148\n",
      "*********************************\n",
      "epoch 74\n",
      "Train_loss: 3.36931157199421\n",
      "val_loss: 0.16592782635627037\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 75\n",
      "Train_loss: 3.3550680507134505\n",
      "val_loss: 0.16997297568951342\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 76\n",
      "Train_loss: 3.4602689185300988\n",
      "val_loss: 0.1992084693740736\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 77\n",
      "Train_loss: 3.6131914690820772\n",
      "val_loss: 0.21379276091985772\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 78\n",
      "Train_loss: 3.788085550628436\n",
      "val_loss: 0.22999375661235877\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 79\n",
      "Train_loss: 3.8683382176424144\n",
      "val_loss: 0.2284026810530328\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 80\n",
      "Train_loss: 3.8656095025124735\n",
      "val_loss: 0.2283244200620087\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 81\n",
      "Train_loss: 3.759932241664159\n",
      "val_loss: 0.2131191343799572\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 82\n",
      "Train_loss: 3.608645368145965\n",
      "val_loss: 0.1894558846384934\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 83\n",
      "Train_loss: 3.438917080949951\n",
      "val_loss: 0.17020467419673166\n",
      "best loss: 0.16592782635627037\n",
      "*********************************\n",
      "epoch 84\n",
      "Train_loss: 3.3192060190757133\n",
      "val_loss: 0.16278432620787314\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 85\n",
      "Train_loss: 3.3025001922516473\n",
      "val_loss: 0.16572343481538537\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 86\n",
      "Train_loss: 3.4016881348035373\n",
      "val_loss: 0.18585173989987752\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 87\n",
      "Train_loss: 3.5602336466181455\n",
      "val_loss: 0.21655556504255147\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 88\n",
      "Train_loss: 3.7152291375527517\n",
      "val_loss: 0.24054364400554812\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 89\n",
      "Train_loss: 3.8255916421106346\n",
      "val_loss: 0.22589782432133607\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 90\n",
      "Train_loss: 3.803819715045703\n",
      "val_loss: 0.22863300187054783\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 91\n",
      "Train_loss: 3.717663539491886\n",
      "val_loss: 0.2058064994063459\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 92\n",
      "Train_loss: 3.557845492888491\n",
      "val_loss: 0.19132701243945327\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 93\n",
      "Train_loss: 3.3978252805006997\n",
      "val_loss: 0.16733753846155763\n",
      "best loss: 0.16278432620787314\n",
      "*********************************\n",
      "epoch 94\n",
      "Train_loss: 3.272893974872272\n",
      "val_loss: 0.16015057726507212\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 95\n",
      "Train_loss: 3.2581149962817815\n",
      "val_loss: 0.162506127084344\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 96\n",
      "Train_loss: 3.3540328504349963\n",
      "val_loss: 0.18357674230247753\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 97\n",
      "Train_loss: 3.5113176261592116\n",
      "val_loss: 0.19877850238699593\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 98\n",
      "Train_loss: 3.6722999684476543\n",
      "val_loss: 0.22724369741694356\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 99\n",
      "Train_loss: 3.7661531445435434\n",
      "val_loss: 0.2190834352776955\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 100\n",
      "Train_loss: 3.7830468773994106\n",
      "val_loss: 0.23032950325710622\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 101\n",
      "Train_loss: 3.767561818568744\n",
      "val_loss: 0.2666868883553619\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 102\n",
      "Train_loss: 3.773845439613654\n",
      "val_loss: 0.241075039475054\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 103\n",
      "Train_loss: 3.756408896686338\n",
      "val_loss: 0.24569960477004626\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 104\n",
      "Train_loss: 3.7410530713517116\n",
      "val_loss: 0.2114385890062503\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 105\n",
      "Train_loss: 3.749962607699123\n",
      "val_loss: 0.2262768385998504\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 106\n",
      "Train_loss: 3.7564652557962384\n",
      "val_loss: 0.23485970829035113\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 107\n",
      "Train_loss: 3.7339182454496798\n",
      "val_loss: 0.2613702950411407\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 108\n",
      "Train_loss: 3.738767005274478\n",
      "val_loss: 0.2350185070599302\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 109\n",
      "Train_loss: 3.7320394028954245\n",
      "val_loss: 0.20694139163021805\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 110\n",
      "Train_loss: 3.71727879374304\n",
      "val_loss: 0.21198823698529887\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 111\n",
      "Train_loss: 3.7043557879984337\n",
      "val_loss: 0.2082177566390935\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 112\n",
      "Train_loss: 3.711118923696201\n",
      "val_loss: 0.24064031882006806\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 113\n",
      "Train_loss: 3.7012923346973112\n",
      "val_loss: 0.22781728583349684\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 114\n",
      "Train_loss: 3.686855149534569\n",
      "val_loss: 0.19960635188882467\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 115\n",
      "Train_loss: 3.6976407352826293\n",
      "val_loss: 0.2341091891756446\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 116\n",
      "Train_loss: 3.676245683710418\n",
      "val_loss: 0.21422824117837902\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 117\n",
      "Train_loss: 3.6743184405471547\n",
      "val_loss: 0.23001155780504914\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 118\n",
      "Train_loss: 3.677132489339055\n",
      "val_loss: 0.24272518721188435\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 119\n",
      "Train_loss: 3.658808094404956\n",
      "val_loss: 0.19521056604776188\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 120\n",
      "Train_loss: 3.6465723859453316\n",
      "val_loss: 0.20356157707921627\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 121\n",
      "Train_loss: 3.654480692218211\n",
      "val_loss: 0.2106471141038464\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 122\n",
      "Train_loss: 3.6504948784824305\n",
      "val_loss: 0.21439058784745457\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123\n",
      "Train_loss: 3.6243899796033356\n",
      "val_loss: 0.2275159031175492\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 124\n",
      "Train_loss: 3.6246605473731033\n",
      "val_loss: 0.20225974065181365\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 125\n",
      "Train_loss: 3.64396319216873\n",
      "val_loss: 0.20249128526604604\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 126\n",
      "Train_loss: 3.6184435091507177\n",
      "val_loss: 0.20054329373190038\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 127\n",
      "Train_loss: 3.620118263298624\n",
      "val_loss: 0.22657713602372415\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 128\n",
      "Train_loss: 3.613472003783623\n",
      "val_loss: 0.2145547671383545\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 129\n",
      "Train_loss: 3.6005095106072083\n",
      "val_loss: 0.18823276373233347\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 130\n",
      "Train_loss: 3.600182166059777\n",
      "val_loss: 0.1978260711144275\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 131\n",
      "Train_loss: 3.594265529053415\n",
      "val_loss: 0.20097524792076613\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 132\n",
      "Train_loss: 3.5979526090539453\n",
      "val_loss: 0.21524926841036537\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 133\n",
      "Train_loss: 3.586313485066064\n",
      "val_loss: 0.21277172007278522\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 134\n",
      "Train_loss: 3.5794485949610344\n",
      "val_loss: 0.1937743244952532\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 135\n",
      "Train_loss: 3.570374457502755\n",
      "val_loss: 0.19791169004603787\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 136\n",
      "Train_loss: 3.558479836352043\n",
      "val_loss: 0.19040172541854183\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 137\n",
      "Train_loss: 3.565082524516767\n",
      "val_loss: 0.20163137423337898\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 138\n",
      "Train_loss: 3.5623690850399674\n",
      "val_loss: 0.21396569701715012\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 139\n",
      "Train_loss: 3.5589673695344035\n",
      "val_loss: 0.18387228418579257\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 140\n",
      "Train_loss: 3.5571028021049536\n",
      "val_loss: 0.1960032828287562\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 141\n",
      "Train_loss: 3.538797999165964\n",
      "val_loss: 0.19592839818024158\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 142\n",
      "Train_loss: 3.548379351309229\n",
      "val_loss: 0.20765014765615322\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 143\n",
      "Train_loss: 3.53006423024358\n",
      "val_loss: 0.20136499895770155\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 144\n",
      "Train_loss: 3.531734316374887\n",
      "val_loss: 0.18768357745985564\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 145\n",
      "Train_loss: 3.525887362615812\n",
      "val_loss: 0.2007426171662592\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 146\n",
      "Train_loss: 3.5368710628920748\n",
      "val_loss: 0.21349814157126842\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 147\n",
      "Train_loss: 3.514415074061727\n",
      "val_loss: 0.20734065088636194\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 148\n",
      "Train_loss: 3.516017940608951\n",
      "val_loss: 0.20312716943428435\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 149\n",
      "Train_loss: 3.5157117514376863\n",
      "val_loss: 0.19091347141917447\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "Epoch    51: reducing learning rate of group 0 to 1.5000e-03.\n",
      "epoch 150\n",
      "Train_loss: 3.5201039398964005\n",
      "val_loss: 0.19806713302598014\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 151\n",
      "Train_loss: 3.24702212064621\n",
      "val_loss: 0.16044074388435586\n",
      "best loss: 0.16015057726507212\n",
      "*********************************\n",
      "epoch 152\n",
      "Train_loss: 3.1951814927496613\n",
      "val_loss: 0.1591289312729498\n",
      "best loss: 0.1591289312729498\n",
      "*********************************\n",
      "epoch 153\n",
      "Train_loss: 3.1763069027656456\n",
      "val_loss: 0.1596813400573328\n",
      "best loss: 0.1591289312729498\n",
      "*********************************\n",
      "epoch 154\n",
      "Train_loss: 3.175161542771949\n",
      "val_loss: 0.1552089944681061\n",
      "best loss: 0.1552089944681061\n",
      "*********************************\n",
      "epoch 155\n",
      "Train_loss: 3.164691810895819\n",
      "val_loss: 0.15796611721182918\n",
      "best loss: 0.1552089944681061\n",
      "*********************************\n",
      "epoch 156\n",
      "Train_loss: 3.1654260712741458\n",
      "val_loss: 0.16030632277177767\n",
      "best loss: 0.1552089944681061\n",
      "*********************************\n",
      "epoch 157\n",
      "Train_loss: 3.1451383167905016\n",
      "val_loss: 0.15925153478084184\n",
      "best loss: 0.1552089944681061\n",
      "*********************************\n",
      "epoch 158\n",
      "Train_loss: 3.1350404736506805\n",
      "val_loss: 0.15973274228591214\n",
      "best loss: 0.1552089944681061\n",
      "*********************************\n",
      "epoch 159\n",
      "Train_loss: 3.1295311351445045\n",
      "val_loss: 0.15415865591989739\n",
      "best loss: 0.15415865591989739\n",
      "*********************************\n",
      "epoch 160\n",
      "Train_loss: 3.1321157442478773\n",
      "val_loss: 0.1572200599968114\n",
      "best loss: 0.15415865591989739\n",
      "*********************************\n",
      "epoch 161\n",
      "Train_loss: 3.1255925227840455\n",
      "val_loss: 0.15886807492950175\n",
      "best loss: 0.15415865591989739\n",
      "*********************************\n",
      "epoch 162\n",
      "Train_loss: 3.120276860927526\n",
      "val_loss: 0.16034667721693624\n",
      "best loss: 0.15415865591989739\n",
      "*********************************\n",
      "epoch 163\n",
      "Train_loss: 3.113481809067109\n",
      "val_loss: 0.1580336332742031\n",
      "best loss: 0.15415865591989739\n",
      "*********************************\n",
      "epoch 164\n",
      "Train_loss: 3.105091689783075\n",
      "val_loss: 0.153982192611022\n",
      "best loss: 0.153982192611022\n",
      "*********************************\n",
      "epoch 165\n",
      "Train_loss: 3.1094180848325794\n",
      "val_loss: 0.15245617146640628\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 166\n",
      "Train_loss: 3.10351226785468\n",
      "val_loss: 0.155988809602346\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 167\n",
      "Train_loss: 3.0952525328995852\n",
      "val_loss: 0.1583910060054633\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 168\n",
      "Train_loss: 3.0936873751467857\n",
      "val_loss: 0.157668276411301\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 169\n",
      "Train_loss: 3.0870442190891816\n",
      "val_loss: 0.15299353645693786\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 170\n",
      "Train_loss: 3.0899748143705286\n",
      "val_loss: 0.1545143418858949\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 171\n",
      "Train_loss: 3.0862329751194113\n",
      "val_loss: 0.15547798261370188\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 172\n",
      "Train_loss: 3.078772000132396\n",
      "val_loss: 0.15350205854367377\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 173\n",
      "Train_loss: 3.076357116614382\n",
      "val_loss: 0.15609008340307456\n",
      "best loss: 0.15245617146640628\n",
      "*********************************\n",
      "epoch 174\n",
      "Train_loss: 3.0735588638547426\n",
      "val_loss: 0.15206135359555464\n",
      "best loss: 0.15206135359555464\n",
      "*********************************\n",
      "epoch 175\n",
      "Train_loss: 3.0715974975519087\n",
      "val_loss: 0.1523311138239916\n",
      "best loss: 0.15206135359555464\n",
      "*********************************\n",
      "epoch 176\n",
      "Train_loss: 3.0677339974974926\n",
      "val_loss: 0.16249048164076202\n",
      "best loss: 0.15206135359555464\n",
      "*********************************\n",
      "epoch 177\n",
      "Train_loss: 3.0748306032003536\n",
      "val_loss: 0.1585584134364323\n",
      "best loss: 0.15206135359555464\n",
      "*********************************\n",
      "epoch 178\n",
      "Train_loss: 3.0604018884644986\n",
      "val_loss: 0.15692129755129164\n",
      "best loss: 0.15206135359555464\n",
      "*********************************\n",
      "epoch 179\n",
      "Train_loss: 3.0558898546855033\n",
      "val_loss: 0.1511058422283303\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 180\n",
      "Train_loss: 3.056568240412917\n",
      "val_loss: 0.1519757314653115\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 181\n",
      "Train_loss: 3.058157179466693\n",
      "val_loss: 0.15739422113305349\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 182\n",
      "Train_loss: 3.058452577357795\n",
      "val_loss: 0.15550376791380363\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 183\n",
      "Train_loss: 3.057137144112633\n",
      "val_loss: 0.15908105131948339\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 184\n",
      "Train_loss: 3.0447121373048276\n",
      "val_loss: 0.15169967808375545\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 185\n",
      "Train_loss: 3.0481608238650058\n",
      "val_loss: 0.1537193378320299\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 186\n",
      "Train_loss: 3.0422603387726452\n",
      "val_loss: 0.15495580867108663\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 187\n",
      "Train_loss: 3.0420006289780246\n",
      "val_loss: 0.15400275190045734\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 188\n",
      "Train_loss: 3.038693910084614\n",
      "val_loss: 0.154976830847659\n",
      "best loss: 0.1511058422283303\n",
      "*********************************\n",
      "epoch 189\n",
      "Train_loss: 3.0361950878140203\n",
      "val_loss: 0.150604843388359\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 190\n",
      "Train_loss: 3.03383881736852\n",
      "val_loss: 0.15088441755102522\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 191\n",
      "Train_loss: 3.0403706790939315\n",
      "val_loss: 0.15393646714553502\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 192\n",
      "Train_loss: 3.0331962633906455\n",
      "val_loss: 0.15384071208660624\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 193\n",
      "Train_loss: 3.03028562115824\n",
      "val_loss: 0.1549036322295535\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 194\n",
      "Train_loss: 3.027119514731427\n",
      "val_loss: 0.152017929368994\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 195\n",
      "Train_loss: 3.0309488188927847\n",
      "val_loss: 0.15205045890355887\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 196\n",
      "Train_loss: 3.026031707562159\n",
      "val_loss: 0.15301348434983164\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 197\n",
      "Train_loss: 3.0218308735133492\n",
      "val_loss: 0.15579745991049618\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 198\n",
      "Train_loss: 3.0153653443367907\n",
      "val_loss: 0.1561112583592942\n",
      "best loss: 0.150604843388359\n",
      "*********************************\n",
      "epoch 199\n",
      "Train_loss: 3.0198208855164066\n",
      "val_loss: 0.15002374212874497\n",
      "best loss: 0.15002374212874497\n",
      "*********************************\n",
      "epoch 200\n",
      "Train_loss: 3.016305979688251\n",
      "val_loss: 0.1507202316590626\n",
      "best loss: 0.15002374212874497\n",
      "*********************************\n",
      "epoch 201\n",
      "Train_loss: 3.0187667580914317\n",
      "val_loss: 0.15298294628092282\n",
      "best loss: 0.15002374212874497\n",
      "*********************************\n",
      "epoch 202\n",
      "Train_loss: 3.0156569330863237\n",
      "val_loss: 0.1556154686322602\n",
      "best loss: 0.15002374212874497\n",
      "*********************************\n",
      "epoch 203\n",
      "Train_loss: 3.0057335508245315\n",
      "val_loss: 0.1557514258463533\n",
      "best loss: 0.15002374212874497\n",
      "*********************************\n",
      "epoch 204\n",
      "Train_loss: 3.010700582347595\n",
      "val_loss: 0.14960038516348842\n",
      "best loss: 0.14960038516348842\n",
      "*********************************\n",
      "epoch 205\n",
      "Train_loss: 3.0125712123422086\n",
      "val_loss: 0.1537606123826758\n",
      "best loss: 0.14960038516348842\n",
      "*********************************\n",
      "epoch 206\n",
      "Train_loss: 3.005885508134412\n",
      "val_loss: 0.15203311871179248\n",
      "best loss: 0.14960038516348842\n",
      "*********************************\n",
      "epoch 207\n",
      "Train_loss: 3.0073143239323805\n",
      "val_loss: 0.15195608622693635\n",
      "best loss: 0.14960038516348842\n",
      "*********************************\n",
      "epoch 208\n",
      "Train_loss: 3.001222748879705\n",
      "val_loss: 0.15570048412201967\n",
      "best loss: 0.14960038516348842\n",
      "*********************************\n",
      "epoch 209\n",
      "Train_loss: 3.006186252581054\n",
      "val_loss: 0.14891064154777314\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 210\n",
      "Train_loss: 2.997579224894706\n",
      "val_loss: 0.15262771945832265\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 211\n",
      "Train_loss: 2.9980744719710435\n",
      "val_loss: 0.15320237641538473\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 212\n",
      "Train_loss: 2.996619085259112\n",
      "val_loss: 0.1536358677229297\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 213\n",
      "Train_loss: 2.9927166441455\n",
      "val_loss: 0.15471091072250323\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 214\n",
      "Train_loss: 2.9930823029132982\n",
      "val_loss: 0.1512973421484603\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 215\n",
      "Train_loss: 2.9891812686803942\n",
      "val_loss: 0.1517263802747098\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 216\n",
      "Train_loss: 2.991103320376272\n",
      "val_loss: 0.1556872818950476\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 217\n",
      "Train_loss: 2.9920228193492324\n",
      "val_loss: 0.15355008807334108\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 218\n",
      "Train_loss: 2.9878609363678335\n",
      "val_loss: 0.15225269844144948\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 219\n",
      "Train_loss: 2.983615673428242\n",
      "val_loss: 0.15136716251940202\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "Epoch   121: reducing learning rate of group 0 to 4.5000e-04.\n",
      "epoch 220\n",
      "Train_loss: 2.9860770148684375\n",
      "val_loss: 0.1531195180259223\n",
      "best loss: 0.14891064154777314\n",
      "*********************************\n",
      "epoch 221\n",
      "Train_loss: 2.885026170757591\n",
      "val_loss: 0.14290475620782786\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 222\n",
      "Train_loss: 2.8673235692128287\n",
      "val_loss: 0.14444274912501082\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 223\n",
      "Train_loss: 2.862338180189532\n",
      "val_loss: 0.14451087635225587\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 224\n",
      "Train_loss: 2.8596100047293227\n",
      "val_loss: 0.14339087407549878\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 225\n",
      "Train_loss: 2.8541220646456615\n",
      "val_loss: 0.14346192254947002\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 226\n",
      "Train_loss: 2.8520633532905686\n",
      "val_loss: 0.14409826118391914\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 227\n",
      "Train_loss: 2.851556971338098\n",
      "val_loss: 0.14359357658121916\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 228\n",
      "Train_loss: 2.851218433468684\n",
      "val_loss: 0.14343322635186775\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 229\n",
      "Train_loss: 2.846792631470952\n",
      "val_loss: 0.14307569920232208\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 230\n",
      "Train_loss: 2.847042676591262\n",
      "val_loss: 0.14359864311480616\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 231\n",
      "Train_loss: 2.8470324210933136\n",
      "val_loss: 0.1434217198646478\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "Epoch   133: reducing learning rate of group 0 to 1.3500e-04.\n",
      "epoch 232\n",
      "Train_loss: 2.8418183307365013\n",
      "val_loss: 0.14422270534361992\n",
      "best loss: 0.14290475620782786\n",
      "*********************************\n",
      "epoch 233\n",
      "Train_loss: 2.8108466696448176\n",
      "val_loss: 0.1418143693769992\n",
      "best loss: 0.1418143693769992\n",
      "*********************************\n",
      "epoch 234\n",
      "Train_loss: 2.8063176439422515\n",
      "val_loss: 0.14148952096671072\n",
      "best loss: 0.14148952096671072\n",
      "*********************************\n",
      "epoch 235\n",
      "Train_loss: 2.8026222351263246\n",
      "val_loss: 0.14157806149922042\n",
      "best loss: 0.14148952096671072\n",
      "*********************************\n",
      "epoch 236\n",
      "Train_loss: 2.8018579594667616\n",
      "val_loss: 0.1415079340314314\n",
      "best loss: 0.14148952096671072\n",
      "*********************************\n",
      "epoch 237\n",
      "Train_loss: 2.802152407605556\n",
      "val_loss: 0.1415795957414361\n",
      "best loss: 0.14148952096671072\n",
      "*********************************\n",
      "epoch 238\n",
      "Train_loss: 2.8012352246806875\n",
      "val_loss: 0.1413294783024099\n",
      "best loss: 0.1413294783024099\n",
      "*********************************\n",
      "epoch 239\n",
      "Train_loss: 2.800142477391744\n",
      "val_loss: 0.14071860738585917\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 240\n",
      "Train_loss: 2.7999532089753343\n",
      "val_loss: 0.14159079639188066\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 241\n",
      "Train_loss: 2.7965945386771778\n",
      "val_loss: 0.14133070579261695\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 242\n",
      "Train_loss: 2.797858700442028\n",
      "val_loss: 0.14100386433334172\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 243\n",
      "Train_loss: 2.796791152358885\n",
      "val_loss: 0.14117188992580484\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 244\n",
      "Train_loss: 2.7959057952236295\n",
      "val_loss: 0.14073119101731688\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 245\n",
      "Train_loss: 2.795717453093935\n",
      "val_loss: 0.14128559140880403\n",
      "best loss: 0.14071860738585917\n",
      "*********************************\n",
      "epoch 246\n",
      "Train_loss: 2.794811128072012\n",
      "val_loss: 0.1406831624563039\n",
      "best loss: 0.1406831624563039\n",
      "*********************************\n",
      "epoch 247\n",
      "Train_loss: 2.7951534069262447\n",
      "val_loss: 0.14109700568790925\n",
      "best loss: 0.1406831624563039\n",
      "*********************************\n",
      "epoch 248\n",
      "Train_loss: 2.7932599296762484\n",
      "val_loss: 0.14119428967177317\n",
      "best loss: 0.1406831624563039\n",
      "*********************************\n",
      "epoch 249\n",
      "Train_loss: 2.792055957904599\n",
      "val_loss: 0.14102872466868246\n",
      "best loss: 0.1406831624563039\n",
      "*********************************\n",
      "epoch 250\n",
      "Train_loss: 2.7925257938097507\n",
      "val_loss: 0.14033514751741133\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 251\n",
      "Train_loss: 2.791071499950846\n",
      "val_loss: 0.14109578112345628\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 252\n",
      "Train_loss: 2.790238395949371\n",
      "val_loss: 0.14073410465126387\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 253\n",
      "Train_loss: 2.7925426906220907\n",
      "val_loss: 0.14112355381437722\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 254\n",
      "Train_loss: 2.788890614636698\n",
      "val_loss: 0.1408413668392793\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 255\n",
      "Train_loss: 2.790246283279686\n",
      "val_loss: 0.14130691896874714\n",
      "best loss: 0.14033514751741133\n",
      "*********************************\n",
      "epoch 256\n",
      "Train_loss: 2.789523605723735\n",
      "val_loss: 0.14029893279625283\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 257\n",
      "Train_loss: 2.788945804810025\n",
      "val_loss: 0.1407379431236476\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 258\n",
      "Train_loss: 2.7868636318198545\n",
      "val_loss: 0.14047616332799642\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 259\n",
      "Train_loss: 2.788906787843187\n",
      "val_loss: 0.1406303768516227\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 260\n",
      "Train_loss: 2.789325729917546\n",
      "val_loss: 0.14086223414157578\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 261\n",
      "Train_loss: 2.789181773035466\n",
      "val_loss: 0.14107997411603712\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 262\n",
      "Train_loss: 2.789668772331976\n",
      "val_loss: 0.1410308718438054\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 263\n",
      "Train_loss: 2.7870993695380926\n",
      "val_loss: 0.14087742556462715\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 264\n",
      "Train_loss: 2.786646400727987\n",
      "val_loss: 0.14063252386981595\n",
      "best loss: 0.14029893279625283\n",
      "*********************************\n",
      "epoch 265\n",
      "Train_loss: 2.7862332719007377\n",
      "val_loss: 0.14015822473818826\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 266\n",
      "Train_loss: 2.785525646680259\n",
      "val_loss: 0.1407146182183255\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 267\n",
      "Train_loss: 2.7848668515276396\n",
      "val_loss: 0.14066121997444755\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 268\n",
      "Train_loss: 2.7849547035282805\n",
      "val_loss: 0.14144916609945204\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 269\n",
      "Train_loss: 2.784044098459776\n",
      "val_loss: 0.14091762699591484\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 270\n",
      "Train_loss: 2.7842378020770404\n",
      "val_loss: 0.14077707266972714\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 271\n",
      "Train_loss: 2.782146287401185\n",
      "val_loss: 0.14049028240137668\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 272\n",
      "Train_loss: 2.7811993987121464\n",
      "val_loss: 0.14083047134580398\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 273\n",
      "Train_loss: 2.784376694206189\n",
      "val_loss: 0.14092637333940927\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 274\n",
      "Train_loss: 2.7819790918580707\n",
      "val_loss: 0.1407094019040412\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 275\n",
      "Train_loss: 2.782247374889573\n",
      "val_loss: 0.1405111491313759\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "Epoch   177: reducing learning rate of group 0 to 4.0500e-05.\n",
      "epoch 276\n",
      "Train_loss: 2.7810548813459426\n",
      "val_loss: 0.14078658563910662\n",
      "best loss: 0.14015822473818826\n",
      "*********************************\n",
      "epoch 277\n",
      "Train_loss: 2.7708393628787222\n",
      "val_loss: 0.1401114194395838\n",
      "best loss: 0.1401114194395838\n",
      "*********************************\n",
      "epoch 278\n",
      "Train_loss: 2.7691610987703212\n",
      "val_loss: 0.14001996768633104\n",
      "best loss: 0.14001996768633104\n",
      "*********************************\n",
      "epoch 279\n",
      "Train_loss: 2.76774510532498\n",
      "val_loss: 0.1401875290840998\n",
      "best loss: 0.14001996768633104\n",
      "*********************************\n",
      "epoch 280\n",
      "Train_loss: 2.7697241118680758\n",
      "val_loss: 0.13972842058295556\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 281\n",
      "Train_loss: 2.768019563308366\n",
      "val_loss: 0.1402100891091637\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 282\n",
      "Train_loss: 2.7692020870659997\n",
      "val_loss: 0.14005771656618288\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 283\n",
      "Train_loss: 2.7688201444812215\n",
      "val_loss: 0.14024308016017864\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 284\n",
      "Train_loss: 2.7680259933617535\n",
      "val_loss: 0.14014134458303348\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 285\n",
      "Train_loss: 2.7657246620013822\n",
      "val_loss: 0.140210240569366\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 286\n",
      "Train_loss: 2.7673547342430003\n",
      "val_loss: 0.1402452269954066\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 287\n",
      "Train_loss: 2.76598445177146\n",
      "val_loss: 0.1399288216220634\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 288\n",
      "Train_loss: 2.765386478438538\n",
      "val_loss: 0.14031673178015436\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 289\n",
      "Train_loss: 2.7665412699527514\n",
      "val_loss: 0.14021729949296532\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 290\n",
      "Train_loss: 2.765671067814125\n",
      "val_loss: 0.1400512708970746\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "Epoch   192: reducing learning rate of group 0 to 1.2150e-05.\n",
      "epoch 291\n",
      "Train_loss: 2.766140353119591\n",
      "val_loss: 0.14011034885830653\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 292\n",
      "Train_loss: 2.7638880296533075\n",
      "val_loss: 0.13998759272794079\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 293\n",
      "Train_loss: 2.7624848924510768\n",
      "val_loss: 0.13978826390845575\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 294\n",
      "Train_loss: 2.763455743691733\n",
      "val_loss: 0.13989582954962346\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 295\n",
      "Train_loss: 2.7624279394833597\n",
      "val_loss: 0.13983629332858127\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 296\n",
      "Train_loss: 2.7610136408740176\n",
      "val_loss: 0.13980084663077855\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 297\n",
      "Train_loss: 2.761762557628367\n",
      "val_loss: 0.1398692829460073\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 298\n",
      "Train_loss: 2.7628611090252524\n",
      "val_loss: 0.1400175102939977\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "epoch 299\n",
      "Train_loss: 2.761522104530138\n",
      "val_loss: 0.13991946198028402\n",
      "best loss: 0.13972842058295556\n",
      "*********************************\n",
      "fold 2 score: 0.13972841997177451\n",
      "fold: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train_loss: 17.82370408096023\n",
      "val_loss: 3.421132542521537\n",
      "best loss: 3.421132542521537\n",
      "*********************************\n",
      "epoch 1\n",
      "Train_loss: 6.54408388702443\n",
      "val_loss: 1.2924404085161636\n",
      "best loss: 1.2924404085161636\n",
      "*********************************\n",
      "epoch 2\n",
      "Train_loss: 6.0469748559744945\n",
      "val_loss: 1.8152607168559993\n",
      "best loss: 1.2924404085161636\n",
      "*********************************\n",
      "epoch 3\n",
      "Train_loss: 5.6519764020069925\n",
      "val_loss: 0.7319401689208999\n",
      "best loss: 0.7319401689208999\n",
      "*********************************\n",
      "epoch 4\n",
      "Train_loss: 5.439888509443373\n",
      "val_loss: 0.6391275363116163\n",
      "best loss: 0.6391275363116163\n",
      "*********************************\n",
      "epoch 5\n",
      "Train_loss: 5.411366424265024\n",
      "val_loss: 0.6932173096898029\n",
      "best loss: 0.6391275363116163\n",
      "*********************************\n",
      "epoch 6\n",
      "Train_loss: 5.508243990832176\n",
      "val_loss: 0.9467526799052653\n",
      "best loss: 0.6391275363116163\n",
      "*********************************\n",
      "epoch 7\n",
      "Train_loss: 5.520968984931683\n",
      "val_loss: 1.1378412091583257\n",
      "best loss: 0.6391275363116163\n",
      "*********************************\n",
      "epoch 8\n",
      "Train_loss: 5.434539688539653\n",
      "val_loss: 0.7722620754244689\n",
      "best loss: 0.6391275363116163\n",
      "*********************************\n",
      "epoch 9\n",
      "Train_loss: 5.274624179355562\n",
      "val_loss: 0.6158126985726522\n",
      "best loss: 0.6158126985726522\n",
      "*********************************\n",
      "epoch 10\n",
      "Train_loss: 5.105813122756751\n",
      "val_loss: 0.4808249597255322\n",
      "best loss: 0.4808249597255322\n",
      "*********************************\n",
      "epoch 11\n",
      "Train_loss: 4.89883039046361\n",
      "val_loss: 0.48902099684542427\n",
      "best loss: 0.4808249597255322\n",
      "*********************************\n",
      "epoch 12\n",
      "Train_loss: 4.676501767262223\n",
      "val_loss: 0.4122151114045891\n",
      "best loss: 0.4122151114045891\n",
      "*********************************\n",
      "epoch 13\n",
      "Train_loss: 4.446848921895392\n",
      "val_loss: 0.3184663868342824\n",
      "best loss: 0.3184663868342824\n",
      "*********************************\n",
      "epoch 14\n",
      "Train_loss: 4.2606369243519655\n",
      "val_loss: 0.28146096092579254\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 15\n",
      "Train_loss: 4.239069046270897\n",
      "val_loss: 0.29743597375164027\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 16\n",
      "Train_loss: 4.377628700332582\n",
      "val_loss: 0.44873264977793537\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 17\n",
      "Train_loss: 4.560282962213126\n",
      "val_loss: 0.46157606899446335\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 18\n",
      "Train_loss: 4.684314976416781\n",
      "val_loss: 0.539835372503361\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 19\n",
      "Train_loss: 4.672081597834821\n",
      "val_loss: 0.37785382897918446\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 20\n",
      "Train_loss: 4.616116839949876\n",
      "val_loss: 0.3584649460311846\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 21\n",
      "Train_loss: 4.471604398065922\n",
      "val_loss: 0.34912186305493786\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 22\n",
      "Train_loss: 4.275364905213797\n",
      "val_loss: 0.29886868122609733\n",
      "best loss: 0.28146096092579254\n",
      "*********************************\n",
      "epoch 23\n",
      "Train_loss: 4.069820438573404\n",
      "val_loss: 0.23518681042895537\n",
      "best loss: 0.23518681042895537\n",
      "*********************************\n",
      "epoch 24\n",
      "Train_loss: 3.903312514739344\n",
      "val_loss: 0.21754128129854827\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 25\n",
      "Train_loss: 3.8852928264000868\n",
      "val_loss: 0.22839787743956175\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 26\n",
      "Train_loss: 4.006390213862093\n",
      "val_loss: 0.2853992094474766\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 27\n",
      "Train_loss: 4.189156914056359\n",
      "val_loss: 0.4144619863992536\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 28\n",
      "Train_loss: 4.347015383178601\n",
      "val_loss: 0.36213656242755976\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 29\n",
      "Train_loss: 4.392234910496213\n",
      "val_loss: 0.33892228349453174\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 30\n",
      "Train_loss: 4.361842055965415\n",
      "val_loss: 0.3315014962401566\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 31\n",
      "Train_loss: 4.233732778710346\n",
      "val_loss: 0.2691058939400251\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 32\n",
      "Train_loss: 4.064400953426859\n",
      "val_loss: 0.23806631891670524\n",
      "best loss: 0.21754128129854827\n",
      "*********************************\n",
      "epoch 33\n",
      "Train_loss: 3.8683428864812415\n",
      "val_loss: 0.21133146894297106\n",
      "best loss: 0.21133146894297106\n",
      "*********************************\n",
      "epoch 34\n",
      "Train_loss: 3.7165495755120412\n",
      "val_loss: 0.19350408538922168\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 35\n",
      "Train_loss: 3.7006316691371204\n",
      "val_loss: 0.1995236198046476\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 36\n",
      "Train_loss: 3.817390300963433\n",
      "val_loss: 0.23475166582028725\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 37\n",
      "Train_loss: 3.983266473397249\n",
      "val_loss: 0.3096196309561352\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 38\n",
      "Train_loss: 4.151435983964371\n",
      "val_loss: 0.35117047431439036\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 39\n",
      "Train_loss: 4.212488371496402\n",
      "val_loss: 0.28893604412892193\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 40\n",
      "Train_loss: 4.205274890169588\n",
      "val_loss: 0.2678373998354852\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 41\n",
      "Train_loss: 4.080733676540426\n",
      "val_loss: 0.2522006198838726\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 42\n",
      "Train_loss: 3.91934373874864\n",
      "val_loss: 0.22788667282181313\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 43\n",
      "Train_loss: 3.7298292569940563\n",
      "val_loss: 0.19453077892045814\n",
      "best loss: 0.19350408538922168\n",
      "*********************************\n",
      "epoch 44\n",
      "Train_loss: 3.5862559035538926\n",
      "val_loss: 0.17959042308268558\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 45\n",
      "Train_loss: 3.567914277601001\n",
      "val_loss: 0.18556978438869237\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 46\n",
      "Train_loss: 3.6838059499725233\n",
      "val_loss: 0.21920719546709447\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 47\n",
      "Train_loss: 3.868704610106132\n",
      "val_loss: 0.27866701053201354\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 48\n",
      "Train_loss: 4.011148658380915\n",
      "val_loss: 0.32066910731593967\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 49\n",
      "Train_loss: 4.096372912079996\n",
      "val_loss: 0.2638353679207788\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 50\n",
      "Train_loss: 4.089253761446349\n",
      "val_loss: 0.26725658982501954\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 51\n",
      "Train_loss: 3.9905871368045855\n",
      "val_loss: 0.23776410945906393\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 52\n",
      "Train_loss: 3.8175641927573487\n",
      "val_loss: 0.2101036111931499\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 53\n",
      "Train_loss: 3.633944409303837\n",
      "val_loss: 0.18203248793637938\n",
      "best loss: 0.17959042308268558\n",
      "*********************************\n",
      "epoch 54\n",
      "Train_loss: 3.496749145811292\n",
      "val_loss: 0.17214846375486936\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 55\n",
      "Train_loss: 3.481671564872136\n",
      "val_loss: 0.17872350414391436\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 56\n",
      "Train_loss: 3.589346670353727\n",
      "val_loss: 0.20728772752240507\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 57\n",
      "Train_loss: 3.763688715751496\n",
      "val_loss: 0.23751279756523724\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 58\n",
      "Train_loss: 3.9182252843598415\n",
      "val_loss: 0.28859426853872855\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 59\n",
      "Train_loss: 3.9946351004250777\n",
      "val_loss: 0.27957287240528117\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 60\n",
      "Train_loss: 3.987115273921301\n",
      "val_loss: 0.2536451368954938\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 61\n",
      "Train_loss: 3.8974414584280774\n",
      "val_loss: 0.22326734302759063\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62\n",
      "Train_loss: 3.742297385464943\n",
      "val_loss: 0.20087198834383882\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 63\n",
      "Train_loss: 3.554678370990771\n",
      "val_loss: 0.1758944338689504\n",
      "best loss: 0.17214846375486936\n",
      "*********************************\n",
      "epoch 64\n",
      "Train_loss: 3.425454532704687\n",
      "val_loss: 0.16488297812970112\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 65\n",
      "Train_loss: 3.4098545365424076\n",
      "val_loss: 0.16893162379243387\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 66\n",
      "Train_loss: 3.5063643144634207\n",
      "val_loss: 0.19385919952464284\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 67\n",
      "Train_loss: 3.6776867527389188\n",
      "val_loss: 0.2439618096797377\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 68\n",
      "Train_loss: 3.848148311126386\n",
      "val_loss: 0.2596764787825532\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 69\n",
      "Train_loss: 3.9198166439844218\n",
      "val_loss: 0.23824157377559974\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 70\n",
      "Train_loss: 3.913731640055623\n",
      "val_loss: 0.2283468167647012\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 71\n",
      "Train_loss: 3.8259241257692778\n",
      "val_loss: 0.22063561394575187\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 72\n",
      "Train_loss: 3.66351636754647\n",
      "val_loss: 0.19089627420149488\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 73\n",
      "Train_loss: 3.4890191487536897\n",
      "val_loss: 0.168067310617081\n",
      "best loss: 0.16488297812970112\n",
      "*********************************\n",
      "epoch 74\n",
      "Train_loss: 3.3591132285451484\n",
      "val_loss: 0.16136469580726925\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 75\n",
      "Train_loss: 3.3439352891307585\n",
      "val_loss: 0.16492391899439016\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 76\n",
      "Train_loss: 3.4410228734393447\n",
      "val_loss: 0.18133975555329446\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 77\n",
      "Train_loss: 3.617866335885569\n",
      "val_loss: 0.22350438384863533\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 78\n",
      "Train_loss: 3.7826614577736812\n",
      "val_loss: 0.28535536601077716\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 79\n",
      "Train_loss: 3.865332245220994\n",
      "val_loss: 0.2190572393168056\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 80\n",
      "Train_loss: 3.854127280391585\n",
      "val_loss: 0.27303062919278914\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 81\n",
      "Train_loss: 3.7641583256033497\n",
      "val_loss: 0.21620808761204072\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 82\n",
      "Train_loss: 3.607047355991679\n",
      "val_loss: 0.1837885720011355\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 83\n",
      "Train_loss: 3.4357606737471564\n",
      "val_loss: 0.16703326501945132\n",
      "best loss: 0.16136469580726925\n",
      "*********************************\n",
      "epoch 84\n",
      "Train_loss: 3.312594309013218\n",
      "val_loss: 0.15719093722610822\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 85\n",
      "Train_loss: 3.2960777774282275\n",
      "val_loss: 0.1617113726200794\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 86\n",
      "Train_loss: 3.3917712074160375\n",
      "val_loss: 0.181383295033228\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 87\n",
      "Train_loss: 3.563669471116322\n",
      "val_loss: 0.20374138356698748\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 88\n",
      "Train_loss: 3.7223439966388647\n",
      "val_loss: 0.25580584608304685\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 89\n",
      "Train_loss: 3.814347186114422\n",
      "val_loss: 0.22498769409399166\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 90\n",
      "Train_loss: 3.8165099384812335\n",
      "val_loss: 0.24151790149101943\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 91\n",
      "Train_loss: 3.7258110983139536\n",
      "val_loss: 0.21355549853346453\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 92\n",
      "Train_loss: 3.5566209574299164\n",
      "val_loss: 0.18075894324723496\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 93\n",
      "Train_loss: 3.391150286329275\n",
      "val_loss: 0.1608884602287935\n",
      "best loss: 0.15719093722610822\n",
      "*********************************\n",
      "epoch 94\n",
      "Train_loss: 3.2702904618430124\n",
      "val_loss: 0.15520026763913147\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 95\n",
      "Train_loss: 3.2554772550256255\n",
      "val_loss: 0.15940822334993213\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 96\n",
      "Train_loss: 3.3483331706752706\n",
      "val_loss: 0.17469142065645743\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 97\n",
      "Train_loss: 3.509772895992439\n",
      "val_loss: 0.20231389582773585\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 98\n",
      "Train_loss: 3.678082762908851\n",
      "val_loss: 0.23512931481412408\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 99\n",
      "Train_loss: 3.7629115047444746\n",
      "val_loss: 0.21335817475731012\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 100\n",
      "Train_loss: 3.7697627151872255\n",
      "val_loss: 0.21694191770666987\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 101\n",
      "Train_loss: 3.7662759152970047\n",
      "val_loss: 0.23585180099220346\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 102\n",
      "Train_loss: 3.7997957123832986\n",
      "val_loss: 0.24494894949568813\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 103\n",
      "Train_loss: 3.760628590621987\n",
      "val_loss: 0.2273768660048796\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 104\n",
      "Train_loss: 3.749439965767533\n",
      "val_loss: 0.20039299390597304\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 105\n",
      "Train_loss: 3.738437927896637\n",
      "val_loss: 0.22357001947689767\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 106\n",
      "Train_loss: 3.7545909309415606\n",
      "val_loss: 0.2121632809613755\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 107\n",
      "Train_loss: 3.733246811435636\n",
      "val_loss: 0.22346881315833145\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 108\n",
      "Train_loss: 3.7274864884162278\n",
      "val_loss: 0.22377531769711906\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 109\n",
      "Train_loss: 3.727454393871904\n",
      "val_loss: 0.20939201424405776\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 110\n",
      "Train_loss: 3.7232117296907954\n",
      "val_loss: 0.21976132676590548\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 111\n",
      "Train_loss: 3.711053446638408\n",
      "val_loss: 0.21039707256129903\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 112\n",
      "Train_loss: 3.7090392561192513\n",
      "val_loss: 0.23383581735775827\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 113\n",
      "Train_loss: 3.700017137061843\n",
      "val_loss: 0.21844897476471659\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 114\n",
      "Train_loss: 3.685468522491262\n",
      "val_loss: 0.2030664324648769\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "Epoch    16: reducing learning rate of group 0 to 1.5000e-03.\n",
      "epoch 115\n",
      "Train_loss: 3.6826722231895115\n",
      "val_loss: 0.20323969303373698\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 116\n",
      "Train_loss: 3.3980007706625863\n",
      "val_loss: 0.1674553742948367\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 117\n",
      "Train_loss: 3.3407943161652573\n",
      "val_loss: 0.16646195283687087\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 118\n",
      "Train_loss: 3.3110854097898876\n",
      "val_loss: 0.16686720396475999\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 119\n",
      "Train_loss: 3.303745319094474\n",
      "val_loss: 0.16019326605757056\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 120\n",
      "Train_loss: 3.2881997713738884\n",
      "val_loss: 0.1634571732801248\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 121\n",
      "Train_loss: 3.279646997961018\n",
      "val_loss: 0.16378023553244572\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 122\n",
      "Train_loss: 3.272292629535794\n",
      "val_loss: 0.16119618941799901\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123\n",
      "Train_loss: 3.2647767993629166\n",
      "val_loss: 0.16847148239720477\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 124\n",
      "Train_loss: 3.254468933609772\n",
      "val_loss: 0.1599028585692875\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 125\n",
      "Train_loss: 3.252344476456792\n",
      "val_loss: 0.16116828401759772\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 126\n",
      "Train_loss: 3.2424999823589706\n",
      "val_loss: 0.16814995742923988\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 127\n",
      "Train_loss: 3.2355989912190704\n",
      "val_loss: 0.15864755668547728\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 128\n",
      "Train_loss: 3.232408497911293\n",
      "val_loss: 0.1641572721870176\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 129\n",
      "Train_loss: 3.2293939099797524\n",
      "val_loss: 0.15659908560722996\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 130\n",
      "Train_loss: 3.227822387789756\n",
      "val_loss: 0.15825166578296418\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 131\n",
      "Train_loss: 3.2160752933113894\n",
      "val_loss: 0.16233189201149636\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 132\n",
      "Train_loss: 3.2155402652344276\n",
      "val_loss: 0.16048259852694083\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 133\n",
      "Train_loss: 3.207065959863787\n",
      "val_loss: 0.16122348381817486\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 134\n",
      "Train_loss: 3.203354743505964\n",
      "val_loss: 0.15586740202356103\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 135\n",
      "Train_loss: 3.206718960782565\n",
      "val_loss: 0.15765674815837583\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 136\n",
      "Train_loss: 3.196524271086408\n",
      "val_loss: 0.15956875887646968\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 137\n",
      "Train_loss: 3.1948437611194396\n",
      "val_loss: 0.1616986492626223\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 138\n",
      "Train_loss: 3.1871647054364343\n",
      "val_loss: 0.1611380804334449\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 139\n",
      "Train_loss: 3.1836849139758114\n",
      "val_loss: 0.15629779509659397\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 140\n",
      "Train_loss: 3.176388906098423\n",
      "val_loss: 0.15702518351344824\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 141\n",
      "Train_loss: 3.1755440947510154\n",
      "val_loss: 0.15669445678374178\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 142\n",
      "Train_loss: 3.1740092842203076\n",
      "val_loss: 0.15893964933536991\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 143\n",
      "Train_loss: 3.1712205840960874\n",
      "val_loss: 0.1589200211987131\n",
      "best loss: 0.15520026763913147\n",
      "*********************************\n",
      "epoch 144\n",
      "Train_loss: 3.166216428962705\n",
      "val_loss: 0.1539595328925569\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 145\n",
      "Train_loss: 3.164739117537801\n",
      "val_loss: 0.15849054820931885\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 146\n",
      "Train_loss: 3.159388600486222\n",
      "val_loss: 0.15771470528901962\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 147\n",
      "Train_loss: 3.1587956791746534\n",
      "val_loss: 0.15858652916589935\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 148\n",
      "Train_loss: 3.1602362542598086\n",
      "val_loss: 0.16103244018006202\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 149\n",
      "Train_loss: 3.1602767867532995\n",
      "val_loss: 0.15567298066254248\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 150\n",
      "Train_loss: 3.146317575944388\n",
      "val_loss: 0.15486570590849608\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 151\n",
      "Train_loss: 3.1452378192170185\n",
      "val_loss: 0.15607930055402602\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 152\n",
      "Train_loss: 3.145654204922944\n",
      "val_loss: 0.15824384656692575\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 153\n",
      "Train_loss: 3.1377399585780585\n",
      "val_loss: 0.15648899477151496\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 154\n",
      "Train_loss: 3.137465549845259\n",
      "val_loss: 0.1543747442710065\n",
      "best loss: 0.1539595328925569\n",
      "*********************************\n",
      "epoch 155\n",
      "Train_loss: 3.141816724995302\n",
      "val_loss: 0.15377017319726183\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 156\n",
      "Train_loss: 3.1331576799749166\n",
      "val_loss: 0.16190701970669288\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 157\n",
      "Train_loss: 3.1368600115798326\n",
      "val_loss: 0.16166844233828062\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 158\n",
      "Train_loss: 3.1236169879320714\n",
      "val_loss: 0.15805295105345174\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 159\n",
      "Train_loss: 3.1288102914615283\n",
      "val_loss: 0.153835642446633\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 160\n",
      "Train_loss: 3.124761388006295\n",
      "val_loss: 0.15711457884079552\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 161\n",
      "Train_loss: 3.1210943213437257\n",
      "val_loss: 0.1561754378600912\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 162\n",
      "Train_loss: 3.1247972551445806\n",
      "val_loss: 0.15665474514938094\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 163\n",
      "Train_loss: 3.1160456593236563\n",
      "val_loss: 0.16029891115895112\n",
      "best loss: 0.15377017319726183\n",
      "*********************************\n",
      "epoch 164\n",
      "Train_loss: 3.1124881821894967\n",
      "val_loss: 0.15352361997323696\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 165\n",
      "Train_loss: 3.1121237601735117\n",
      "val_loss: 0.15508910762873737\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 166\n",
      "Train_loss: 3.1090425526082774\n",
      "val_loss: 0.15943842404420747\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 167\n",
      "Train_loss: 3.1141850747314113\n",
      "val_loss: 0.15509217596855304\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 168\n",
      "Train_loss: 3.1055053584065524\n",
      "val_loss: 0.15904759192508022\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 169\n",
      "Train_loss: 3.0977415867126035\n",
      "val_loss: 0.15596737007622366\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 170\n",
      "Train_loss: 3.108541703308685\n",
      "val_loss: 0.15411500767505343\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 171\n",
      "Train_loss: 3.1003434429483407\n",
      "val_loss: 0.1569189335470429\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 172\n",
      "Train_loss: 3.100457881176635\n",
      "val_loss: 0.15535850482426303\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 173\n",
      "Train_loss: 3.102499358047847\n",
      "val_loss: 0.15781697300204311\n",
      "best loss: 0.15352361997323696\n",
      "*********************************\n",
      "epoch 174\n",
      "Train_loss: 3.0905888471529184\n",
      "val_loss: 0.15309061721528036\n",
      "best loss: 0.15309061721528036\n",
      "*********************************\n",
      "epoch 175\n",
      "Train_loss: 3.0901020645984163\n",
      "val_loss: 0.15391338009443137\n",
      "best loss: 0.15309061721528036\n",
      "*********************************\n",
      "epoch 176\n",
      "Train_loss: 3.092112237815711\n",
      "val_loss: 0.15588166116510524\n",
      "best loss: 0.15309061721528036\n",
      "*********************************\n",
      "epoch 177\n",
      "Train_loss: 3.0849755471596523\n",
      "val_loss: 0.1582731317833422\n",
      "best loss: 0.15309061721528036\n",
      "*********************************\n",
      "epoch 178\n",
      "Train_loss: 3.0816745034824082\n",
      "val_loss: 0.16169144064241953\n",
      "best loss: 0.15309061721528036\n",
      "*********************************\n",
      "epoch 179\n",
      "Train_loss: 3.082595462730272\n",
      "val_loss: 0.15211422373555872\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n",
      "epoch 180\n",
      "Train_loss: 3.0833617667852082\n",
      "val_loss: 0.1548888558291698\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n",
      "epoch 181\n",
      "Train_loss: 3.0819672324015603\n",
      "val_loss: 0.15648915234928124\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n",
      "epoch 182\n",
      "Train_loss: 3.085895031233745\n",
      "val_loss: 0.1557389127655632\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n",
      "epoch 183\n",
      "Train_loss: 3.076950172952233\n",
      "val_loss: 0.15932925626218455\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 184\n",
      "Train_loss: 3.0713016077573334\n",
      "val_loss: 0.15319304422642488\n",
      "best loss: 0.15211422373555872\n",
      "*********************************\n",
      "epoch 185\n",
      "Train_loss: 3.0748411136061335\n",
      "val_loss: 0.15196564786544114\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 186\n",
      "Train_loss: 3.0710533077921296\n",
      "val_loss: 0.1547790753156144\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 187\n",
      "Train_loss: 3.0700384214182566\n",
      "val_loss: 0.15605124683652455\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 188\n",
      "Train_loss: 3.069523061489357\n",
      "val_loss: 0.1594861147211352\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 189\n",
      "Train_loss: 3.069437528781777\n",
      "val_loss: 0.15225727765112193\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 190\n",
      "Train_loss: 3.0669373490933167\n",
      "val_loss: 0.15794454541652558\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 191\n",
      "Train_loss: 3.064312630146729\n",
      "val_loss: 0.1571315972121577\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 192\n",
      "Train_loss: 3.053854673769095\n",
      "val_loss: 0.1560498673417983\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 193\n",
      "Train_loss: 3.0599302461112705\n",
      "val_loss: 0.15546936346126708\n",
      "best loss: 0.15196564786544114\n",
      "*********************************\n",
      "epoch 194\n",
      "Train_loss: 3.050144329176298\n",
      "val_loss: 0.15039341446769705\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 195\n",
      "Train_loss: 3.0537453229825813\n",
      "val_loss: 0.1540993646105473\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 196\n",
      "Train_loss: 3.054020732312305\n",
      "val_loss: 0.1552051759943224\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 197\n",
      "Train_loss: 3.0550469448887987\n",
      "val_loss: 0.15426480860891606\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 198\n",
      "Train_loss: 3.0443724799993017\n",
      "val_loss: 0.1585172302955793\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 199\n",
      "Train_loss: 3.049518285661633\n",
      "val_loss: 0.15092439007280678\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 200\n",
      "Train_loss: 3.045706558641306\n",
      "val_loss: 0.1534489480433992\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 201\n",
      "Train_loss: 3.04779974325697\n",
      "val_loss: 0.15545463878172197\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 202\n",
      "Train_loss: 3.0489439700854573\n",
      "val_loss: 0.1552122241841582\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 203\n",
      "Train_loss: 3.0400757550960975\n",
      "val_loss: 0.1601204361191679\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 204\n",
      "Train_loss: 3.034165371484625\n",
      "val_loss: 0.15135494141171538\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "Epoch   106: reducing learning rate of group 0 to 4.5000e-04.\n",
      "epoch 205\n",
      "Train_loss: 3.0354189176704764\n",
      "val_loss: 0.15250260316166223\n",
      "best loss: 0.15039341446769705\n",
      "*********************************\n",
      "epoch 206\n",
      "Train_loss: 2.9182311862737214\n",
      "val_loss: 0.1450233802771093\n",
      "best loss: 0.1450233802771093\n",
      "*********************************\n",
      "epoch 207\n",
      "Train_loss: 2.8975908768190926\n",
      "val_loss: 0.144691271639955\n",
      "best loss: 0.144691271639955\n",
      "*********************************\n",
      "epoch 208\n",
      "Train_loss: 2.8940547055611847\n",
      "val_loss: 0.14441635154602794\n",
      "best loss: 0.14441635154602794\n",
      "*********************************\n",
      "epoch 209\n",
      "Train_loss: 2.8877132644310315\n",
      "val_loss: 0.14285945219567547\n",
      "best loss: 0.14285945219567547\n",
      "*********************************\n",
      "epoch 210\n",
      "Train_loss: 2.8827979390901257\n",
      "val_loss: 0.14251952195468823\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 211\n",
      "Train_loss: 2.8807688484644998\n",
      "val_loss: 0.14483294471388758\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 212\n",
      "Train_loss: 2.882321441056817\n",
      "val_loss: 0.1440684480336164\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 213\n",
      "Train_loss: 2.8752808433174355\n",
      "val_loss: 0.14379383726636044\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 214\n",
      "Train_loss: 2.8748864965730925\n",
      "val_loss: 0.14298027707004848\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 215\n",
      "Train_loss: 2.8745464283143436\n",
      "val_loss: 0.1434365817553687\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 216\n",
      "Train_loss: 2.8702021931138706\n",
      "val_loss: 0.14289456380469548\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 217\n",
      "Train_loss: 2.8691266668064617\n",
      "val_loss: 0.1442305147005995\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 218\n",
      "Train_loss: 2.8693036605750653\n",
      "val_loss: 0.14446893981311662\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 219\n",
      "Train_loss: 2.8677935325322284\n",
      "val_loss: 0.1427734343968876\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 220\n",
      "Train_loss: 2.8682572418961683\n",
      "val_loss: 0.1431368232742638\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "Epoch   122: reducing learning rate of group 0 to 1.3500e-04.\n",
      "epoch 221\n",
      "Train_loss: 2.863725827944648\n",
      "val_loss: 0.1441443478515254\n",
      "best loss: 0.14251952195468823\n",
      "*********************************\n",
      "epoch 222\n",
      "Train_loss: 2.8239295685015637\n",
      "val_loss: 0.14213896003372575\n",
      "best loss: 0.14213896003372575\n",
      "*********************************\n",
      "epoch 223\n",
      "Train_loss: 2.816925554923893\n",
      "val_loss: 0.14186174500514032\n",
      "best loss: 0.14186174500514032\n",
      "*********************************\n",
      "epoch 224\n",
      "Train_loss: 2.81389360481168\n",
      "val_loss: 0.14089714982592183\n",
      "best loss: 0.14089714982592183\n",
      "*********************************\n",
      "epoch 225\n",
      "Train_loss: 2.8101964786638005\n",
      "val_loss: 0.14077080823758464\n",
      "best loss: 0.14077080823758464\n",
      "*********************************\n",
      "epoch 226\n",
      "Train_loss: 2.8112931274469055\n",
      "val_loss: 0.14102487550309775\n",
      "best loss: 0.14077080823758464\n",
      "*********************************\n",
      "epoch 227\n",
      "Train_loss: 2.8122906526651166\n",
      "val_loss: 0.14144192559265742\n",
      "best loss: 0.14077080823758464\n",
      "*********************************\n",
      "epoch 228\n",
      "Train_loss: 2.808475011365585\n",
      "val_loss: 0.14083121979411917\n",
      "best loss: 0.14077080823758464\n",
      "*********************************\n",
      "epoch 229\n",
      "Train_loss: 2.807280247300216\n",
      "val_loss: 0.1405423496768355\n",
      "best loss: 0.1405423496768355\n",
      "*********************************\n",
      "epoch 230\n",
      "Train_loss: 2.8088450285070237\n",
      "val_loss: 0.14067436546281742\n",
      "best loss: 0.1405423496768355\n",
      "*********************************\n",
      "epoch 231\n",
      "Train_loss: 2.806490894748091\n",
      "val_loss: 0.13987444939395566\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 232\n",
      "Train_loss: 2.807592543958357\n",
      "val_loss: 0.1405372875547232\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 233\n",
      "Train_loss: 2.8039060207302016\n",
      "val_loss: 0.14068586266022784\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 234\n",
      "Train_loss: 2.805244386861819\n",
      "val_loss: 0.14041140501806298\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 235\n",
      "Train_loss: 2.8037558330711443\n",
      "val_loss: 0.1404517325608358\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 236\n",
      "Train_loss: 2.8029527193903934\n",
      "val_loss: 0.1413815173955514\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 237\n",
      "Train_loss: 2.8059760824485203\n",
      "val_loss: 0.14083566600515454\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 238\n",
      "Train_loss: 2.8028939170410303\n",
      "val_loss: 0.14132907917982737\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 239\n",
      "Train_loss: 2.801086007833817\n",
      "val_loss: 0.14067466945773754\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 240\n",
      "Train_loss: 2.801838537284757\n",
      "val_loss: 0.14047933041673455\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 241\n",
      "Train_loss: 2.8014730251413806\n",
      "val_loss: 0.14038411352524927\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "Epoch   143: reducing learning rate of group 0 to 4.0500e-05.\n",
      "epoch 242\n",
      "Train_loss: 2.8010631307236538\n",
      "val_loss: 0.14043440564431517\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 243\n",
      "Train_loss: 2.7886201126641357\n",
      "val_loss: 0.14036847184947418\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 244\n",
      "Train_loss: 2.787071502796006\n",
      "val_loss: 0.14016515908712543\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 245\n",
      "Train_loss: 2.7842413387547023\n",
      "val_loss: 0.13992918591728634\n",
      "best loss: 0.13987444939395566\n",
      "*********************************\n",
      "epoch 246\n",
      "Train_loss: 2.7843713346604444\n",
      "val_loss: 0.13945785356886536\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 247\n",
      "Train_loss: 2.78376414785101\n",
      "val_loss: 0.13962344909171442\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 248\n",
      "Train_loss: 2.7845891489790384\n",
      "val_loss: 0.14014031950637532\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 249\n",
      "Train_loss: 2.7817043159743826\n",
      "val_loss: 0.1396827866470543\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 250\n",
      "Train_loss: 2.78242111551502\n",
      "val_loss: 0.13998714410058508\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 251\n",
      "Train_loss: 2.783311734748565\n",
      "val_loss: 0.1397430448894813\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 252\n",
      "Train_loss: 2.7836520015935204\n",
      "val_loss: 0.13978843245653136\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 253\n",
      "Train_loss: 2.781669161945022\n",
      "val_loss: 0.13956027911245159\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 254\n",
      "Train_loss: 2.7834141938832\n",
      "val_loss: 0.13978827632849747\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 255\n",
      "Train_loss: 2.7837359392809886\n",
      "val_loss: 0.13979333798798108\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 256\n",
      "Train_loss: 2.7819703927179518\n",
      "val_loss: 0.13975960568262347\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "Epoch   158: reducing learning rate of group 0 to 1.2150e-05.\n",
      "epoch 257\n",
      "Train_loss: 2.7823430537167164\n",
      "val_loss: 0.1396705209361309\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 258\n",
      "Train_loss: 2.7768975208529922\n",
      "val_loss: 0.13947855284027144\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 259\n",
      "Train_loss: 2.776976644554522\n",
      "val_loss: 0.13953129839687314\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 260\n",
      "Train_loss: 2.777698188562151\n",
      "val_loss: 0.13959078855578652\n",
      "best loss: 0.13945785356886536\n",
      "*********************************\n",
      "epoch 261\n",
      "Train_loss: 2.7774723909151215\n",
      "val_loss: 0.13934316354192236\n",
      "best loss: 0.13934316354192236\n",
      "*********************************\n",
      "epoch 262\n",
      "Train_loss: 2.777464922026543\n",
      "val_loss: 0.13934914356550238\n",
      "best loss: 0.13934316354192236\n",
      "*********************************\n",
      "epoch 263\n",
      "Train_loss: 2.7766872901039386\n",
      "val_loss: 0.13941737596059067\n",
      "best loss: 0.13934316354192236\n",
      "*********************************\n",
      "epoch 264\n",
      "Train_loss: 2.7782014335657523\n",
      "val_loss: 0.13927876606284212\n",
      "best loss: 0.13927876606284212\n",
      "*********************************\n",
      "epoch 265\n",
      "Train_loss: 2.775517063755735\n",
      "val_loss: 0.1394693542158901\n",
      "best loss: 0.13927876606284212\n",
      "*********************************\n",
      "epoch 266\n",
      "Train_loss: 2.7756885818984776\n",
      "val_loss: 0.13918891610361997\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 267\n",
      "Train_loss: 2.776936805649488\n",
      "val_loss: 0.13938348767278017\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 268\n",
      "Train_loss: 2.776737016176\n",
      "val_loss: 0.1395420316853154\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 269\n",
      "Train_loss: 2.7778176509503605\n",
      "val_loss: 0.13941507333760247\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 270\n",
      "Train_loss: 2.775028030804225\n",
      "val_loss: 0.139336263942799\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 271\n",
      "Train_loss: 2.776821301001407\n",
      "val_loss: 0.13936432387876452\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 272\n",
      "Train_loss: 2.7775277626841417\n",
      "val_loss: 0.1393108122191777\n",
      "best loss: 0.13918891610361997\n",
      "*********************************\n",
      "epoch 273\n",
      "Train_loss: 2.7752314081890384\n",
      "val_loss: 0.13898422115933365\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 274\n",
      "Train_loss: 2.776387391439381\n",
      "val_loss: 0.13920884720969517\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 275\n",
      "Train_loss: 2.7777280404919424\n",
      "val_loss: 0.13939882431955353\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 276\n",
      "Train_loss: 2.775117123560486\n",
      "val_loss: 0.13935113728062276\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 277\n",
      "Train_loss: 2.775648584617545\n",
      "val_loss: 0.13936033753124064\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 278\n",
      "Train_loss: 2.7754805096205524\n",
      "val_loss: 0.1394874447645004\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 279\n",
      "Train_loss: 2.7751320591210105\n",
      "val_loss: 0.13927799820814035\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 280\n",
      "Train_loss: 2.7760752994978968\n",
      "val_loss: 0.13929762373928928\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 281\n",
      "Train_loss: 2.7738355463574544\n",
      "val_loss: 0.139189528831545\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 282\n",
      "Train_loss: 2.7751814753436688\n",
      "val_loss: 0.13935144433095598\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 283\n",
      "Train_loss: 2.7761661735435506\n",
      "val_loss: 0.139332278204866\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch 284\n",
      "Train_loss: 2.776316557344604\n",
      "val_loss: 0.13930973907398422\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 285\n",
      "Train_loss: 2.772836932904937\n",
      "val_loss: 0.13926450596752232\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 286\n",
      "Train_loss: 2.7724018353834987\n",
      "val_loss: 0.13934301246552724\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 287\n",
      "Train_loss: 2.7758105982413976\n",
      "val_loss: 0.13928091255574895\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 288\n",
      "Train_loss: 2.776588426538274\n",
      "val_loss: 0.13908971115333962\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 289\n",
      "Train_loss: 2.7728492533738107\n",
      "val_loss: 0.13916974836447196\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 290\n",
      "Train_loss: 2.7753775108705168\n",
      "val_loss: 0.1391964283294978\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 291\n",
      "Train_loss: 2.776551042847213\n",
      "val_loss: 0.13927155952230721\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 292\n",
      "Train_loss: 2.7744866615992754\n",
      "val_loss: 0.13959339583934718\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 293\n",
      "Train_loss: 2.774229083739623\n",
      "val_loss: 0.1393292129974219\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 294\n",
      "Train_loss: 2.7752387510778496\n",
      "val_loss: 0.13922556064162395\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 295\n",
      "Train_loss: 2.7725422466899436\n",
      "val_loss: 0.1392661910447635\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 296\n",
      "Train_loss: 2.7749379650456194\n",
      "val_loss: 0.13948483953837806\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 297\n",
      "Train_loss: 2.773811239825358\n",
      "val_loss: 0.13935205653196855\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 298\n",
      "Train_loss: 2.7752843240390925\n",
      "val_loss: 0.13943163324406402\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "epoch 299\n",
      "Train_loss: 2.773119937837488\n",
      "val_loss: 0.13941047247137264\n",
      "best loss: 0.13898422115933365\n",
      "*********************************\n",
      "fold 3 score: 0.13898421947733097\n",
      "fold: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Train_loss: 16.083799121444866\n",
      "val_loss: 2.0572829801258607\n",
      "best loss: 2.0572829801258607\n",
      "*********************************\n",
      "epoch 1\n",
      "Train_loss: 6.483368869553994\n",
      "val_loss: 1.257884837947335\n",
      "best loss: 1.257884837947335\n",
      "*********************************\n",
      "epoch 2\n",
      "Train_loss: 5.976434335760088\n",
      "val_loss: 0.969340746856444\n",
      "best loss: 0.969340746856444\n",
      "*********************************\n",
      "epoch 3\n",
      "Train_loss: 5.650135253885095\n",
      "val_loss: 0.7265890116600879\n",
      "best loss: 0.7265890116600879\n",
      "*********************************\n",
      "epoch 4\n",
      "Train_loss: 5.418634197687749\n",
      "val_loss: 0.6302943036046065\n",
      "best loss: 0.6302943036046065\n",
      "*********************************\n",
      "epoch 5\n",
      "Train_loss: 5.389579655964895\n",
      "val_loss: 0.7913249828108825\n",
      "best loss: 0.6302943036046065\n",
      "*********************************\n",
      "epoch 6\n",
      "Train_loss: 5.485076296830385\n",
      "val_loss: 1.241960748161058\n",
      "best loss: 0.6302943036046065\n",
      "*********************************\n",
      "epoch 7\n",
      "Train_loss: 5.5230641651939285\n",
      "val_loss: 1.1178015847824836\n",
      "best loss: 0.6302943036046065\n",
      "*********************************\n",
      "epoch 8\n",
      "Train_loss: 5.4528555538999255\n",
      "val_loss: 0.8415972645969171\n",
      "best loss: 0.6302943036046065\n",
      "*********************************\n",
      "epoch 9\n",
      "Train_loss: 5.284379875537318\n",
      "val_loss: 0.5809896269584085\n",
      "best loss: 0.5809896269584085\n",
      "*********************************\n",
      "epoch 10\n",
      "Train_loss: 5.1370320925956054\n",
      "val_loss: 0.6989792235775153\n",
      "best loss: 0.5809896269584085\n",
      "*********************************\n",
      "epoch 11\n",
      "Train_loss: 4.953039265096353\n",
      "val_loss: 0.53172212529678\n",
      "best loss: 0.53172212529678\n",
      "*********************************\n",
      "epoch 12\n",
      "Train_loss: 4.711650105711566\n",
      "val_loss: 0.40403005015519666\n",
      "best loss: 0.40403005015519666\n",
      "*********************************\n",
      "epoch 13\n",
      "Train_loss: 4.478295345057448\n",
      "val_loss: 0.33094123199181263\n",
      "best loss: 0.33094123199181263\n",
      "*********************************\n",
      "epoch 14\n",
      "Train_loss: 4.298413472765879\n",
      "val_loss: 0.29790853574748927\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 15\n",
      "Train_loss: 4.274480267616845\n",
      "val_loss: 0.3100585863800321\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 16\n",
      "Train_loss: 4.408501912984377\n",
      "val_loss: 0.37601346341237485\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 17\n",
      "Train_loss: 4.5891071176737634\n",
      "val_loss: 0.48521687919798095\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 18\n",
      "Train_loss: 4.718632701110523\n",
      "val_loss: 0.4923434578171731\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 19\n",
      "Train_loss: 4.70697236265277\n",
      "val_loss: 0.39346884529160475\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 20\n",
      "Train_loss: 4.632800600262708\n",
      "val_loss: 0.4063230474821911\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 21\n",
      "Train_loss: 4.487135850342602\n",
      "val_loss: 0.3561154232003557\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 22\n",
      "Train_loss: 4.296191609834507\n",
      "val_loss: 0.3126814647205654\n",
      "best loss: 0.29790853574748927\n",
      "*********************************\n",
      "epoch 23\n",
      "Train_loss: 4.090869482267686\n",
      "val_loss: 0.24719495180616483\n",
      "best loss: 0.24719495180616483\n",
      "*********************************\n",
      "epoch 24\n",
      "Train_loss: 3.9257213048187203\n",
      "val_loss: 0.22891890125917655\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 25\n",
      "Train_loss: 3.90356198986206\n",
      "val_loss: 0.24091375728069317\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 26\n",
      "Train_loss: 4.033871324625211\n",
      "val_loss: 0.2726823024049027\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 27\n",
      "Train_loss: 4.220689481761188\n",
      "val_loss: 0.3471660820889085\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 28\n",
      "Train_loss: 4.356959990074362\n",
      "val_loss: 0.4127081699945494\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 29\n",
      "Train_loss: 4.418827317291606\n",
      "val_loss: 0.3305900091131497\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 30\n",
      "Train_loss: 4.371052890701382\n",
      "val_loss: 0.36855361724678515\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 31\n",
      "Train_loss: 4.248900433581491\n",
      "val_loss: 0.2857827457081336\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 32\n",
      "Train_loss: 4.0800764694527665\n",
      "val_loss: 0.2892112442222506\n",
      "best loss: 0.22891890125917655\n",
      "*********************************\n",
      "epoch 33\n",
      "Train_loss: 3.8838475028843686\n",
      "val_loss: 0.2149856737254612\n",
      "best loss: 0.2149856737254612\n",
      "*********************************\n",
      "epoch 34\n",
      "Train_loss: 3.7315892556931956\n",
      "val_loss: 0.20208740815537954\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 35\n",
      "Train_loss: 3.710922655413567\n",
      "val_loss: 0.20861053243446864\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 36\n",
      "Train_loss: 3.8308239889073326\n",
      "val_loss: 0.25093875754179623\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 37\n",
      "Train_loss: 4.009596997215973\n",
      "val_loss: 0.2972829732170307\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 38\n",
      "Train_loss: 4.166136754205488\n",
      "val_loss: 0.3218765787999379\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 39\n",
      "Train_loss: 4.237537932157728\n",
      "val_loss: 0.28339841562350376\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 40\n",
      "Train_loss: 4.213301745792006\n",
      "val_loss: 0.2918926066278601\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 41\n",
      "Train_loss: 4.106515146605452\n",
      "val_loss: 0.2540852692023013\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 42\n",
      "Train_loss: 3.9412976998156104\n",
      "val_loss: 0.2561338783144498\n",
      "best loss: 0.20208740815537954\n",
      "*********************************\n",
      "epoch 43\n",
      "Train_loss: 3.7503786106098462\n",
      "val_loss: 0.1968999839897872\n",
      "best loss: 0.1968999839897872\n",
      "*********************************\n",
      "epoch 44\n",
      "Train_loss: 3.6067174976151355\n",
      "val_loss: 0.1864106049257713\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 45\n",
      "Train_loss: 3.5883301236851306\n",
      "val_loss: 0.19231262910547178\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 46\n",
      "Train_loss: 3.697789001923287\n",
      "val_loss: 0.22363399490749702\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 47\n",
      "Train_loss: 3.8773803695434195\n",
      "val_loss: 0.28078428388783466\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 48\n",
      "Train_loss: 4.028891801117087\n",
      "val_loss: 0.31694153784399887\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 49\n",
      "Train_loss: 4.111888720772956\n",
      "val_loss: 0.2608986760547867\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 50\n",
      "Train_loss: 4.100095732719343\n",
      "val_loss: 0.26052994129926316\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 51\n",
      "Train_loss: 3.9984035439625627\n",
      "val_loss: 0.2589210128590084\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 52\n",
      "Train_loss: 3.830209467675981\n",
      "val_loss: 0.2194207484294334\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 53\n",
      "Train_loss: 3.6503678156250805\n",
      "val_loss: 0.18783073215746843\n",
      "best loss: 0.1864106049257713\n",
      "*********************************\n",
      "epoch 54\n",
      "Train_loss: 3.510878309402637\n",
      "val_loss: 0.17658792764589892\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 55\n",
      "Train_loss: 3.493292646338395\n",
      "val_loss: 0.18206518360474255\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 56\n",
      "Train_loss: 3.59975335583761\n",
      "val_loss: 0.20662242428686162\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 57\n",
      "Train_loss: 3.7755274042866622\n",
      "val_loss: 0.2904158394460235\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 58\n",
      "Train_loss: 3.9343402683606135\n",
      "val_loss: 0.32536741033657607\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 59\n",
      "Train_loss: 4.01806711426798\n",
      "val_loss: 0.2630077541922918\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 60\n",
      "Train_loss: 3.992171557823295\n",
      "val_loss: 0.24500097265433823\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 61\n",
      "Train_loss: 3.914369897660641\n",
      "val_loss: 0.24992620344291103\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62\n",
      "Train_loss: 3.7483377338575554\n",
      "val_loss: 0.19973608713904795\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 63\n",
      "Train_loss: 3.5736400641902613\n",
      "val_loss: 0.18034801466253453\n",
      "best loss: 0.17658792764589892\n",
      "*********************************\n",
      "epoch 64\n",
      "Train_loss: 3.4361455324780286\n",
      "val_loss: 0.17017609958096053\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 65\n",
      "Train_loss: 3.4204809467842616\n",
      "val_loss: 0.17732186230309263\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 66\n",
      "Train_loss: 3.521418842305153\n",
      "val_loss: 0.196561033173315\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 67\n",
      "Train_loss: 3.6896506779716645\n",
      "val_loss: 0.22774116772507366\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 68\n",
      "Train_loss: 3.851082552135407\n",
      "val_loss: 0.2705822750835465\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 69\n",
      "Train_loss: 3.9347006564969096\n",
      "val_loss: 0.24303803006155947\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 70\n",
      "Train_loss: 3.9240992220424413\n",
      "val_loss: 0.28140783788790513\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 71\n",
      "Train_loss: 3.8367944376891336\n",
      "val_loss: 0.22894530055185242\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 72\n",
      "Train_loss: 3.677183842206778\n",
      "val_loss: 0.22841231134495832\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 73\n",
      "Train_loss: 3.5132896056261846\n",
      "val_loss: 0.17695343692727367\n",
      "best loss: 0.17017609958096053\n",
      "*********************************\n",
      "epoch 74\n",
      "Train_loss: 3.3758783799498318\n",
      "val_loss: 0.16803478085195825\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 75\n",
      "Train_loss: 3.3607297471202218\n",
      "val_loss: 0.17064584152576612\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 76\n",
      "Train_loss: 3.4621257628438085\n",
      "val_loss: 0.1908120610659709\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 77\n",
      "Train_loss: 3.6250163902015884\n",
      "val_loss: 0.22741450003365088\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 78\n",
      "Train_loss: 3.77985884424705\n",
      "val_loss: 0.24255048983830163\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 79\n",
      "Train_loss: 3.8668575859119256\n",
      "val_loss: 0.2316753342391561\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 80\n",
      "Train_loss: 3.868289317086536\n",
      "val_loss: 0.23061535114159668\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 81\n",
      "Train_loss: 3.78259254186542\n",
      "val_loss: 0.2258603605095739\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 82\n",
      "Train_loss: 3.6229023871962327\n",
      "val_loss: 0.19453285823699615\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 83\n",
      "Train_loss: 3.4441626833665637\n",
      "val_loss: 0.17184459915053898\n",
      "best loss: 0.16803478085195825\n",
      "*********************************\n",
      "epoch 84\n",
      "Train_loss: 3.3240169245344053\n",
      "val_loss: 0.16376411684401687\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 85\n",
      "Train_loss: 3.308767000592882\n",
      "val_loss: 0.1679945583023273\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 86\n",
      "Train_loss: 3.4007428766662424\n",
      "val_loss: 0.18539205840441672\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 87\n",
      "Train_loss: 3.5699607365169763\n",
      "val_loss: 0.2065579418584134\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 88\n",
      "Train_loss: 3.7234763864224223\n",
      "val_loss: 0.2534598536381213\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 89\n",
      "Train_loss: 3.8269310860859953\n",
      "val_loss: 0.23134436378737083\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 90\n",
      "Train_loss: 3.8113000537236688\n",
      "val_loss: 0.2135703030697987\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 91\n",
      "Train_loss: 3.7191154385433873\n",
      "val_loss: 0.20897466866310552\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 92\n",
      "Train_loss: 3.5639518733079885\n",
      "val_loss: 0.18515657589829954\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 93\n",
      "Train_loss: 3.40006242366506\n",
      "val_loss: 0.16708147947872443\n",
      "best loss: 0.16376411684401687\n",
      "*********************************\n",
      "epoch 94\n",
      "Train_loss: 3.278318411189537\n",
      "val_loss: 0.1606635109428921\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 95\n",
      "Train_loss: 3.2639894046836098\n",
      "val_loss: 0.16375137805442583\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 96\n",
      "Train_loss: 3.3552950096372594\n",
      "val_loss: 0.18622041346060642\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 97\n",
      "Train_loss: 3.5184625969539063\n",
      "val_loss: 0.20272263986921668\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 98\n",
      "Train_loss: 3.682067095537571\n",
      "val_loss: 0.2347851530987823\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 99\n",
      "Train_loss: 3.766524319630576\n",
      "val_loss: 0.21331271634334203\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 100\n",
      "Train_loss: 3.786324188188295\n",
      "val_loss: 0.21906014805938823\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 101\n",
      "Train_loss: 3.7738876499684877\n",
      "val_loss: 0.25080980832489946\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 102\n",
      "Train_loss: 3.78529449218263\n",
      "val_loss: 0.24126267452123093\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 103\n",
      "Train_loss: 3.7654387678615255\n",
      "val_loss: 0.23973648142757162\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 104\n",
      "Train_loss: 3.7706614443082436\n",
      "val_loss: 0.2122117373617653\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 105\n",
      "Train_loss: 3.753686678576408\n",
      "val_loss: 0.2276007080067882\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 106\n",
      "Train_loss: 3.7397231559643673\n",
      "val_loss: 0.2510290242340807\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 107\n",
      "Train_loss: 3.7447134679737535\n",
      "val_loss: 0.2218810677620073\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 108\n",
      "Train_loss: 3.747566150691879\n",
      "val_loss: 0.24396233132406145\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 109\n",
      "Train_loss: 3.7438481124539766\n",
      "val_loss: 0.21796531107333447\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 110\n",
      "Train_loss: 3.7292906592743846\n",
      "val_loss: 0.21229339808736022\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 111\n",
      "Train_loss: 3.716929082013984\n",
      "val_loss: 0.2212730016089084\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 112\n",
      "Train_loss: 3.7122695469031615\n",
      "val_loss: 0.22533181610552083\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 113\n",
      "Train_loss: 3.7083499731515257\n",
      "val_loss: 0.22116293477519278\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 114\n",
      "Train_loss: 3.6939397869055517\n",
      "val_loss: 0.19809352869163113\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 115\n",
      "Train_loss: 3.6927442700638355\n",
      "val_loss: 0.21064669904601163\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 116\n",
      "Train_loss: 3.682918367962049\n",
      "val_loss: 0.21194554979298277\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 117\n",
      "Train_loss: 3.687053589781911\n",
      "val_loss: 0.21665295023021386\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 118\n",
      "Train_loss: 3.6777854282834466\n",
      "val_loss: 0.21646950713159302\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 119\n",
      "Train_loss: 3.6649757214245824\n",
      "val_loss: 0.21237660440316114\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 120\n",
      "Train_loss: 3.6672625162638073\n",
      "val_loss: 0.20407844759816857\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 121\n",
      "Train_loss: 3.658937300292323\n",
      "val_loss: 0.21020504552184643\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 122\n",
      "Train_loss: 3.6543004527643017\n",
      "val_loss: 0.22709734169243728\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 123\n",
      "Train_loss: 3.646331548902151\n",
      "val_loss: 0.21132168319120576\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124\n",
      "Train_loss: 3.641758701624104\n",
      "val_loss: 0.19435415846262827\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 125\n",
      "Train_loss: 3.6163587118206357\n",
      "val_loss: 0.1928327224131917\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 126\n",
      "Train_loss: 3.619924397361224\n",
      "val_loss: 0.21747208201263463\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 127\n",
      "Train_loss: 3.616057953573452\n",
      "val_loss: 0.22219867145181182\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 128\n",
      "Train_loss: 3.61471769536181\n",
      "val_loss: 0.2250852916645232\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 129\n",
      "Train_loss: 3.6155541028830793\n",
      "val_loss: 0.2008106626897117\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 130\n",
      "Train_loss: 3.611509263579901\n",
      "val_loss: 0.2041366165384181\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 131\n",
      "Train_loss: 3.6043508684942025\n",
      "val_loss: 0.20878982684408237\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 132\n",
      "Train_loss: 3.5860367195099228\n",
      "val_loss: 0.2169787019504341\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 133\n",
      "Train_loss: 3.5904438078355225\n",
      "val_loss: 0.2128547878009404\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 134\n",
      "Train_loss: 3.581295625265047\n",
      "val_loss: 0.19837429345193433\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 135\n",
      "Train_loss: 3.5767428864723287\n",
      "val_loss: 0.2023674091260334\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "Epoch    37: reducing learning rate of group 0 to 1.5000e-03.\n",
      "epoch 136\n",
      "Train_loss: 3.5779532107857532\n",
      "val_loss: 0.22441965087694166\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 137\n",
      "Train_loss: 3.307866519513124\n",
      "val_loss: 0.1688368744613405\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 138\n",
      "Train_loss: 3.2555736487809193\n",
      "val_loss: 0.1647591746919197\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 139\n",
      "Train_loss: 3.228804437022886\n",
      "val_loss: 0.16218126928671364\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 140\n",
      "Train_loss: 3.2258273105760438\n",
      "val_loss: 0.16519821551872904\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 141\n",
      "Train_loss: 3.215249931363613\n",
      "val_loss: 0.16200703703192265\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 142\n",
      "Train_loss: 3.2031238661669414\n",
      "val_loss: 0.16429848478545211\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 143\n",
      "Train_loss: 3.1949128849607704\n",
      "val_loss: 0.16370210130083088\n",
      "best loss: 0.1606635109428921\n",
      "*********************************\n",
      "epoch 144\n",
      "Train_loss: 3.18843695608975\n",
      "val_loss: 0.15943834331106582\n",
      "best loss: 0.15943834331106582\n",
      "*********************************\n",
      "epoch 145\n",
      "Train_loss: 3.182559095960553\n",
      "val_loss: 0.16189881253270164\n",
      "best loss: 0.15943834331106582\n",
      "*********************************\n",
      "epoch 146\n",
      "Train_loss: 3.1846165438597835\n",
      "val_loss: 0.15902401865644317\n",
      "best loss: 0.15902401865644317\n",
      "*********************************\n",
      "epoch 147\n",
      "Train_loss: 3.173423034624449\n",
      "val_loss: 0.1625358789280181\n",
      "best loss: 0.15902401865644317\n",
      "*********************************\n",
      "epoch 148\n",
      "Train_loss: 3.1674198081386686\n",
      "val_loss: 0.16307624165248336\n",
      "best loss: 0.15902401865644317\n",
      "*********************************\n",
      "epoch 149\n",
      "Train_loss: 3.1600947037528337\n",
      "val_loss: 0.1578611768498164\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 150\n",
      "Train_loss: 3.154066894818898\n",
      "val_loss: 0.16027912033770558\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 151\n",
      "Train_loss: 3.1541344982876183\n",
      "val_loss: 0.16116641178086055\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 152\n",
      "Train_loss: 3.1421616870781723\n",
      "val_loss: 0.16152393997104178\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 153\n",
      "Train_loss: 3.143367157171749\n",
      "val_loss: 0.16002290992601037\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 154\n",
      "Train_loss: 3.142417688663703\n",
      "val_loss: 0.159718499464798\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 155\n",
      "Train_loss: 3.137426557628368\n",
      "val_loss: 0.15971236590453425\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 156\n",
      "Train_loss: 3.1379479026607116\n",
      "val_loss: 0.16412455996297148\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 157\n",
      "Train_loss: 3.1317087491323834\n",
      "val_loss: 0.15955132661350926\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 158\n",
      "Train_loss: 3.1240280479026357\n",
      "val_loss: 0.16070726255186185\n",
      "best loss: 0.1578611768498164\n",
      "*********************************\n",
      "epoch 159\n",
      "Train_loss: 3.1200681932895193\n",
      "val_loss: 0.15779946445242843\n",
      "best loss: 0.15779946445242843\n",
      "*********************************\n",
      "epoch 160\n",
      "Train_loss: 3.1250416730039214\n",
      "val_loss: 0.15864884020151632\n",
      "best loss: 0.15779946445242843\n",
      "*********************************\n",
      "epoch 161\n",
      "Train_loss: 3.117046308524358\n",
      "val_loss: 0.15747847468636977\n",
      "best loss: 0.15747847468636977\n",
      "*********************************\n",
      "epoch 162\n",
      "Train_loss: 3.111557725459281\n",
      "val_loss: 0.15964097878718114\n",
      "best loss: 0.15747847468636977\n",
      "*********************************\n",
      "epoch 163\n",
      "Train_loss: 3.108818063158806\n",
      "val_loss: 0.16033315730139733\n",
      "best loss: 0.15747847468636977\n",
      "*********************************\n",
      "epoch 164\n",
      "Train_loss: 3.10867319300818\n",
      "val_loss: 0.1559685484568152\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 165\n",
      "Train_loss: 3.1048533023711506\n",
      "val_loss: 0.15976240867252592\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 166\n",
      "Train_loss: 3.1067807470657853\n",
      "val_loss: 0.1584204176224039\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 167\n",
      "Train_loss: 3.099169845698257\n",
      "val_loss: 0.16019300455849234\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 168\n",
      "Train_loss: 3.1017713949525487\n",
      "val_loss: 0.16050432654485808\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 169\n",
      "Train_loss: 3.097582705083571\n",
      "val_loss: 0.15762906814276353\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 170\n",
      "Train_loss: 3.0925905149611586\n",
      "val_loss: 0.15812721390793272\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 171\n",
      "Train_loss: 3.087177215741262\n",
      "val_loss: 0.16165811012417508\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 172\n",
      "Train_loss: 3.0876395100570413\n",
      "val_loss: 0.15701702278061908\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 173\n",
      "Train_loss: 3.083921778434996\n",
      "val_loss: 0.16403736605647204\n",
      "best loss: 0.1559685484568152\n",
      "*********************************\n",
      "epoch 174\n",
      "Train_loss: 3.084525274542796\n",
      "val_loss: 0.1559117466017523\n",
      "best loss: 0.1559117466017523\n",
      "*********************************\n",
      "epoch 175\n",
      "Train_loss: 3.082984256949306\n",
      "val_loss: 0.15706000751345797\n",
      "best loss: 0.1559117466017523\n",
      "*********************************\n",
      "epoch 176\n",
      "Train_loss: 3.0795708234024275\n",
      "val_loss: 0.15796863392685526\n",
      "best loss: 0.1559117466017523\n",
      "*********************************\n",
      "epoch 177\n",
      "Train_loss: 3.0747047185355174\n",
      "val_loss: 0.1625004165194385\n",
      "best loss: 0.1559117466017523\n",
      "*********************************\n",
      "epoch 178\n",
      "Train_loss: 3.0787262226690544\n",
      "val_loss: 0.16106094703504614\n",
      "best loss: 0.1559117466017523\n",
      "*********************************\n",
      "epoch 179\n",
      "Train_loss: 3.0714066176538495\n",
      "val_loss: 0.15534928378669233\n",
      "best loss: 0.15534928378669233\n",
      "*********************************\n",
      "epoch 180\n",
      "Train_loss: 3.0692705349091507\n",
      "val_loss: 0.156856601224482\n",
      "best loss: 0.15534928378669233\n",
      "*********************************\n",
      "epoch 181\n",
      "Train_loss: 3.0609960859887924\n",
      "val_loss: 0.15703252904548218\n",
      "best loss: 0.15534928378669233\n",
      "*********************************\n",
      "epoch 182\n",
      "Train_loss: 3.0601735965733217\n",
      "val_loss: 0.15708901829599145\n",
      "best loss: 0.15534928378669233\n",
      "*********************************\n",
      "epoch 183\n",
      "Train_loss: 3.068008199678978\n",
      "val_loss: 0.15640513028350306\n",
      "best loss: 0.15534928378669233\n",
      "*********************************\n",
      "epoch 184\n",
      "Train_loss: 3.062885699685415\n",
      "val_loss: 0.15513344894608247\n",
      "best loss: 0.15513344894608247\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 185\n",
      "Train_loss: 3.060695544530543\n",
      "val_loss: 0.155539790234363\n",
      "best loss: 0.15513344894608247\n",
      "*********************************\n",
      "epoch 186\n",
      "Train_loss: 3.0552751506257385\n",
      "val_loss: 0.1591666337393565\n",
      "best loss: 0.15513344894608247\n",
      "*********************************\n",
      "epoch 187\n",
      "Train_loss: 3.0551433205755485\n",
      "val_loss: 0.1571909496090947\n",
      "best loss: 0.15513344894608247\n",
      "*********************************\n",
      "epoch 188\n",
      "Train_loss: 3.056672784668049\n",
      "val_loss: 0.16035986657045811\n",
      "best loss: 0.15513344894608247\n",
      "*********************************\n",
      "epoch 189\n",
      "Train_loss: 3.05526103844443\n",
      "val_loss: 0.15499221971057986\n",
      "best loss: 0.15499221971057986\n",
      "*********************************\n",
      "epoch 190\n",
      "Train_loss: 3.052735943084421\n",
      "val_loss: 0.1548233566679713\n",
      "best loss: 0.1548233566679713\n",
      "*********************************\n",
      "epoch 191\n",
      "Train_loss: 3.046504331550411\n",
      "val_loss: 0.15519347060328484\n",
      "best loss: 0.1548233566679713\n",
      "*********************************\n",
      "epoch 192\n",
      "Train_loss: 3.046997475768218\n",
      "val_loss: 0.1575450974463597\n",
      "best loss: 0.1548233566679713\n",
      "*********************************\n",
      "epoch 193\n",
      "Train_loss: 3.0460201893521077\n",
      "val_loss: 0.15920531325987328\n",
      "best loss: 0.1548233566679713\n",
      "*********************************\n",
      "epoch 194\n",
      "Train_loss: 3.0462397950167226\n",
      "val_loss: 0.1552363053187323\n",
      "best loss: 0.1548233566679713\n",
      "*********************************\n",
      "epoch 195\n",
      "Train_loss: 3.0397299311540507\n",
      "val_loss: 0.15268188685586326\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 196\n",
      "Train_loss: 3.0379079654390395\n",
      "val_loss: 0.15689467790740008\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 197\n",
      "Train_loss: 3.038895212217619\n",
      "val_loss: 0.1600216842959279\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 198\n",
      "Train_loss: 3.040387182015564\n",
      "val_loss: 0.16269860180205878\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 199\n",
      "Train_loss: 3.032941618543196\n",
      "val_loss: 0.1550005112427834\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 200\n",
      "Train_loss: 3.0340782722472994\n",
      "val_loss: 0.15716853593961472\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 201\n",
      "Train_loss: 3.031190813896465\n",
      "val_loss: 0.16030644835449445\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 202\n",
      "Train_loss: 3.0313017766206407\n",
      "val_loss: 0.1609996991711244\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 203\n",
      "Train_loss: 3.0250565534135156\n",
      "val_loss: 0.1614252350080243\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 204\n",
      "Train_loss: 3.0280029606121004\n",
      "val_loss: 0.1547079166612166\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 205\n",
      "Train_loss: 3.0243103403798304\n",
      "val_loss: 0.15632423174849788\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "Epoch   107: reducing learning rate of group 0 to 4.5000e-04.\n",
      "epoch 206\n",
      "Train_loss: 3.0249431586010815\n",
      "val_loss: 0.15754817089733356\n",
      "best loss: 0.15268188685586326\n",
      "*********************************\n",
      "epoch 207\n",
      "Train_loss: 2.916083360991629\n",
      "val_loss: 0.1477996366673586\n",
      "best loss: 0.1477996366673586\n",
      "*********************************\n",
      "epoch 208\n",
      "Train_loss: 2.8995641352492805\n",
      "val_loss: 0.14809806183018054\n",
      "best loss: 0.1477996366673586\n",
      "*********************************\n",
      "epoch 209\n",
      "Train_loss: 2.8952952545511685\n",
      "val_loss: 0.14670234399663423\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 210\n",
      "Train_loss: 2.8915027554172488\n",
      "val_loss: 0.1475001389192546\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 211\n",
      "Train_loss: 2.8894806050914594\n",
      "val_loss: 0.14778612871332647\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 212\n",
      "Train_loss: 2.8860753233601777\n",
      "val_loss: 0.14812124260207632\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 213\n",
      "Train_loss: 2.8824418695599308\n",
      "val_loss: 0.1472502263856933\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 214\n",
      "Train_loss: 2.879531501206236\n",
      "val_loss: 0.14683789382310886\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 215\n",
      "Train_loss: 2.8795352934523137\n",
      "val_loss: 0.14758380175822391\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 216\n",
      "Train_loss: 2.878404319065584\n",
      "val_loss: 0.1474436462996557\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 217\n",
      "Train_loss: 2.8762338781575667\n",
      "val_loss: 0.148050931113488\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 218\n",
      "Train_loss: 2.8736303370560674\n",
      "val_loss: 0.1478906680615742\n",
      "best loss: 0.14670234399663423\n",
      "*********************************\n",
      "epoch 219\n",
      "Train_loss: 2.87675730706314\n",
      "val_loss: 0.14609352158999248\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 220\n",
      "Train_loss: 2.8722340580892287\n",
      "val_loss: 0.14683405689537024\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 221\n",
      "Train_loss: 2.872241934165852\n",
      "val_loss: 0.1471719336721969\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 222\n",
      "Train_loss: 2.871649724338677\n",
      "val_loss: 0.1479794016608476\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 223\n",
      "Train_loss: 2.8710017198975395\n",
      "val_loss: 0.1471748475516876\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 224\n",
      "Train_loss: 2.86577196942404\n",
      "val_loss: 0.1468713595519018\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 225\n",
      "Train_loss: 2.866730150709411\n",
      "val_loss: 0.14682898731262958\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 226\n",
      "Train_loss: 2.8643035284028593\n",
      "val_loss: 0.14764934685742373\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 227\n",
      "Train_loss: 2.8652483430950277\n",
      "val_loss: 0.14726480686251844\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 228\n",
      "Train_loss: 2.8621272303033267\n",
      "val_loss: 0.14627788730026745\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 229\n",
      "Train_loss: 2.8630749356487177\n",
      "val_loss: 0.14661959969443325\n",
      "best loss: 0.14609352158999248\n",
      "*********************************\n",
      "epoch 230\n",
      "Train_loss: 2.861399638975588\n",
      "val_loss: 0.14582349715348752\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 231\n",
      "Train_loss: 2.8606060560176774\n",
      "val_loss: 0.14683113729514466\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 232\n",
      "Train_loss: 2.8601071111166103\n",
      "val_loss: 0.14715228543320877\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 233\n",
      "Train_loss: 2.8565471411932215\n",
      "val_loss: 0.14885348514827831\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 234\n",
      "Train_loss: 2.8599515379394385\n",
      "val_loss: 0.14707214947152059\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 235\n",
      "Train_loss: 2.8560349706733485\n",
      "val_loss: 0.1469395191563129\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 236\n",
      "Train_loss: 2.853376091458345\n",
      "val_loss: 0.14708074612432956\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 237\n",
      "Train_loss: 2.8570176597245815\n",
      "val_loss: 0.14777829795284825\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 238\n",
      "Train_loss: 2.8534521194208504\n",
      "val_loss: 0.14778659099168742\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 239\n",
      "Train_loss: 2.855708696236636\n",
      "val_loss: 0.14679568023627007\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 240\n",
      "Train_loss: 2.852195610150147\n",
      "val_loss: 0.14650462266233458\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "Epoch   142: reducing learning rate of group 0 to 1.3500e-04.\n",
      "epoch 241\n",
      "Train_loss: 2.850509961940024\n",
      "val_loss: 0.14710622813364638\n",
      "best loss: 0.14582349715348752\n",
      "*********************************\n",
      "epoch 242\n",
      "Train_loss: 2.8146731433334065\n",
      "val_loss: 0.14482383598365053\n",
      "best loss: 0.14482383598365053\n",
      "*********************************\n",
      "epoch 243\n",
      "Train_loss: 2.8101995887652875\n",
      "val_loss: 0.1439759977850891\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n",
      "epoch 244\n",
      "Train_loss: 2.8072632363847063\n",
      "val_loss: 0.14426705086579883\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 245\n",
      "Train_loss: 2.8076348440367704\n",
      "val_loss: 0.14408038461010292\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n",
      "epoch 246\n",
      "Train_loss: 2.8060839653971765\n",
      "val_loss: 0.14420273022814734\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n",
      "epoch 247\n",
      "Train_loss: 2.8060601696896668\n",
      "val_loss: 0.14461168188981197\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n",
      "epoch 248\n",
      "Train_loss: 2.803517262638548\n",
      "val_loss: 0.144338895427485\n",
      "best loss: 0.1439759977850891\n",
      "*********************************\n",
      "epoch 249\n",
      "Train_loss: 2.8021437361532184\n",
      "val_loss: 0.1438393733800109\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 250\n",
      "Train_loss: 2.802381591140181\n",
      "val_loss: 0.14461966632862008\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 251\n",
      "Train_loss: 2.8013731435319023\n",
      "val_loss: 0.14427856561157953\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 252\n",
      "Train_loss: 2.8005620494453267\n",
      "val_loss: 0.1441364145863956\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 253\n",
      "Train_loss: 2.8016737577021615\n",
      "val_loss: 0.1440899003080201\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 254\n",
      "Train_loss: 2.7989001276467276\n",
      "val_loss: 0.14407255315454975\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 255\n",
      "Train_loss: 2.796775311858616\n",
      "val_loss: 0.14433275337763993\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 256\n",
      "Train_loss: 2.8007983752459698\n",
      "val_loss: 0.14428931094613537\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 257\n",
      "Train_loss: 2.8003966898742703\n",
      "val_loss: 0.1444888750034323\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 258\n",
      "Train_loss: 2.799790419306532\n",
      "val_loss: 0.14410648145002214\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 259\n",
      "Train_loss: 2.798813538334005\n",
      "val_loss: 0.144320013014678\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "Epoch   161: reducing learning rate of group 0 to 4.0500e-05.\n",
      "epoch 260\n",
      "Train_loss: 2.7974443133016336\n",
      "val_loss: 0.14418692055763846\n",
      "best loss: 0.1438393733800109\n",
      "*********************************\n",
      "epoch 261\n",
      "Train_loss: 2.7859017098962555\n",
      "val_loss: 0.14340386389151222\n",
      "best loss: 0.14340386389151222\n",
      "*********************************\n",
      "epoch 262\n",
      "Train_loss: 2.7842628975099895\n",
      "val_loss: 0.14366129926662058\n",
      "best loss: 0.14340386389151222\n",
      "*********************************\n",
      "epoch 263\n",
      "Train_loss: 2.783611072514314\n",
      "val_loss: 0.14342243734611074\n",
      "best loss: 0.14340386389151222\n",
      "*********************************\n",
      "epoch 264\n",
      "Train_loss: 2.783323863871918\n",
      "val_loss: 0.14371472151465012\n",
      "best loss: 0.14340386389151222\n",
      "*********************************\n",
      "epoch 265\n",
      "Train_loss: 2.7820863507340547\n",
      "val_loss: 0.1433562755469565\n",
      "best loss: 0.1433562755469565\n",
      "*********************************\n",
      "epoch 266\n",
      "Train_loss: 2.7823118114914047\n",
      "val_loss: 0.14336886129788057\n",
      "best loss: 0.1433562755469565\n",
      "*********************************\n",
      "epoch 267\n",
      "Train_loss: 2.7812741741561986\n",
      "val_loss: 0.14356965322393359\n",
      "best loss: 0.1433562755469565\n",
      "*********************************\n",
      "epoch 268\n",
      "Train_loss: 2.7837829649271617\n",
      "val_loss: 0.1433776109155768\n",
      "best loss: 0.1433562755469565\n",
      "*********************************\n",
      "epoch 269\n",
      "Train_loss: 2.7839948879815224\n",
      "val_loss: 0.14336656016288887\n",
      "best loss: 0.1433562755469565\n",
      "*********************************\n",
      "epoch 270\n",
      "Train_loss: 2.781631116668445\n",
      "val_loss: 0.14334215178668888\n",
      "best loss: 0.14334215178668888\n",
      "*********************************\n",
      "epoch 271\n",
      "Train_loss: 2.782674959081077\n",
      "val_loss: 0.1432653969291198\n",
      "best loss: 0.1432653969291198\n",
      "*********************************\n",
      "epoch 272\n",
      "Train_loss: 2.780887345792707\n",
      "val_loss: 0.14334169105965439\n",
      "best loss: 0.1432653969291198\n",
      "*********************************\n",
      "epoch 273\n",
      "Train_loss: 2.782137478764853\n",
      "val_loss: 0.14320783042820487\n",
      "best loss: 0.14320783042820487\n",
      "*********************************\n",
      "epoch 274\n",
      "Train_loss: 2.7816125922919714\n",
      "val_loss: 0.14328028602618206\n",
      "best loss: 0.14320783042820487\n",
      "*********************************\n",
      "epoch 275\n",
      "Train_loss: 2.7793881833833787\n",
      "val_loss: 0.14324590024861636\n",
      "best loss: 0.14320783042820487\n",
      "*********************************\n",
      "epoch 276\n",
      "Train_loss: 2.7793643048895205\n",
      "val_loss: 0.14319416715599703\n",
      "best loss: 0.14319416715599703\n",
      "*********************************\n",
      "epoch 277\n",
      "Train_loss: 2.7784477259652163\n",
      "val_loss: 0.14326493719426392\n",
      "best loss: 0.14319416715599703\n",
      "*********************************\n",
      "epoch 278\n",
      "Train_loss: 2.7803185183646\n",
      "val_loss: 0.14319693209161974\n",
      "best loss: 0.14319416715599703\n",
      "*********************************\n",
      "epoch 279\n",
      "Train_loss: 2.7796280909118987\n",
      "val_loss: 0.1432171939167489\n",
      "best loss: 0.14319416715599703\n",
      "*********************************\n",
      "epoch 280\n",
      "Train_loss: 2.7814187459832764\n",
      "val_loss: 0.14355952200931954\n",
      "best loss: 0.14319416715599703\n",
      "*********************************\n",
      "epoch 281\n",
      "Train_loss: 2.778127930638773\n",
      "val_loss: 0.14318864172860107\n",
      "best loss: 0.14318864172860107\n",
      "*********************************\n",
      "epoch 282\n",
      "Train_loss: 2.7799070027049764\n",
      "val_loss: 0.14308425491255145\n",
      "best loss: 0.14308425491255145\n",
      "*********************************\n",
      "epoch 283\n",
      "Train_loss: 2.7802914684044278\n",
      "val_loss: 0.1434209032632638\n",
      "best loss: 0.14308425491255145\n",
      "*********************************\n",
      "epoch 284\n",
      "Train_loss: 2.77904975739491\n",
      "val_loss: 0.1432701555044304\n",
      "best loss: 0.14308425491255145\n",
      "*********************************\n",
      "epoch 285\n",
      "Train_loss: 2.7807305004055536\n",
      "val_loss: 0.14305953852968178\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 286\n",
      "Train_loss: 2.7799493472152554\n",
      "val_loss: 0.1432176542973685\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 287\n",
      "Train_loss: 2.7800128726565445\n",
      "val_loss: 0.14311326679019729\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 288\n",
      "Train_loss: 2.778381132529982\n",
      "val_loss: 0.14356612178262415\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 289\n",
      "Train_loss: 2.7791342062024547\n",
      "val_loss: 0.1434132255051565\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 290\n",
      "Train_loss: 2.7787029730329706\n",
      "val_loss: 0.1434087739300853\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 291\n",
      "Train_loss: 2.779355273071848\n",
      "val_loss: 0.14359391022734275\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 292\n",
      "Train_loss: 2.779997903165801\n",
      "val_loss: 0.14336747936806735\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 293\n",
      "Train_loss: 2.777528979855429\n",
      "val_loss: 0.14342182519323235\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 294\n",
      "Train_loss: 2.7791356304188115\n",
      "val_loss: 0.14354770208535758\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 295\n",
      "Train_loss: 2.777299782655124\n",
      "val_loss: 0.14318802608573844\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "Epoch   197: reducing learning rate of group 0 to 1.2150e-05.\n",
      "epoch 296\n",
      "Train_loss: 2.7778331630006186\n",
      "val_loss: 0.1435346544276881\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 297\n",
      "Train_loss: 2.7743641375351724\n",
      "val_loss: 0.1431436643640501\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 298\n",
      "Train_loss: 2.7756936649056057\n",
      "val_loss: 0.14313122721105095\n",
      "best loss: 0.14305953852968178\n",
      "*********************************\n",
      "epoch 299\n",
      "Train_loss: 2.7737948761760336\n",
      "val_loss: 0.14293181988518155\n",
      "best loss: 0.14293181988518155\n",
      "*********************************\n",
      "fold 4 score: 0.14293181764439908\n",
      "CV score: 0.14048028612813243\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 300\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 128\n",
    "oof_path = base_dir+f\"oofs/oof_{MODEL_NAME}\"\n",
    "device = torch.device('cuda:0')\n",
    "kf = StratifiedKFold(N_FOLDS,shuffle=True, random_state=42)\n",
    "oof = np.zeros([75450,80])\n",
    "y_true = le.inverse_transform(y_tr[:75450].reshape(-1)).reshape(oof.shape)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tr[:75450,0,0], X_tr[:75450,0,0])):\n",
    "    print(\"fold:\",fold)\n",
    "    model_path = base_dir+f'models/{MODEL_NAME}/model_{fold}.pt'\n",
    "\n",
    "    model = BrainModel().to(device)\n",
    "    \n",
    "    train_index = np.concatenate([train_index,np.arange(75450,len(X_tr))])\n",
    "\n",
    "    train_dataset = TensorDataset(torch.Tensor(X_tr[train_index]),torch.LongTensor(y_tr[train_index]))\n",
    "    val_dataset = TensorDataset(torch.Tensor(X_tr[val_index]),torch.LongTensor(y_tr[val_index]))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=3*BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "    if (not os.path.exists(model_path)) or True:\n",
    "        optimizer = optim.Ranger(model.parameters(), lr=5e-3, weight_decay=0, alpha=0.5, k=5)\n",
    "        scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = int(5 * len(train_dataloader)), eta_min=1e-5, last_epoch=-1)\n",
    "        scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10, threshold=0.0001, min_lr=1e-5, verbose=True)\n",
    "        \n",
    "        best_score = 999  \n",
    "        for epoch in tqdm(range(N_EPOCHS),leave=False):\n",
    "            Train_loss = train_one_epoch(model, optimizer, scheduler1, train_dataloader, epoch, device)\n",
    "            val_loss = evaluation(model, val_dataloader, device)\n",
    "            if epoch >= 100:\n",
    "                scheduler2.step(val_loss)\n",
    "            if val_loss < best_score:\n",
    "                best_score = val_loss\n",
    "                torch.save(model.state_dict(),model_path)\n",
    "            print(f\"epoch {epoch}\")\n",
    "            print(f\"Train_loss: {Train_loss}\")\n",
    "            print(f\"val_loss: {val_loss}\")\n",
    "            print(f\"best loss: {best_score}\")\n",
    "            print(\"*********************************\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    val_pred = inference(model, val_dataloader, device, False)\n",
    "    oof[val_index] = val_pred\n",
    "    mask = X_tr[val_index,:,2]==0\n",
    "    print(f\"fold {fold} score:\",mean_absolute_error(y_true[val_index][mask], oof[val_index][mask]))\n",
    "mask = X_tr[:75450,:,2]==0\n",
    "print(\"CV score:\",mean_absolute_error(y_true[mask], oof[mask]))\n",
    "np.save(oof_path,oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch15]",
   "language": "python",
   "name": "conda-env-.conda-pytorch15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31610.007673,
   "end_time": "2021-10-13T13:02:25.335355",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-13T04:15:35.327682",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12a4846039954478b23397922ba2e7a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64b71c2ec725460c9d97dea18c1b8221",
       "max": 300,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7953bfe3acad4a50b5f687991211d2bc",
       "value": 300
      }
     },
     "4927652eb1c346fa84a78d6c5e045622": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "504ef3b85c1c4688bc53a334675aa601": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f594d87f3fb741b8a87da31348a174da",
       "placeholder": "​",
       "style": "IPY_MODEL_ec150678fe7a4cf98a1a68ccd5409338",
       "value": "100%"
      }
     },
     "575472c0debb49129fb32c54a9289a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_504ef3b85c1c4688bc53a334675aa601",
        "IPY_MODEL_12a4846039954478b23397922ba2e7a2",
        "IPY_MODEL_f6679feabe7341e19bce028fd9597ebd"
       ],
       "layout": "IPY_MODEL_a561495982104452b7b4ba9fdd176a37"
      }
     },
     "5b660f1a714a4a8a8e2f2aefcd060220": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64b71c2ec725460c9d97dea18c1b8221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7953bfe3acad4a50b5f687991211d2bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a561495982104452b7b4ba9fdd176a37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec150678fe7a4cf98a1a68ccd5409338": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f594d87f3fb741b8a87da31348a174da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6679feabe7341e19bce028fd9597ebd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5b660f1a714a4a8a8e2f2aefcd060220",
       "placeholder": "​",
       "style": "IPY_MODEL_4927652eb1c346fa84a78d6c5e045622",
       "value": " 300/300 [8:44:09&lt;00:00, 104.89s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
